---
title: A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care
authors:
- Oliver Normand
- Esther Borsi
- Mitch Fruin
- Lauren E Walker
- Jamie Heagerty
- Chris C. Holmes
- Anthony J Avery
- Iain E Buchan
- Harry Coppock
date: '2025-12-24'
categories:
- cs.AI
pdf_url: https://arxiv.org/pdf/2512.21127v1
arxiv_id: 2512.21127v1
tags:
- paper
- alphaxiv/hot
- topic/cs-AI
---

# A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care

**Authors:** Oliver Normand, Esther Borsi, Mitch Fruin, Lauren E Walker, Jamie Heagerty...

**Date:** 2025-12-24 | **Categories:** cs.AI

[PDF](https://arxiv.org/pdf/2512.21127v1) | [AlphaXiv](https://alphaxiv.org/abs/2512.21127v1)

## Abstract

Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

## Notes

