---
url: https://review.insignia.vc/2025/03/10/ai-moat/
title: "Is Proprietary Data Still a Moat in the AI Race? - Insignia Business Review"
clipped: 2025-12-23 13:00
source: slack
slack_channel: healthcare-aiml-deskresearch
---

# Is Proprietary Data Still a Moat in the AI Race? - Insignia Business Review

> Source: [https://review.insignia.vc/2025/03/10/ai-moat/](https://review.insignia.vc/2025/03/10/ai-moat/)

How modern AI startups leverage new LLM versions to reduce the need for exclusive data, the crucial role of distribution in securing market leadership, and more

### Share this:

* [Click to share on Facebook (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=facebook "Click to share on Facebook")
* [Click to share on LinkedIn (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=linkedin "Click to share on LinkedIn")
* [Click to share on Twitter (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=twitter "Click to share on Twitter")
* [Click to share on WhatsApp (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=jetpack-whatsapp "Click to share on WhatsApp")

[AI Notes](https://review.insignia.vc/category/industry/ai-notes/) [Featured](https://review.insignia.vc/category/featured/) [Industry](https://review.insignia.vc/category/industry/)

# Is Proprietary Data Still a Moat in the AI Race?

How modern AI startups leverage new LLM versions to reduce the need for exclusive data, the crucial role of distribution in securing market leadership, and more

### Share this:

* [Click to share on Facebook (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=facebook "Click to share on Facebook")
* [Click to share on LinkedIn (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=linkedin "Click to share on LinkedIn")
* [Click to share on Twitter (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=twitter "Click to share on Twitter")
* [Click to share on WhatsApp (Opens in new window)](https://review.insignia.vc/2025/03/10/ai-moat/?share=jetpack-whatsapp "Click to share on WhatsApp")

By
[Peter Mascarinas](https://review.insignia.vc/author/petermascarinas/)
//

[March 10, 2025](https://review.insignia.vc/2025/03/10/)

Artificial intelligence is evolving under **winner-take-most** dynamics, where a few players capture outsized value. Unlike traditional tech moats, today’s AI agents can **scale rapidly** by piggybacking on ever-more-powerful large language models (LLMs) instead of relying on massive proprietary datasets.

This shift lets businesses build and iterate on AI products faster than ever. In this article, we explore how modern AI startups leverage new LLM versions to reduce the need for exclusive data, the crucial role of distribution in securing market leadership, and what this means for startups weighing **data moats** versus **distribution-first** strategies.

### **AI Agents and Instant Scale with New LLMs**

Every major jump in LLM capability – from GPT-3 to GPT-4 and beyond – gives AI companies a boost in performance almost overnight. An AI agent’s skills can improve dramatically just by upgrading the underlying model, without years of additional data collection. This means a small startup can achieve in weeks what once took huge datasets and research teams.

For example, when OpenAI released GPT-4, many AI products saw quality leaps simply by switching their API to the new model. Complex tasks like summarization, coding, or reasoning became more accurate, enabling startups to **scale** their offerings quickly. This rapid improvement lowers the bar for new entrants: instead of painstakingly training a model from scratch, a team can **plug into the latest LLM** and immediately offer cutting-edge capabilities.

Take **Bolt.new** – an AI agent for software development – which effectively wrapped Anthropic’s advanced Claude model in a user-friendly coding environment. By leveraging Claude’s power (instead of building its own model), Bolt.new rocketed to over **$8 million in annual recurring revenue (ARR) within two months** of launch​ ([Latent Space](https://www.latent.space/p/bolt#:~:text=When%20we%20first%20observed%20that,accelerating%20its%20initial%20%244m%20feat)). This astonishing growth was possible because the heavy lifting (training a state-of-the-art model) was done by others; Bolt.new’s innovation was orchestrating that model to build apps from natural language. The case of Bolt.new shows how quickly an AI product can capture value by riding the wave of a new LLM version.

Another example is in healthcare: **Abridge**, a startup creating AI “medical scribes,” uses generative models to turn doctor-patient conversations into clinical notes. When more powerful LLMs became available, Abridge could scale its system to handle more complex medical dialogues without needing entirely new datasets – the improved language understanding of the model did the work. Abridge’s latest tool even **integrates directly into Epic’s electronic health record platform**, so clinicians can generate notes in real time during patient visits​ ([Digital Health News](https://www.digitalhealthnews.com/abridge-expands-ai-powered-medical-documentation-to-emergency-departments#:~:text=Abridge%27s%20new%20AI%20tool%20integrates,Epic%27s%20mobile%20and%20desktop%20platforms)). By swiftly adopting new LLM capabilities and embedding into existing workflows, Abridge rapidly expanded across major health systems. The **speed of improvement** with each LLM release gave these AI agents a head start, highlighting a winner-take-most pattern: those who adopt and distribute new AI capabilities fastest often seize a huge user base before others catch up.

### **Diminishing Need for Proprietary Data Moats**

In the past, companies often touted proprietary data as an **AI moat** – exclusive datasets that would make their models better over time. But today, foundation models are largely trained on **public or web-scale data**, and techniques like transfer learning and synthetic data generation further shrink the advantage of owning a unique corpus. As one analysis noted, *“foundation models are primarily built on public data…the value of private data is limited.”*​ ([Bessemer Venture Partners](https://www.bvp.com/atlas/artificial-intelligence-is-generating-massive-consumer-surplus#:~:text=,of%20private%20data%20is%20limited)). In other words, a clever algorithm fine-tuned on a bit of domain data can often rival another trained on a giant private dataset, thanks to the broad knowledge already baked into modern LLMs.

This isn’t to say data is useless – in specialized fields, it can still add critical nuance. Healthcare AI offers a case in point. **Eleos Health**, which builds AI to assist behavioral therapy documentation, emphasizes that general-purpose models won’t fully grasp therapy nuances without domain-specific training.

*“Only a tool built and trained with behavioral health-specific data and clinical expertise can capture the depth and nuance of therapy conversations,”* explains Eleos’s Chief Clinical Officer​ ([Eleos Health](https://eleos.health/blog-posts/behavioral-health-specific-ai/#:~:text=%E2%80%9CFor%20any%20technology%20that%20leverages,based%20behavioral%20health%20clinics.%E2%80%9D)). Eleos has clinicians guide and fine-tune its models on real therapy session data, aiming for accuracy that a generic LLM might miss. This proprietary data approach can yield a better product in that niche – a **data moat** of sorts.

However, the overall trend is that these moats are harder to maintain. Each new LLM release narrows the performance gap that proprietary data might have initially provided.

Competitors can often achieve “good enough” results by using the latest public models and a small amount of fine-tuning. In fact, much of the **value in AI is being passed to consumers** rather than captured by companies. Basic AI features are becoming **commodity “table stakes”** – for instance, AI writing and summarization are now built into Notion, Google Workspace, Microsoft Office, and many other products **for free**​ ([Bessemer Venture Partners](https://www.bvp.com/atlas/artificial-intelligence-is-generating-massive-consumer-surplus#:~:text=,if%20any%29%20it%20generates)). Everyone has access to similar AI capabilities, which means no single app can easily lock in users solely by having “the better model.”

Even reinforcement learning from human feedback (RLHF), often cited as a way to improve models with proprietary user data, provides only a minor edge. Industry observers note that **RLHF alone isn’t a durable moat** unless you already have a large, engaged user base providing that feedback.

It’s described as a *“secondary moat… the result of a distribution advantage or network effect. That is the real moat.”*​ ([Bessemer Venture Partners](https://www.bvp.com/atlas/artificial-intelligence-is-generating-massive-consumer-surplus#:~:text=,Midjourney%20embodies%20this)). In short, having unique data helps, but it’s rarely enough to ensure dominance. **Access to the best models is broad**, and the next breakthrough can quickly wipe out a lead that was built purely on data quality or quantity.

### **Distribution: The Real Advantage in Replicable AI**

If cutting-edge AI capabilities are increasingly accessible to all, how do certain companies still pull ahead? The answer often lies in **distribution**. In a winner-take-most market, getting your AI solution widely adopted — through platform integrations, network effects, or simply being first to solve a pressing user problem — creates a self-reinforcing advantage. Strong distribution can beat strong algorithms, especially when algorithms are replicable.

AI products today are remarkably easy to **replicate or fast-follow** if you have distribution on your side. Large tech firms demonstrate this plainly: the moment a novel AI feature shows promise, it can be rolled into an incumbent’s product suite and pushed out to millions of users via an update.

Startups face the same from bigger rivals or well-funded peers. An enterprise analysis of AI search company **Glean** highlighted that its AI-powered knowledge search could be **“at risk of replication by emerging competitors or integration into offerings from established [software] providers,”** given how quickly others can now build similar tech​ ([Manhattan Venture Research](https://www.mvp.vc/company-initations/initiation-report-glean-technologies---disrupting-enterprise-knowledge-management-with-ai#:~:text=Glean%E2%80%99s%20proprietary%20AI,can%20be%20developed%2010X%20faster)).

A concrete example was Snowflake (a data cloud company) training a comparable enterprise LLM in just three months with a $2M investment​ – something that would have been unthinkable a few years ago. With modern tools, “any web, mobile, or backend application can be developed 10x faster and 3x cheaper with generative AI” than before​. In such an environment, a great idea doesn’t give you a long-term monopoly; you must scale it *fast*.

This is why distribution is often the deciding factor. Glean, despite the threat of copycats, managed to gain an **early-mover advantage** by rapidly signing up enterprises for its AI search and deeply integrating into their workflows.

That early adoption and integration into tools like Slack, Zendesk, and Salesforce created switching costs and user habits that favor Glean, even if competitors arise. As the analysis noted, *“an early-mover advantage coupled with enterprises’ rapid adoption of AI favors Glean.”*​ ([Manhattan Venture Research](https://www.mvp.vc/company-initations/initiation-report-glean-technologies---disrupting-enterprise-knowledge-management-with-ai#:~:text=Moreover%2C%20any%20web%2C%20mobile%2C%20or,enhancements%20of%20its%20platform%20and)). In essence, Glean’s distribution foothold in numerous organizations becomes its moat, more so than its algorithms.

Another illustration comes from the coding assistant space. **GitHub Copilot**, powered by OpenAI’s models, had a distribution trump card by being integrated into Visual Studio Code and GitHub from day one. This allowed Copilot to amass a huge user base before other coding AIs like **Cursor** or **Replit Ghostwriter** could catch up.

Those competitors might employ similar or even improved LLMs, but without comparable distribution (integrated IDEs, community reach), they fight an uphill battle. Conversely, if a smaller player gains traction, the incumbent can quickly plug in the latest model and offer a similar feature via an update to reclaim users. It’s a classic **platform advantage**: owning the channel to users lets you fast-follow innovations.

The healthcare AI arena shows how distribution partnerships are key. We saw how Abridge partnered with Epic Systems to embed its AI in one of the most widely used electronic health record platforms​ ([Digital Health News](https://www.digitalhealthnews.com/abridge-expands-ai-powered-medical-documentation-to-emergency-departments#:~:text=Abridge%27s%20new%20AI%20tool%20integrates,Epic%27s%20mobile%20and%20desktop%20platforms)). That move instantly placed Abridge inside hospital workflows across several major health systems, a distribution boost that’s hard for a new entrant to match. In behavioral health, Eleos similarly works on integration with electronic medical record systems and securing contracts with large provider networks. A product that’s **embedded and trusted** has a big head start in the race.

Finally, consider **HeyGen**, the generative video startup. By focusing on making their AI video creation extremely easy to use and aggressively marketing through product showcases, HeyGen scaled to **$35 million ARR (annual recurring revenue) from just $1 million in a little over a year**​ ([AIM Research](https://aimresearch.co/generative-ai/u-s-startup-heygen-secures-60-million-for-ai-video-push#:~:text=The%20company%20has%20seen%20its,been%20profitable%20since%20Q2%202023)). The technology behind talking avatar videos isn’t exclusive to HeyGen – rivals can also leverage text-to-speech, deepfake, and image generation models. But HeyGen’s swift go-to-market and broad adoption (especially among businesses seeking quick video content) gave it a brand and user-base advantage. The result: the company became profitable and secured a large funding round to cement its lead. This **speed of execution and distribution** – getting in front of as many users as possible – illustrates why, in AI, you often see a few breakout winners taking most of the market’s rewards.

### **Data Moats vs. Distribution-First: Implications for Startups**

For AI startups, a key strategic question is where to invest for defensibility: build a **proprietary data moat**, or focus on **distribution and speed**, or some combination of both. The dynamics we’ve discussed suggest a few considerations for this decision:

* **Data moats** are useful in specialized areas (e.g., healthcare, legal AI), where exclusive datasets dramatically enhance accuracy.
* **Distribution-first strategies** work best when AI capabilities can be **easily replicated**. Scaling fast and integrating into existing workflows gives companies a lasting advantage.
* **Hybrid models** combine both: launching with a strong distri

[... truncated ...]