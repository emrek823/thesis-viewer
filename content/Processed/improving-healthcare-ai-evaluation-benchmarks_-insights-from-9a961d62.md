---
source_pdf: "https://drive.google.com/file/d/1_EwkjQIJI86hvBukqoIcVlyVD_WPu-GN/view"
drive_folder: "Research"
type: research

ingested: 2025-12-28
original_filename: "Improving Healthcare AI Evaluation Benchmarks_ Insights from APEX, MedPI, WONDERBREAD, and Beyond.pdf"
---

> **Original:** [View Original PDF](https://drive.google.com/file/d/1_EwkjQIJI86hvBukqoIcVlyVD_WPu-GN/view)

# Improving Healthcare AI Evaluation Benchmarks: Insights from APEX, MedPI, WONDERBREAD, and Beyond

## Current State of AI Benchmarks in Healthcare

**APEX (AI Productivity Index):** APEX is a recently introduced benchmark evaluating whether frontier AI models can perform high-value knowledge work across domains including medicine [1]. APEX v1.0 consists of 200 expert-crafted test cases (50 in primary medical care and 150 in law, consulting, banking) with realistic prompts that would take a human expert 1-8 hours to complete [1]. Each case provides reference evidence documents and a detailed rubric of ~20-36 criteria for judging quality [4, 2]. Models produce a static deliverable (e.g. a consultation plan or analysis) which is then scored against the rubric by a panel of AI and human judges [3]. **Strengths:** APEX's tasks are grounded in real-world professional workflows, requiring complex reasoning and incorporation of evidence [1, 4]. The rubric-based evaluation (with criteria like accuracy, depth, adherence to guidelines) yields granular performance metrics rather than a single score [5, 2]. This approach surfaces where models fall short of expert-level work – for example, GPT-5 scored ~64% on APEX while human experts would be near 100%, and medicine proved the most challenging domain (models averaged only ~47.5% on medical tasks) [6]. APEX highlights that even top models lack the nuance and depth of real physicians, often missing critical details and reasoning steps [7]. **Limitations:** APEX tasks are single-turn and static - there is no interactive dialogue or prolonged decision process. Critically, models are provided with all relevant background sources upfront, so retrieval skills are not tested [8]. The tasks focus on final written output quality; they do not exercise a model's ability to use tools or take actions over time. Scoring relies on large LM judges, which, while correlated with human grades, could introduce biases or blind spots [9]. In short, APEX measures complex static deliverables well, but doesn't capture dynamic workflow execution or information-seeking behavior in healthcare settings.

**MedPI:** MedPI (Medical Performance Index) is an evaluation framework proposed in a 2024 Lumos Labs whitepaper focusing specifically on healthcare AI capabilities (beyond just exam-style Q&A). It aims to assess an AI's performance across core clinical tasks such as diagnostic reasoning, treatment planning, patient communication, and documentation in realistic contexts. Like APEX, MedPI uses expert-designed scenarios and rubrics, but it emphasizes clinical workflow understanding over isolated question answering. For example, a MedPI case might present a patient's history, symptoms, lab results, and require the model to produce an assessment and plan. Each scenario can involve interpreting findings, suggesting next steps, and explaining reasoning - reflecting how a doctor works through a case. **Strengths:** MedPI targets what truly matters in practice, moving past trivial medical board questions [10]. It evaluates not only factual accuracy but also clinical decision-making, adherence to care guidelines, and the usefulness of explanations (e.g. does the AI flag uncertainty or suggest confirming tests?). By looking at end-to-end case handling, MedPI begins to capture workflow context (how well the AI integrates multiple pieces of information over a patient encounter). **Limitations:** As a new framework (whitepaper stage), MedPI scenarios are still mostly static vignettes - e.g. one simulated patient case at a time - rather than fully interactive patient engagements. There is typically a single "turn" where the model outputs a consultation note or advice based on a given case. Thus, current MedPI evaluations may not include longitudinal follow-up (e.g. adjusting treatment after seeing patient response) or multi-party coordination (nurse, specialist, etc.). Also, like APEX, the model isn't explicitly required to retrieve external knowledge - the necessary patient data is provided. Safety and uncertainty handling are considered qualitatively, but there might not yet be a systematic way to reward safe deferment or heavily penalize dangerous recommendations beyond losing rubric points. These gaps point to the need for extending MedPI into a more dynamic, risk-aware benchmark (addressed in proposals later).

**WONDERBREAD:** WONDERBREAD is a benchmark introduced at NeurIPS 2024 to evaluate AI models on business process management (BPM) tasks – many of which are analogous to complex healthcare administrative workflows [11, 12]. It provides ~3,000 human-demonstrated workflows (e.g. screen recordings and step-by-step logs of processes) and defines 6 distinct tasks such as workflow documentation, knowledge transfer (e.g. generating SOPs from demos), and process validation [12]. The benchmark is multimodal: models might watch a video or observe a sequence of actions and then produce an output (like a written procedure or a verification report). WONDERBREAD's evaluation harness can automatically check, for instance, how many steps in a video demo the model correctly recalled or whether it detected all deviations from a prescribed process [13]. **Strengths:** This benchmark moves beyond static QA into workflow understanding. It tests an AI's ability to interpret a sequence of events and produce structured outputs, which is highly relevant to clinical settings (think of parsing a patient's electronic health record timeline or transcribing a procedure). Models like GPT-4 have shown they can generate decent workflow documentation (recalling ~88% of steps from a video) [13]. WONDERBREAD's tasks mimic real enterprise scenarios, so it evaluates skills like following multi-step procedures, maintaining state across steps, and handling multimodal input (vision + text) – something standard medical exams don't cover. **Limitations:** The environment in WONDERBREAD is still largely offline; the model observes a recorded demonstration or given input and then outputs an answer. It's not an interactive agent continuously operating in a live system. Also, while it covers generic workflows, it's not healthcare-specific - the tasks (documenting a process, improving a workflow) need adaptation to clinical domains. Notably, results revealed that today's models struggle with finer-grained process validation (F1 < 0.3 in checking if a workflow was done correctly) even if they can summarize the steps [13]. This indicates current AI may miss subtle errors or compliance issues, a serious limitation in high-stakes fields like medicine. In summary, WONDERBREAD provides a glimpse of evaluating AI on multi-step procedures, but lacks medical context and real-time interactivity, leaving gaps in assessing how an AI would perform in a live clinical process.

### Summary Table – Key Characteristics of Current Benchmarks

| Benchmark        | Scope & Format                                                                 | What It Measures                                                                 | Key Strengths                                                                                                                                                                          | Key Limitations                                                                                                                                                                                              |
| :--------------- | :----------------------------------------------------------------------------- | :------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| APEX v1.0 (2025) | 200 static expert tasks in 4 domains (incl. 50 medical). Single-turn deliverable with provided sources [1]. | Economic value of outputs; correctness, depth via rubric criteria [1].           | High-realism prompts from industry experts; multi-criteria rubrics; highlights gap to human expert level [2].                                                                            | No interactivity or tool use; no retrieval (sources given) [8]; LM-based grading biases; domain coverage limited to four areas.                                                                                  |
| MedPI (2024)     | Dozens of clinical case vignettes (primary care, specialties). Static scenario -> model writes advice/notes. | Clinical decision-making, coherence of plans, guideline adherence in context.    | Holistic clinical focus (beyond exams); evaluates reasoning on patient cases and explanations [10].                                                                                        | Lacks multi-turn patient interaction; no longitudinal follow-up; safety handling not explicitly quantified; not yet an interactive sim.                                                                         |
| WONDERBREAD (2024) [12] | ~3k recorded workflows, 6 BPM tasks (e.g. generate SOP from video, verify process). Multimodal I/O. | Workflow understanding: recall of steps, ability to generate process docs, detect deviations [13]. | Multimodal and multi-step evaluation; real-world enterprise processes; automated metrics (e.g. recall %, F1) [13]. | Not healthcare-specific; model isn't an active agent, just post-hoc analysis; struggles with fine validation (low F1 means missing errors) [13].                                                                   |

**Analysis:** Together, these benchmarks represent the state-of-the-art in evaluating AI for complex tasks. APEX and MedPI bring domain experts and detailed rubrics to bear on static high-stakes questions, while WONDERBREAD introduces sequential, multimodal tasks. However, real-world healthcare scenarios - e.g. managing a patient's care over weeks, coordinating between doctor and insurer, safely handling uncertainty - are not yet fully captured. The next sections identify specific gaps and propose how we can bridge them by learning from other industries.

## Gaps in Real-World Readiness

Despite their advances, current benchmarks fall short of measuring an AI's true clinical readiness. Key missing elements include:

*   **Retrieval and Evidence Assembly:** In practice, a medical AI assistant may need to pull the latest research or patient data from a database. APEX provides all sources to the model (making the test easier) [8] and MedPI cases typically include the necessary info. None of the benchmarks gauge whether the AI can find relevant information on its own. In a hospital scenario, the model should know how to search medical literature or a patient's chart - and importantly, judge source credibility. Current evaluations skip this step, so a model could score well by reasoning with given facts, yet still fail at gathering those facts in the first place.

*   **Tool Use and Integration:** Real clinical workflows involve tools - from calculators (e.g. an app to calculate drug dosage or risk scores) to EHR software. A doctor might use a growth chart tool or a drug interaction checker. APEX's authors themselves note future versions should incorporate software and multi-turn interactions [15]. Today's benchmarks rarely test if an AI can invoke tools or APIs. For example, can it query a lab system for a patient's results, or use a calendar to schedule follow-ups? Without this, we don't know if a model that can write a great plan can also execute or facilitate it using digital systems.

*   **Longitudinal and Multi-step Workflows\*:** Healthcare problems play out over time. A diagnosis might require follow-up tests; treatment response must be monitored and plans adjusted. Current benchmarks are one-and-done - the model gives one answer per case. There's no evaluation of *longitudinal performance*: can the AI correctly react to new information over multiple encounters? Similarly, many healthcare processes involve multiple steps and hand-offs (e.g. primary care to specialist referral to pharmacy fulfillment). Existing tests don't capture whether an AI understands these *process flows* end-to-end. We risk overestimating an AI that does well on single-turn questions but would stumble when tasks are chained together.

*   **Multi-Agent Coordination:** In the real world, an AI assistant might need to interface with different stakeholders - doctors, patients, payers, pharmacies - each with their own goals. For instance, obtaining an insurance prior authorization involves back-and-forth communication between a clinic and a payer. None of the current benchmarks include multi-agent interactions or negotiation. They implicitly assume one AI interacting with a static prompt or environment. We have no benchmark for how an AI handles the “payer-provider-pharmacy” loop – e.g. explaining a treatment to a patient while justifying coverage to an insurer. This is crucial for workflows like discharge planning or chronic care coordination.

*   **Safety and Uncertainty Handling:** A major gap is testing whether an AI knows when not to act. Human clinicians routinely say "I'm not sure – let's order another test" or will double-check a risky decision. Benchmarks currently mostly reward correctness of content; they don't strongly penalize unsafe suggestions beyond a lower rubric score, nor do they reward a model for cautious behavior like appropriately deferring or asking for human help. In fact, models with aggressive answers might score higher on helpfulness benchmarks but could be dangerous in practice. We need evaluation of an AI's ability to detect its own uncertainty, refrain from guessing, and follow safety protocols (e.g. contraindication warnings). This isn't explicitly measured in APEX/MedPI today.

*   **Adversarial and Stress Testing:** Real-world environments are noisy and adversarial. Patients might give misleading information (intentionally or not), or bad actors might try to trick the AI (e.g. a user asking medical advice in a way to induce a harmful response). Current benchmarks assume honest, clean inputs. They don't include adversarial scenarios like a patient withholding key symptoms, or an input crafted to confuse the model. Additionally, models aren't stress-tested under distribution shifts (e.g. rare diseases or atypical presentations). Other high-stakes fields (like cybersecurity or finance) incorporate adversarial evals to ensure robustness [16, 17]. Healthcare AI evaluation has yet to systematically include "red team" tests for worst-case behaviors.

In summary, today's benchmarks are valuable but overly static and narrow. They excel at measuring what an AI says in ideal conditions. The next generation must evaluate how an AI acts in realistic, messy environments - retrieving knowledge, using tools, coordinating with others, managing uncertainty, and resisting adversarial challenges.

## Cross-Industry Benchmarks and Lessons

Other industries have grappled with evaluating AI and autonomous systems in complex environments. We can draw parallels from these domains – their simulators, reward designs, and safety guardrails – to inform better healthcare AI benchmarks.

### Self-Driving Cars: Simulation & Domain Randomization

Autonomous driving is an arena where safety is paramount, and testing AI in simulated environments is now standard. Platforms like CARLA (an open-source driving simulator) let developers evaluate self-driving algorithms on virtual roads with configurable traffic, weather, and scenarios. Crucially, the automotive field employs scenario-based testing - e.g., how does the AI handle a child running into the street, or a sudden tire blowout? These scenarios are analogous to medical edge cases (like anaphylaxis during a routine exam). One practice we should emulate is domain randomization: in simulation, factors like lighting, vehicle models, or road layout are randomized so that the AI doesn't overfit to one exact setup [18]. By exposing the model to myriad variations (rainy vs. sunny, heavy traffic vs. light), engineers ensure that the real world looks like just another variation the AI has seen [19]. In healthcare, domain randomization could mean varying patient attributes and noise: e.g. randomly altering vitals within normal ranges, changing how data is formatted, or simulating different hospital protocols. This guards against a model that works only on one hospital's EHR format or only on neatly presented cases. The self-driving industry also heavily uses simulation as a safety buffer – an AV must prove itself for millions of virtual miles before any limited real-world trial. Similarly, we might require health AIs to pass thousands of simulated patient encounters (including rare crises) without critical failures as a predicate to clinical deployment.

Another lesson is how performance is measured: self-driving benchmarks don't just use a single score, but a collection of safety metrics (collisions per 1,000 miles, interventions required, etc.). A healthcare AI benchmark could similarly track multiple outcomes: diagnostic accuracy, treatment success rate, near-miss events (e.g. almost gave a dangerous medication but caught itself), and so on. Failure mode analysis is routine in AV testing – when the car AI fails in sim, engineers analyze why (missed a pedestrian vs. misread a sign). We should incorporate that mindset: benchmarks should facilitate analysis of how the AI failed (e.g. misinterpreted symptom vs. math error in dosage) to drive improvements.

Finally, regulation in autonomous driving requires extensive evidence from simulation. Entities like NHTSA encourage manufacturers to test for edge cases. Healthcare regulators (FDA, etc.) could similarly leverage simulation-based benchmark results as evidence that an AI has been challenged against a broad spectrum of clinical scenarios, much like a driving test for medical AI.

### Robotics and Reinforcement Learning: Isaac Gym and Beyond

In robotics, particularly reinforcement learning (RL) for control, the community has developed high-fidelity simulators such as NVIDIA's Isaac Gym [21]. Isaac Gym enables thousands of parallel robot simulations on a GPU, massively speeding up training and evaluation [20]. This approach – massive parallel simulation – is key for evaluating performance across many randomized trials. For instance, researchers can test a robotic leg's walking policy on 1,000 different virtual terrains simultaneously to assess robustness. In healthcare, we might imagine simulating hundreds of patient cases in parallel (with varied profiles) to statistically evaluate an AI's consistency and reliability. The ability to run many scenarios quickly means we can include more rare but important events in our evaluation (just as robotics can include rare rugged terrain).

Robotics benchmarks also emphasize continuous performance over time. Rather than a single-step output, a robot's reward might accumulate over an episode (did it reach the goal without falling?). Likewise, a healthcare AI evaluation could assign a reward or score over a sequence of decisions: e.g. managing a simulated patient for 10 days of hospital stay, with points for each correct decision and penalties for each mistake. This would mirror RL environments where the AI's policy is assessed on the total outcome (patient recovered, complications avoided, etc.), not just a single answer.

Moreover, robotics has pioneered safety constraints in training and eval. In physical robot testing, certain actions (like too high velocity) are disallowed or result in an immediate episode termination to protect the hardware. Translating that to healthcare: our benchmarks or training environments could terminate (or harshly penalize) an episode if the AI makes a critical safety error (e.g. recommending a lethal dose), reinforcing that such actions are unacceptable. Robotics RL often uses reward shaping – combining multiple objectives (speed, energy efficiency, stability) by weighting them. This offers a template for how to combine multiple criteria in healthcare AI rewards (accuracy, explanation quality, patient satisfaction, etc.) into one composite score for optimization, as we will discuss in the reward design section.

The Isaac Gym paradigm also highlights the value of a rich simulated physics environment. In healthcare, an analogy might be a rich simulated hospital or patient physiology environment – for example, a patient simulator that responds to interventions (akin to how a physics engine responds to forces). While nascent, there are efforts to simulate patient health trajectories. If we can plug an AI policy into such a simulator, we can evaluate its performance in silico (e.g. does the patient's simulated blood pressure improve under the AI's treatment plan or crash?). This kind of end-to-end simulation-based evaluation would be a leap forward from static benchmarks.

### Fintech Risk/Fraud Monitoring: Adversarial Evaluation and Guardrails

Finance has long dealt with adaptive adversaries (fraudsters) and uses continuous evaluation to keep models effective. In fraud detection, a static model that performed well last month might be defeated by new fraud tactics next month [16, 22]. To address this, fintech has embraced ideas like adversarial training and simulation of attackers. For example, researchers have framed credit card fraud as a Markov Decision Process where the fraudster is an agent trying to maximize stolen funds, and the fraud detection system is the environment opposing it [23, 24]. By simulating this cat-and-mouse game, they can evaluate how quickly an AI fraud detector adapts to new fraud patterns and even derive the optimal adversarial strategies to then patch against. The key takeaway for healthcare is the importance of evaluation under adversarial conditions. We should test medical AI with scenarios where input data might be intentionally corrupted or where the "user" might try to get inappropriate advice (a form of adversarial input). Fintech's approach of continually updating the model and measuring its resilience to adversaries maps to, say, measuring a medical AI's resilience to misinformation (does it rely on obviously falsified lab results or seek clarification?).

Fintech also employs guardrail policies extensively. For instance, banks set hard rules like "if a transaction is above $X and overseas, flag it for review regardless of model output". This ensures that even if the AI is overly optimistic, certain high-risk events trigger a manual check. In evaluating AI, we could similarly ensure the model adheres to guardrails: e.g. a benchmark could include a rule that if a patient's symptoms indicate a possible stroke, the AI must recommend immediate ER referral - any other action yields a zero score for that case. By encoding such guardrails into the evaluation, we effectively require the AI to demonstrate it won't violate critical safety rules. Fintech also balances false positives vs false negatives explicitly (fraud vs customer insult rate) [25]. In health, that's analogous to balancing sensitivity (catching illness) vs specificity (not over-treating). Benchmarks could include metrics for both, ensuring an AI isn't tuned to maximize one at the undue expense of the other.

Another cross-industry insight is regulatory oversight. Just as banking regulators require stress-testing credit risk models under worst-case economic scenarios ("stress tests"), one could envision healthcare regulators expecting AI models to be stress-tested under worst-case clinical scenarios (e.g. an outbreak, or a highly complex patient with multiple comorbidities) and see if they still perform safely. Fintech's practice of using simulated data to generate rare events (like market crashes or extreme fraud bursts) to test models could be applied by simulating rare but critical medical events (pandemic surges, etc.) in our benchmarks.

Overall, fintech teaches us that evaluation is not a one-time static event but an ongoing process with adversarial inputs and strong guardrails – a philosophy healthcare AI evaluation should adopt to maintain safety and effectiveness over time.

### Coding Copilots: Unit Tests as Evaluation and Reward

AI coding assistants (like GitHub Copilot or OpenAI's Codex) offer a compelling parallel to medical AI because they are evaluated on problem-solving tasks with objective correctness criteria. In coding, the de facto benchmark is having the AI write code that passes a suite of unit tests. For example, the HumanEval benchmark for code generation uses hidden test cases, and the AI's score is the percentage of tests passed. This approach is essentially treating each unit test as a verifiable reward signal – the code either produced the correct output or not [26]. We can learn from this by developing "unit tests" for healthcare scenarios. What would that look like? Perhaps a set of checks for each clinical case: Did the AI's recommendation improve the simulated patient's outcome? Did it avoid contraindicated drugs? Did it follow all guideline steps? Each of these could be an analogue to a unit test that returns pass/fail, contributing to an overall score.

The coding domain has even moved toward reinforcement learning from test feedback: instead of just evaluating after the fact, some systems use test results as a reward to further train the model to generate correct solutions [27, 26]. In healthcare, this could translate to iterative improvement: an AI plan could be "executed" in a simulator (like treating a virtual patient), and the outcome (patient gets better or worse) feeds back as a reward to refine the AI's policy. This closes the loop between evaluation and training in a powerful way. It's essentially training AI to pass the clinical equivalent of unit tests, which might be things like: patient's blood pressure normalized, infection resolved, no adverse events occurred, etc.

Moreover, coding benchmarks illustrate the value of sandboxed evaluation. When running AI-generated code against tests, it's done in a safe sandbox environment to prevent harmful operations. Similarly, a healthcare AI's decisions (especially if it's an agent that can place orders or send messages) should be tested in a sandbox hospital environment. For example, an AI ordering medications in a sim EHR – we evaluate what it would have done, without risking real patients. This concept is starting to emerge in "virtual hospitals" for training clinicians; we can repurpose it for AI.

Finally, coding copilots highlight efficiency and style metrics. It's not only did the code work, but was it efficient or overly convoluted? Did it follow style guidelines? In healthcare, aside from clinical correctness, we might measure things like computational efficiency (did the AI use minimal queries/tools to reach an answer?) or communication quality (was the AI's explanation to a patient empathetic and clear?). These are softer metrics, but coding benchmarks show they can be quantified (linters for style, performance benchmarks for runtime). We can analogously quantify aspects of an AI's performance beyond pure accuracy, embedding those into our evaluation.

In summary, the software engineering world demonstrates how automated evaluation (unit tests), simulation (sandbox execution), and even reward-based fine-tuning can create robust AI assistants. Adopting these ideas, healthcare benchmarks can become more rigorous (with test-like criteria for each scenario) and more actionable (providing signals to improve models, not just score them).

## Proposing Next-Generation Benchmarks for Healthcare AI

Building on the insights above, we propose several next-generation benchmark frameworks that would significantly improve the evaluation of healthcare AI systems. These are conceptual at this stage, but each is informed by the successes in other domains and addresses the gaps identified.

### 1. Retrieval-Grounded APEX: Dynamic Knowledge and Provenance

**What it is:** An extension of the APEX concept where the AI must acquire and cite evidence as part of solving medical tasks. Instead of handing the model all the relevant sources, the benchmark would provide a large knowledge repository (e.g. a dump of medical literature, guidelines, patient records) and require the AI to retrieve the information it needs. For example, a task might ask, "Advise on a treatment plan for a patient with rare condition X who failed first-line therapy," without providing the treatment guidelines. The model would need to search a document corpus (maybe a simulated UpToDate or research database) to find relevant advice before composing its answer.

**How it works:** Evaluation would have two components: the quality of the final answer (as in APEX, via expert rubric or LM grading) and a provenance score. Provenance could be scored by checking if the model's claims are supported by the sources it retrieved and cited. Essentially, did it find the right evidence, and can it point to it? The benchmark could use an automated check for citation correctness: if the model says "According to Study Y..." or provides a reference, is that reference actually relevant and accurate? Models that produce answers without sources or with hallucinated citations would be penalized. This is analogous to open-book QA evaluations, but with a heavy emphasis on proper sourcing – akin to how academic writing is judged.

**Why it's useful:** This tests information-seeking behavior critical for real deployments. In the wild, a medical AI should know its limits and fetch external info (drug dosages, new trial results) rather than rely solely on internal knowledge. By evaluating retrieval, we encourage development of models that integrate with databases and the web. It also inherently tests for reduced hallucinations – an AI that's forced to back up what it says will likely be more truthful. Essentially, Retrieval-grounded APEX measures not just "Can the model solve this problem?" but "Can it show its work with evidence?" – a key requirement for trust in healthcare. This benchmark could even incorporate a metric for efficient search: models get a limited number of queries or a time limit, rewarding those that find necessary data with minimal cost (similar to how we might reward fewer steps in a robotics task).

**Example scenario:** An AI is asked to write a brief on "Emerging therapies for Alzheimer's suitable for a patient with kidney impairment." The knowledge base contains journal articles and FDA guidelines. The AI must search through it, identify that certain drugs are contraindicated in renal failure, find an alternative therapy in a recent study, and then formulate a recommendation citing that study. The scoring would reward correctness and completeness of the recommendation (did it cover emerging options? address kidney issues?) and also check that the recommendation cites the study that indeed supports the chosen therapy. If the AI fails to retrieve any sources, that would severely hurt its score. If it retrieves sources but ignores a key piece of evidence (like a warning in guidelines), that's a deduction. This setup mirrors a clinician consulting references and justifying a plan - a strong sign of real-world readiness.

### 2. WONDERLAND: Interactive EHR Workflow Benchmark

**What it is:** WONDERLAND is envisioned as the healthcare counterpart to WONDERBREAD – an interactive, executable sandbox environment for end-to-end clinical workflows. It would simulate a hospital or clinic's digital ecosystem (EHR, billing system, pharmacy system) and present complex tasks that require an AI agent to navigate these systems. Essentially, WONDERLAND would be a virtual hospital sandbox where AI "agents" can perform tasks like ordering labs, scheduling appointments, filling out billing codes, updating chart notes, and communicating between departments.

**Structure:** The benchmark would include various scenarios drawn from real healthcare operations. For example: - **Electronic Health Record (EHR) Task:** “Place a telesitter order for a patient with fall risk in the EHR" – similar to the workflow demoed by Wornow et al. with ECLAIR [28, 29]. The AI has to go through the steps a nurse would: open patient record, navigate to orders, fill in the correct form fields, and confirm. - **Revenue Cycle (RCM) Task:** “Submit an insurance prior authorization for an MRI and follow through until approval." This might involve filling a form, writing a justification letter, responding to a clarifying question from the insurance (simulated by the environment), and updating the patient record. - **Cross-system Task:** “Discharge a patient and arrange home healthcare services including equipment and follow-up appointments." The AI might have to coordinate between the hospital EHR, a pharmacy system for medications, and an outside home health scheduling system.

WONDERLAND would be multimodal and interactive. The AI agent would see a UI (which could be represented as text descriptions of the interface or actual rendered images for a vision+text model). It would issue actions like clicks or keystrokes or API calls to accomplish tasks. Essentially, this benchmark brings the AI off the page of static text and into a simulated computer environment.

**Evaluation:** Success in WONDERLAND is measured by task completion and efficiency. Did the AI successfully achieve the goal (order placed, authorization approved, discharge packet completed) without errors? Metrics could include: - Completion rate (percentage of tasks fully completed correctly). - Time or steps taken (an efficiency metric - fewer steps or quicker completion is better, up to a point). - Error rate (e.g., attempted to click wrong UI element, entered invalid data - akin to hitting obstacles). - Perhaps score modifiers for quality: for instance, if the AI had to draft a justification letter to the insurer, that letter can be evaluated for persuasiveness and completeness by a language model or human (embedding some APEX-like rubric for sub-components).

Because this is a sandbox, it's possible to simulate things going wrong - e.g., a form submission might get rejected and the AI needs to handle it. Evaluation would also cover robustness: does the AI recover from an unexpected prompt (like a pop-up alert in the EHR)? We could borrow the concept of "mean time to failure" from software testing - how long can the AI operate without a mistake that stalls progress or requires human intervention.

**Why it's useful:** This benchmark directly measures what many healthcare AI startups ultimately want: an AI that can do the button-clicking drudgery in the EHR or billing systems (the administrative burden). By testing in a realistic environment, we uncover integration issues - a model might be great at planning but fails at the precise syntax needed to fill a form. It also tests multi-step autonomy: can the AI carry a task from start to finish, maintaining context? Importantly, it's a safe way to evaluate potentially risky actions (like ordering a medication) because it's all simulated. If the AI tries something unsafe (like ordering a contraindicated drug), the sandbox can flag it and that becomes a scored error without harming a real patient.

WONDERLAND would push AI to demonstrate it can be a workflow agent, not just a medical Q&A bot. Success on this benchmark would signal an AI is ready to automate real hospital processes (with appropriate oversight). Additionally, the data from such benchmarks (success/failure logs, strategies taken) could guide interface design improvements - e.g., if AI consistently struggles with a certain EHR form, maybe that form is too complex even for humans and needs redesign.

In implementation, WONDERLAND could leverage existing open-source EHR simulators or create simplified versions of Epic/Cerner interfaces. The tasks should cover a range of difficulty and involve realistic sequences of actions. Collaboration with healthcare IT professionals would be needed to define these workflows and acceptable variations (since in real life, multiple paths can achieve the same goal in software).

### 3. MedPI++: Stochastic Patients and Longitudinal Evaluation

**What it is:** MedPI++ is an enhanced version of the MedPI framework that incorporates stochastic, longitudinal scenarios with an emphasis on safety. Rather than single static cases, MedPI++ would simulate patient cases that evolve over time or vary in presentation. This brings an element of unpredictability and continuity, akin to managing a real patient panel or cohort over multiple visits.

**Key features:** - **Stochastic Patient Simulator:** MedPI++ would include a parameterized patient model. For instance, a hypertension management scenario might randomize initial blood pressure, co-existing conditions, and how the patient responds to medication. On each run, the scenario is slightly different (like rolling dice on whether the patient has a side effect or not). This ensures models aren't simply memorizing a fixed case, and it tests adaptability. We can build these stochastic elements based on clinical probabilities (e.g., 10% chance the patient is non-compliant with meds, 5% chance they develop a complication, etc.). - **Longitudinal Multi-turn Interaction:** Instead of one prompt and answer, the benchmark would proceed in turns or stages. For example, Turn 1: given initial visit info, the AI gives a diagnostic impression and plan. Then the simulator (or evaluator) provides follow-up info: say the patient returns in 2 weeks with new symptoms or lab results (Turn 2). The AI must adjust its recommendations. This could go on for several turns, covering initial presentation to outcome. The model's performance is measured across the whole timeline - did it eventually converge to the correct diagnosis? Did it modify treatment appropriately when circumstances changed? - **Safety-First Scoring:** MedPI++ would explicitly integrate safety into the score. Certain critical mistakes (like prescribing a drug the patient is allergic to, or failing to address "red flag" symptoms) would incur large penalties, much more than in current rubrics. Conversely, if the AI appropriately says "I'm not certain - refer to specialist" or chooses to abstain from a risky guess, it could earn a positive reward for that prudent action. Essentially, we bake in the idea that knowing when to stop or seek help is a skill to be rewarded, not just seen as an inability. We might implement this by having "bonus" criteria in rubrics for appropriate caution and by assigning, say, a -100% score to any case where a truly dangerous error is made (zeroing out the reward to reflect an unacceptable outcome).

**Evaluation:** Each case scenario would result in an aggregate score that combines multiple aspects: - **Clinical outcome:** did the patient get better, or was the correct diagnosis reached? (In simulation, we can define an outcome metric). - **Process quality:** did the AI follow standard of care steps? E.g., if guidelines say to test for X, and the AI did so, that's a point. - **Safety events:** count of any critical errors or near misses. - **Efficiency:** did the AI avoid unnecessary tests or overly long dialogues?

Because of stochasticity, we wouldn't judge a model on a single run; we would run multiple variations and look at average performance and worst-case performance. For instance, a model might handle routine cases well but fail badly if the patient has an uncommon complication – that should be visible in the evaluation (perhaps via a minimum score or a variance analysis across simulations).

**Why it's useful:** MedPI++ pushes evaluation closer to in vivo testing of a clinical assistant. It answers questions like: Can the AI manage a patient over time? Does it revise its hypotheses when new info arises (key for real diagnostic work)? Is it resilient to small changes in case details or does it break if things aren't exactly like training data? By randomizing aspects, we prevent overfitting and gauge generalization. The emphasis on longitudinal care also allows evaluating how the AI handles cumulative effects - e.g., after several decisions, did it cause the patient to deteriorate or improve? This is something static one-step benchmarks cannot measure.

Safety-first scoring is crucial for aligning model development with clinical values. If model teams know that a single unsafe recommendation can ruin their score, they will prioritize caution in design (e.g. more conservative outputs, better checking mechanisms). Over time, this can shift the culture from "the model must always answer” to “the model must first be safe.” In many ways, MedPI++ would operationalize the medical maxim "first, do no harm" into the evaluation metric itself.

**Example:** A MedPI++ scenario could simulate managing a diabetic patient. Turn 1: patient with high blood sugar, AI recommends starting a medication and lifestyle changes. Turn 2: follow-up, patient reports new symptoms of foot pain and numbness, AI should check for neuropathy or vascular issues. Turn 3: an ulcer develops, AI should refer to a specialist or suggest aggressive treatment. If at any turn the AI suggests something dangerous (like an incorrect insulin dose that would hypoglycemia) it's heavily penalized. If the AI appropriately adjusts treatment at each step and the ulcer heals in the simulation, it scores high. This measures not only knowledge, but the continuity of care – an essential real-world aspect.

### 4. Cross-Organizational Workflow RL Environments: Payer-Provider-Pharmacy Loops

**What it is:** This is a forward-looking benchmark concept where we simulate an entire healthcare ecosystem with multiple agents and roles. Think of it as a multi-agent reinforcement learning environment representing, for example, a doctor's office, an insurance company, and a pharmacy interacting to solve a patient's case. The AI (or multiple AIs) would take on certain roles in this environment, and the benchmark evaluates the outcomes of their interactions over a series of tasks or episodes.

**Structure:** In a cross-organizational simulation, we define agents such as: - **Provider agent** - making clinical decisions, ordering treatments. - **Payer agent** - evaluating claims, approving or denying requests based on policy and cost. - **Pharmacy agent** - dispensing medications or suggesting generic alternatives. (Optionally, a **Patient agent** - which could be a relatively simpler model or rule-set with preferences like wanting lower costs, or stopping medication if side effects are too bad, etc.)

The environment would have a state encoding relevant information (patient health status, insurance rules, drug formularies, etc.). Agents exchange messages or actions: e.g., Provider submits a treatment plan to Payer, Payer either approves or requests justification, Pharmacy might respond with availability or substitute drug suggestions, Provider can revise the plan, etc. This loop continues until the patient's issue is resolved (or not).

This essentially creates a micro-simulation of the healthcare delivery process. Each episode could correspond to managing one patient's scenario from initial visit to final outcome, with all the administrative back-and-forth included.

**Evaluation:** We would define a reward function for the overall outcome and possibly individual rewards for each agent if they are separate AIs. For instance: - **The patient outcome** could be a primary reward (did the patient get appropriate care and improve?). - **Cost and efficiency** could also factor in (was the cost to payer minimized without harming outcome? was care delivered in a timely manner?). - **Compliance and satisfaction:** Did the interactions meet all necessary approvals (no insurance fraud or errors)? Is the patient (simulated) satisfied (e.g., not too many delays or out-of-pocket costs)?

We might aggregate these into a single score for the episode or keep a vector of metrics (which regulators might prefer to see trade-offs).

Crucially, this environment allows testing scenarios like: - **Prior authorization challenges:** Does the AI (as provider) know how to navigate when an insurer initially denies an expensive MRI? A good AI might automatically provide additional justification or suggest an alternative test that is more likely to be approved, whereas a poor one might just give up (failing the patient) or order it anyway (patient gets stuck with a bill or delay). - **Formulary management:** If a drug isn't covered, does the AI work with the pharmacy agent to pick an equivalent that is covered, or get an exception made? - **Multi-agent safety checks:** Perhaps the pharmacy agent could catch an unsafe prescription and send an alert; we can evaluate if the provider agent appropriately responds (e.g., "pharmacy says high dose – will adjust dosage" vs. ignoring it).

This kind of benchmark is very ambitious, essentially creating a game environment for healthcare delivery. It could initially be implemented in a simplified way (maybe using reinforcement learning frameworks like OpenAI Gym or PettingZoo for multi-agent, with simplified rules).

**Why it's useful:** It evaluates something we currently completely miss: coordination and negotiation skills of AI in healthcare processes. Real-world outcomes often depend not just on one doctor's decision, but on bureaucratic approvals and logistics. If an AI doctor is to be truly effective, it must handle these realities. A model could be brilliant medically but useless if it can't navigate insurance rules or collaborate with the pharmacy - this benchmark would reveal that. It also forces consideration of multiple objectives (clinical vs financial). For example, maybe the AI can get the patient a very expensive drug that cures them - clinically optimal but bankrupts the system - versus a slightly less effective but affordable therapy that the insurer prefers. The benchmark environment can be tuned to reward an optimal balance (like a modest cost with good outcome).

Additionally, this multi-agent sim would allow testing adversarial or conflicting goals: what if the payer agent's policy is a bit adversarial (to save cost)? The AI provider might need to advocate strongly (as human doctors do in peer-to-peer insurer calls). We can see if the AI can "persuade" or strategically present information to get approval - a high-order capability.

This benchmark is akin to a complex role-playing game for AI, where success is measured in patient health and system efficiency. It would likely yield a wealth of data on where AI negotiations or planning fail. For example, the AI might learn to always choose the cheapest option to avoid hassle - but then patient outcomes suffer. If we catch that in evaluation, we know to refine the reward or constraints (just as in multi-agent economic simulations, you don't want agents exploiting loopholes like printing money; in healthcare AI you don't want it gaming the system at patient expense).

Implementing this would probably involve simplified models for each party at first (maybe the benchmark provides rule-based payer/pharmacy agents that follow predetermined logic), and we measure the AI in the provider role. Over time, one could even test multiple AIs in each role (imagine an AI insurer too) and evaluate system-level outcomes, which edges into health economics and policy. But even the simpler case of a fixed-rule insurer and testing an AI doctor agent's ability to get things approved would be very illuminating.

In summary, the cross-organizational RL benchmark would push AI to operate within the constraints and frictions of the real healthcare system, not just idealized doctor-patient interaction. It's about making sure an AI can deliver value in context, navigating rules and collaborating, which is the next frontier for truly deployable healthcare AI.

## Designing Robust Reward Functions for Evaluation

Creating next-gen benchmarks is only half the battle – we also need to design robust reward and scoring functions that drive the right behaviors. A good reward function in this context is one that accurately reflects success in a healthcare task and discourages undesirable behavior. Below we discuss how to combine multiple criteria into evaluation rewards, drawing from rubric-based scoring, provenance checking, safety constraints, and efficiency metrics:

*   **Multi-Criteria Rubric Balancing:** In complex tasks, a single score often hides important nuances. A rubric (like in APEX) breaks performance into criteria (accuracy, completeness, reasoning quality, etc.). For evaluation purposes, we may combine these into an overall reward, but it's crucial to weigh them appropriately. One approach is a weighted sum of rubric elements, reflecting their importance. For example, correctness might be 50% of the score, reasoning clarity 20%, completeness 20%, and format/style 10%. We must be cautious that the weights truly mirror priorities - likely, safety and correctness criteria should have the highest weights. Another approach is to use a minimum criterion threshold: require that certain critical criteria (like “no dangerous advice") are passed, otherwise the overall reward is severely reduced regardless of other aspects. This prevents a model from, say, getting 8/10 on many minor points but failing a major point and still scoring decently. The balancing should also consider correlation between criteria; if a model is very wrong, it might automatically fail multiple rubric items, doubling-penalizing. Clustering or a non-linear reward (like geometric mean) can mitigate that by not overly counting correlated failures. Ultimately, combining rubric criteria in evaluation is about reflecting holistic competence – we want the reward to incentivize models that are consistently good across all important dimensions rather than excelling in one and ignoring another.

*   **Provenance and Citation Rewards:** As proposed, if an AI is required to provide sources, we can bake provenance checking into the reward. For instance, each claim in an answer that is backed by a correct citation could earn a point, and each unsubstantiated claim could lose points. We might use automated tools or another AI to verify that citations indeed support the text. Moreover, we can have a binary "pass/fail" on a provenance criterion: e.g., no hallucinated references is required to pass. If any cited source is fabricated or irrelevant, the model could get a zero on the evidence score (since in medical contexts, a fake citation is a serious breach). On the positive side, we could reward models for using fewer but higher-quality sources (conciseness in evidence) to discourage dumping a bibliography of random papers. Provenance scoring might also account for how recent or authoritative the sources are, encouraging models to pull from trusted guidelines or latest research. By including these factors in the reward, we encourage models that don't just answer correctly, but demonstrate accountability for their answers - a trait likely to increase trust among clinicians.

*   **Hard Penalties for Unsafe Outputs:** In healthcare AI evaluation, certain outputs should trigger an immediate and significant penalty. For example, recommending a dose that would clearly be toxic, or an intervention contraindicated for the patient, or exhibiting biased/offensive behavior. The reward function can encode this as a large negative reward (or zero out the score for that case). Essentially, unsafe outputs are treated as absorbing failures – like a robot falling off a cliff in an RL environment yields a large negative reward. By doing so, we make it clear that no amount of positive performance elsewhere can compensate for a critical safety lapse. This mirrors how in real clinical evaluation, a single fatal mistake overshadows a dozen good calls. Technically, we might implement this as a set of rules or detectors that scan the model's output for certain red flags (e.g., mention of a lethal dosage, or a phrase like "no need to monitor for side effects" when that's dangerously wrong). If triggered, the scoring function could set reward = -1 (if we allow negative) or assign a very low score for that scenario. One has to be careful to calibrate these – we want to catch clear-cut unsafe actions, not penalize a model for minor issues. In development, one could build a library of "never events" (analogous to what hospitals do for quality control) that if present in output, result in hard penalties. This approach creates a deterrent against reckless AI behavior: model developers will try to avoid tripping those rules at all costs, ideally by training models to recognize and steer away from those dangerous zones.

*   **Abstention and Uncertainty Rewards:** Counterintuitively, sometimes the best action of an AI is to do nothing or defer. We should explicitly reward appropriate abstention. For instance, if a model outputs "I'm not confident - this should be handled by a specialist" in a case that is truly beyond its scope, that should be scored higher than if it made a random guess. Implementing this can be tricky because we don't want to encourage the model to abdicate responsibility on every tough question (an AI that always says "ask a human" is safe but not useful). One method is to introduce an "abstain option" in the benchmark: the model can choose a special output like `<DEFERRAL>` and then the scoring treats that as correct if indeed a correct solution is not expected of the model at that point. For example, in MedPI++, maybe if a patient develops a very complex condition, deferring is acceptable. We might give partial credit for deferral – say 50% of the points - whereas giving a dangerous wrong answer would be 0%. That creates a hierarchy: correct answer = 100, safe deferral = 50, unsafe attempt = 0. Over many scenarios, a model that wisely defers only when truly necessary will score well, whereas one that over-defers will miss out on points, and one that under-defers will accumulate zeros from dangerous errors. The key is calibrating it so that judicious use of abstention is rewarded, but abuse is discouraged. Additionally, we can encourage expressions of uncertainty where appropriate - e.g., in rubrics, give a point if the model notes lack of information or suggests confirming a finding instead of plowing ahead blindly. This fosters calibrated models that communicate uncertainty (which is what clinicians do) rather than those that are confidently wrong.

*   **Efficiency and Cost Metrics:** In real workflows, especially multi-step ones, time and resource usage matter. Our benchmarks can incorporate efficiency by adding mild penalties for excessive actions or delays. For instance, in an interactive environment like WONDERLAND, every additional step or query the model makes could subtract a small amount from the reward (to simulate time/ cost). If two models both complete a task correctly but one did it in 5 steps and the other in 50, the one with 5 steps should score higher. This encourages optimization and more human-like efficiency. Similarly, in the multi-agent environment, if an AI chooses a path that incurs much higher cost (e.g., ordering a battery of unnecessary tests), the reward could be lower even if the patient outcome is okay. We have to balance this with not discouraging necessary thoroughness - i.e., we penalize waste, not proper due diligence. One way is to set a threshold of "expected steps" or "standard of care cost" and only penalize beyond that. We can also introduce an “episode length" penalty in RL terms: shorter successful episodes get a bonus. Efficiency metrics should remain secondary to correctness and safety (we'd never want a model to skip needed steps just to be faster), so these penalties might be relatively small or only come into play when comparing models of equal safety/ accuracy. Over time, however, including such metrics ensures that as models get more capable, they also learn to be practical - offering solutions that save time for healthcare staff and reduce unnecessary expenditures.

Designing the reward function really comes down to aligning with human values and priorities in healthcare: safety, effectiveness, explainability, efficiency. By mixing these elements in the right proportions, we create a guiding signal for AI development. Indeed, there is precedent in AI research for using composite rewards. For example, in code generation, one might use a weighted reward of "passes tests + shorter code length" to get concise correct solutions [26]. In our case, we might combine "clinical correctness + safety adherence + resource optimality".

Finally, to make reward functions robust, it can help to involve human experts in the loop initially to sanity-check the scoring. Perhaps a panel of clinicians can review a sample of outputs and the automated score, to see if it truly reflects their judgment of what is better. This kind of validation can prevent mis-weighting that would otherwise incentivize weird model behaviors.

In summary, a robust reward function in healthcare AI benchmarks will likely be a multi-term formula, something like:

$$ \text{Score} = w_1 \text{(Outcome Quality)} + w_2 \text{(Process/Rubric Score)} + w_3 \text{(Evidence/ Provenance)} - w_4 \text{(Unsafe Penalties)} - w_5 \text{(Inefficiency Penalties)}, $$

with logic in place to handle abstentions and critical fails. Tuning those weights and rules is non-trivial, but the effort is justified by the end result: an evaluation that truly rewards the kind of AI behavior we want in healthcare and heavily discourages what we don't want. This will guide developers to train AI models that excel under these comprehensive metrics, ultimately producing safer and more useful systems.

## Practical Roadmap for Adoption

Creating improved benchmarks is only useful if they are adopted by the community - including AI developers, healthcare institutions, and regulators. In this section, we outline how these stakeholders can collaborate to implement the next-gen benchmarks, and we address potential risks and mitigations during the rollout.

### Adoption by Startups and Health Systems

Startups and AI vendors should see these benchmarks as a chance to differentiate and build trust. Concretely, a healthcare AI startup could start using a suite of these advanced benchmarks internally to evaluate each new model update. By doing so, they can track improvements not just on academic test sets, but on realistic tasks like "completion of prior-auth workflow" or "safe handling of longitudinal case". Startups can publish their models' benchmark results (similar to how AI labs publish MMLU or MedQA scores today) to signal their product is closing real-world gaps. This is analogous to car companies advertising safety ratings - a startup might say, "Our model scores 95% on MedPI++ with zero critical errors, and completes 90% of tasks in WONDERLAND autonomously." For health systems (hospitals, clinics) looking to procure AI solutions, these benchmarks provide objective metrics to compare products. A hospital CTO could ask vendors: “Has your virtual assistant been evaluated in a simulated EHR environment? How does it perform on complex coordination tasks?" If one product has been through WONDERLAND and another hasn't, the choice becomes clearer.

Health systems themselves can contribute by providing realistic scenarios and data (de-identified) to enhance benchmarks. For instance, a consortium of hospitals might define the top 10 administrative pain points to encode in WONDERLAND. By doing so, they ensure benchmarks test what matters to end-users. Hospitals could also require in contracts that AI tools pass certain benchmark thresholds before being allowed on real data, akin to a credentialing step.

There's also room for open benchmarking platforms - perhaps an online leaderboard (similar to NLP leaderboards) where models can be evaluated on these complex benchmarks in a standardized way. Startups could submit model APIs to be tested on hidden cases in MedPI++ or cross-org environments, with results posted publicly. This would drive competition not just on who has the largest model, but who has the most robust, workflow-savvy model. Importantly, it can stimulate innovation in the areas we identified as weak (retrieval, tool use, etc.), as companies will attempt novel approaches to climb the rankings.

### Role of Regulators and Policy

Regulators (like the FDA in the US, or EMA in Europe) are increasingly focusing on AI in healthcare. Improved benchmarks can serve as a bridge between technical metrics and regulatory requirements. Regulators could incorporate these benchmarks into guidance documents or even approval processes. For example, an FDA submission for a clinical AI might need to include results on a standardized evaluation suite (covering safety tests, etc.), similar to how clinical trials are required for drug approval. While regulators won't solely rely on sim benchmarks (they will always want some real-world validation), showing strong performance in a well-designed benchmark can expedite the process by demonstrating due diligence in risk mitigation.

We could see the emergence of an independent evaluation body or consortium - perhaps a partnership between medical societies and AI standards organizations - that regularly updates and administers these benchmarks. Think of it like Underwriters Laboratories (UL) for AI, or the ISO standards. Such a body could maintain hidden test scenarios (to prevent gaming) and certify models that meet threshold performance. Regulators could then say, “We strongly recommend using certified models that have passed the XYZ benchmark suite for clinical tasks." While not outright mandating it, this creates pressure in the market to comply.

Policymakers might also fund the development of these benchmarks. Given the public interest in safe AI, government grants or public-private partnerships could support building the simulation environments and scoring infrastructure. For instance, a national digital health agency could host a “Virtual Hospital Challenge" where companies test their AIs in increasingly complex simulated settings (similar to how DARPA runs autonomous challenges). The outcome of these events can guide policy on what level of autonomy is permissible or where to draw the line for requiring human oversight.

One must be mindful that regulators will ask: do these benchmarks truly correlate with real-world outcomes? As part of the roadmap, it will be important to validate benchmark performance vs. real performance. For example, if a model consistently passes simulation tests, does it also do well in limited live trials? We should design studies to confirm this link, which will in turn give regulators confidence that benchmarks are meaningful.

## Risks and Mitigations

**Risk 1: Benchmark Overfitting.** Once benchmarks become a "test to pass", there's a risk companies will tailor their models too specifically to the benchmark, possibly exploiting quirks without genuinely being better in practice. This is akin to students cramming for a test versus truly learning the material. To mitigate this, the benchmarks themselves must be broad, evolving, and difficult to game. Strategies include: - **Large scenario pools:** Ensure there are enough varied cases that models must generalize, not just memorize answers. If MedPI++ has hundreds of patient cases with randomization, overfitting to all of them is as hard as actually learning medicine. - **Regular updates:** Periodically add new scenarios or change the environment. For example, each year introduce a few new rare conditions or workflow changes reflecting current practice. This keeps developers on their toes and prevents stale optimization. - **Hidden test sets:** Just as in ML competitions, keep some evaluation scenarios secret. A model shouldn't know the exact scenario beforehand. If a company is internally testing on a known set, the public/official eval could use a held-out set. This way, true performance and adaptability are measured. - **Evaluator ensembles:** Use multiple methods to score (human experts, AI judges, automatic metrics) so that optimizing to one scoring mechanism doesn't guarantee success unless the model truly is good. For instance, if a company somehow optimizes to fool an LM judge, a parallel human review on samples could catch disparities [9]. Ensembles of evaluators reduce single-point gaming.

**Risk 2: Simulation-to-Reality (Sim2Real) Gap.** No matter how detailed the simulator, reality will always have differences. A model might ace the sandbox (e.g. WONDERLAND) but struggle with the idiosyncrasies of a real EHR interface or real patient behavior. To narrow this gap: - **High-fidelity simulation:** Invest in making the simulation as realistic as possible. Domain randomization, as discussed, helps the model not rely on specific simulator artifacts [18]. Including real-world variability (typos in records, patients who don't follow instructions) in the sim can prepare models for those in reality. - **Pilot testing in controlled real settings:** Before full deployment, test the AI in small, monitored real-world trials. Compare its performance to the sim predictions. If discrepancies arise, analyze them and incorporate those situations back into the simulation or benchmarks as new test cases. Essentially, continuously loop between sim and real to refine the former. - **Human fallback mechanisms:** In reality, even if a model is benchmark-certified, initial deployments should include human oversight who can catch issues the sim didn't. Over time, as confidence builds, the oversight can lessen. This phased approach ensures that any sim2real gaps that do appear don't cause harm. - **Transparency about limitations:** Encourage reporting of where the model struggled in sim - e.g. "the model had difficulty with prior-auth scenarios involving multiple appeals." This might hint that in real life it could get stuck too. Users can then be cautious in those contexts, and developers can focus on improvements there.

**Risk 3: Regulatory Misalignment.** If benchmarks are developed purely by technologists, they might emphasize certain metrics that regulators or clinicians don't ultimately care about, or vice versa. For instance, our benchmark might score cost efficiency, but a regulator might say patient safety is the only concern and be wary that optimizing cost could harm care. Alternatively, a regulator could require evidence that isn't captured by current benchmarks (like bias/fairness across subpopulations). Mitigation includes: - **Involve diverse stakeholders in benchmark design:** From the outset, include clinicians, patient representatives, and regulators in the advisory group for these benchmarks. This ensures the criteria and weights align with societal values and policy needs. - **Modular scoring:** Make benchmarks flexible such that different users (regulator vs hospital vs developer) can extract the metrics they care about. For example, the evaluation could output a breakdown: safety score, efficacy score, efficiency score, bias score, etc. A regulator might only consider safety and efficacy portions when approving, ignoring efficiency if that's not their purview. - **Policy sandbox:** Work with regulators to treat initial use of these benchmarks as a learning exercise, not a strict standard. For instance, in early days, an agency might say "Report your benchmark results, but we won't reject approval solely on them until we gather enough data on correlation with real outcomes." This non-binding phase allows alignment to be assessed and the benchmark adjusted if needed. Over time, if it's proven that, say, 100% MedPI++ score always translates to great clinical performance, then it can become a requirement. - **Fairness and equity checks:** To align with regulatory and ethical priorities, benchmarks should include scenarios representing diverse patient demographics and conditions. If our benchmarks inadvertently only test on Western male patient data, models could be unsafe elsewhere, causing regulatory concern. Building diversity into the benchmark cases mitigates this and aligns with broader health equity goals.

Beyond these specific risks, another general risk is resource and complexity overhead. These new benchmarks are more complex to run than answering a list of questions. This could slow down model development or exclude smaller players. Mitigation: create scaled versions of benchmarks (like easy, medium, hard) - maybe a subset of scenarios for quick iteration and the full suite for final evaluation. Also, share the benchmark environments as open tools so companies don't have to reinvent them. If a startup can easily plug their model into an open-source WONDERLAND simulator, they're more likely to use it regularly.

Finally, community governance is key. The benchmarks should be maintained by a neutral party or consortium to ensure credibility. And there should be channels for feedback - e.g., if a model finds a loophole or a stakeholder notices a missing important scenario, the benchmark can be updated. Think of it as an evolving standard, much like medical guidelines evolve - staying current with what we learn about AI behavior in the wild.

In conclusion, the roadmap to adoption involves collaboration between innovators and regulators, transparency in results, and iterative refinement. By proactively addressing overfitting, sim gaps, and alignment with real-world values, we can ensure these benchmarks actually serve their purpose: being reliable predictors of an AI's fitness for healthcare work. Over time, widespread use of these evaluations will raise the bar for all AI systems entering healthcare, much as crash tests and safety ratings did for the auto industry.

## Conclusion

Healthcare AI is poised to progress from answering exam questions to orchestrating entire clinical workflows. To guide this evolution safely and effectively, our evaluation methods must undergo a similar transformation - from static tests to dynamic, comprehensive benchmarks. The frameworks discussed - inspired by APEX, MedPI, WONDERBREAD and fortified with lessons from autonomous vehicles, robotics, finance, and software - represent a pathway for healthcare AI evaluation:

*   **Stage 1: Static Expertise (Yesterday).** Benchmarks like medical board exams or basic QA measured isolated knowledge recall. This was a necessary starting point (analogous to testing a human student's book knowledge), but it's no longer sufficient [10]. A model passing USMLE is now the "bare minimum" [10].
*   **Stage 2: Contextualized Tasks (Today).** APEX and MedPI bring us into the era of evaluating AI on realistic deliverables and decision-making within a case. They are like giving an AI a take-home assignment that a professional might do. WONDERBREAD goes further to assess process understanding. These benchmarks start aligning AI performance with economically and clinically valuable tasks [14, 11]. However, they largely remain offline and single-player.
*   **Stage 3: Workflow and Interaction (Emerging).** The next-gen benchmarks we propose (Retrieval-grounded tasks, WONDERLAND, MedPI++, etc.) push into interactivity and realism. Here the AI is tested in simulated environments and multi-turn interactions that mimic real workflows. This is similar to how a residency program evaluates a doctor on rotations, not just written tests - we'd be evaluating the AI in a virtual internship of sorts. The inclusion of retrieval, tools, and multi-agent elements ensures the AI can actually function in the connected, digital, teamwork-heavy context of modern healthcare.
*   **Stage 4: Autonomous and Safe Operation (Future).** Ultimately, interactive RL environments and multi-agent simulations pave the way to evaluate fully autonomous AI systems in healthcare scenarios. This is the equivalent of a driving test for AI: can it navigate a complex course with unpredictable events and still reach a safe outcome? By importing best practices from self-driving (like extensive scenario simulation and safety monitors) [30], from robotics (massively parallel training and testing) [20], and from finance (adversarial robustness) [22, 17], healthcare can leapfrog into this stage relatively quickly. We don't need to reinvent the wheel – we can adapt the "simulator + stress test + reward" approaches that have matured elsewhere, tailoring them to clinical needs.

This roadmap is ambitious but attainable. The technology to build rich simulations and automated evaluators exists; it's a matter of curating the right clinical content and standards. More importantly, the will to use them is growing – clinicians, seeing early AI errors, are calling for more rigorous vetting, and industry leaders know that proving real-world readiness is essential to gain user trust.

By implementing these improved benchmarks, we create a virtuous cycle: better evaluation drives better models, which enable safer deployment, which in turn builds trust and adoption, fueling further innovation. In essence, we shift the competition from "Who has the biggest model?" to "Who has the most clinically adept model?" - a far more meaningful race.

In conclusion, advanced healthcare AI benchmarks combining static knowledge testing, workflow understanding, and interactive simulation will be the foundation upon which credible, effective, and safe AI systems are built for medicine. They will allow us to identify models that are not just smart in theory, but street-smart (or clinic-smart) in practice. As venture investors or stakeholders, supporting the development and adoption of these benchmarks is an investment in the quality assurance infrastructure of the coming AI-driven healthcare revolution. It's how we ensure that the next generation of AI "doctors" and assistants are thoroughly road-tested – through virtual patients, clinics, and adversaries – before they ever meet their first real patient. This approach, leveraging cross-industry wisdom, can accelerate progress while avoiding pitfalls, ultimately helping healthcare leapfrog into an AI-enhanced future with confidence and safety.

---
## Sources:

*   Vidgen, B. et al. “The AI Productivity Index (APEX): Evaluating Frontier AI on Knowledge-Work Tasks" (arXiv 2509.25721) – introduces APEX benchmark covering law, consulting, banking, and primary care tasks with expert-created prompts and rubrics [14, 2].
*   Lumos Labs (2024). MedPI Whitepaper – proposes a Medical Performance Index for holistic evaluation of clinical AI beyond exam Q&A, emphasizing workflow context and expert-defined criteria (unpublished, summary derived from context).
*   Wornow, M. et al. “WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks" (NeurIPS Datasets & Benchmarks 2024) – details a 3,000-workflow dataset and tasks like workflow documentation and validation; highlights models' high recall (88%) but low validation F1 (<0.3) on process understanding [12, 13].
*   Wornow, M. et al. "Automating the Enterprise with Foundation Models" (VLDB 2024 / arXiv 2405.03710) – introduces ECLAIR system applying multimodal GPT-4 to RPA; demonstrates near-human 93% on workflow understanding and 40% end-to-end task completion from documentation alone, pointing to remaining gap for full autonomy [31].
*   Hazy Research Blog (Stanford) – “ECLAIR: A Treat for the Enterprise" (May 2024) – discusses limitations of rule-based RPA and the promise of foundation models; notes $4T productivity potential in admin workflows and questions trusting GPT-4 with complex tasks like coordinating surgery, motivating robust evaluation\* [32, 30].
*   Stanford Center for Research on Foundation Models - Nigam Shah interview on MedHELM (2025) – stresses that passing exams is not enough and 95% of reported LLM evals lack EHR context; describes creating 120 scenarios for clinical tasks to evaluate models in context, illustrating the need for domain-specific benchmarks [10, 33].
*   Mead, A. et al. "Detecting Fraud in Adversarial Environments: A Reinforcement Learning Approach" (U.Virginia, 2018) – frames credit card fraud as an MDP with a fraudster agent against a classifier; shows adaptive strategies and argues for moving beyond static models to adversary-aware evaluation [23, 22].
*   Ramakrishnan, J. "Sim-to-Real Autonomous Driving in CARLA using Domain Randomization" (MSc Thesis, 2022) – explains simulation-to-reality gap and domain randomization: by randomizing visual and environment parameters, the agent perceives the real world as just another variation of the simulator, improving robustness [18].
*   ElegantRL Documentation – Description of NVIDIA Isaac Gym: a GPU-accelerated physics simulator enabling thousands of parallel environments for training, achieving 2-3 orders speedup in continuous control tasks [20].
*   Le et al. (2022), Guo et al. (2024) – via Jun et al. (2025) “Learning to Generate Unit Tests via Adversarial RL" – note that unit tests are widely used as verifiable reward functions in RL for code generation, providing task-specific feedback for correctness [26].

---
1.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
2.  The AI Productivity Index (APEX)
3.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
4.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
5.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
6.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
7.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
8.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
9.  [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
10. Evaluating AI in context: Which LLM is best for real health care needs? [https://med.stanford.edu/news/insights/2025/04/ai-artificial-intelligence-evaluation-algorithm.html](https://med.stanford.edu/news/insights/2025/04/ai-artificial-intelligence-evaluation-algorithm.html)
11. WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks | OpenReview [https://openreview.net/forum?id=rZlLfa81D8&referrer=%5Bthe%20profile%20of%20Michael%20Wornow%5D(%2Fprofile%3Fid%3D~Michael_Wornow1)](https://openreview.net/forum?id=rZlLfa81D8&referrer=%5Bthe%20profile%20of%20Michael%20Wornow%5D(%2Fprofile%3Fid%3D~Michael_Wornow1))
12. WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks | OpenReview [https://openreview.net/forum?id=rZlLfa81D8&referrer=%5Bthe%20profile%20of%20Michael%20Wornow%5D(%2Fprofile%3Fid%3D~Michael_Wornow1)](https://openreview.net/forum?id=rZlLfa81D8&referrer=%5Bthe%20profile%20of%20Michael%20Wornow%5D(%2Fprofile%3Fid%3D~Michael_Wornow1))
13. WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks | OpenReview [https://openreview.net/forum?id=rZlLfa81D8&referrer=%5Bthe%20profile%20of%20Michael%20Wornow%5D(%2Fprofile%3Fid%3D~Michael_Wornow1)](https://openreview.net/forum?id=rZlLfa81D8&referrer=%5Bthe%20profile%20of%20Michael%20Wornow%5D(%2Fprofile%3Fid%3D~Michael_Wornow1))
14. [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
15. [https://arxiv.org/html/2509.25721v1](https://arxiv.org/html/2509.25721v1)
16. Microsoft Word - Capstone-AdversarialLearning-2018-SIEDSpaper.docx [https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf](https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf)
17. Microsoft Word - Capstone-AdversarialLearning-2018-SIEDSpaper.docx [https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf](https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf)
18. Sim-to-Real autonomous driving in CARLA using Image Translation and Deep Deterministic Policy Gradient [https://theses.liacs.nl/pdf/2021-2022-RamJ.pdf](https://theses.liacs.nl/pdf/2021-2022-RamJ.pdf)
19. Sim-to-Real autonomous driving in CARLA using Image Translation and Deep Deterministic Policy Gradient [https://theses.liacs.nl/pdf/2021-2022-RamJ.pdf](https://theses.liacs.nl/pdf/2021-2022-RamJ.pdf)
20. How to run worker parallelism: Isaac Gym - ElegantRL 0.3.1 documentation [https://elegantrl.readthedocs.io/en/latest/tutorial/isaacgym.html](https://elegantrl.readthedocs.io/en/latest/tutorial/isaacgym.html)
21. How to run worker parallelism: Isaac Gym - ElegantRL 0.3.1 documentation [https://elegantrl.readthedocs.io/en/latest/tutorial/isaacgym.html](https://elegantrl.readthedocs.io/en/latest/tutorial/isaacgym.html)
22. Microsoft Word - Capstone-AdversarialLearning-2018-SIEDSpaper.docx [https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf](https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf)
23. Microsoft Word - Capstone-AdversarialLearning-2018-SIEDSpaper.docx [https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf](https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf)
24. Microsoft Word - Capstone-AdversarialLearning-2018-SIEDSpaper.docx [https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf](https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf)
25. Microsoft Word - Capstone-AdversarialLearning-2018-SIEDSpaper.docx [https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf](https://api.dsi.virginia.edu/sites/default/files/attachments/2018-04/Detecting%20Fraud%20in%20Adversarial%20Environments%20-%20A%20Reinforcement%20Learning%20Approach.pdf)
26. Learning to Generate Unit test via Adversarial Reinforcement Learning [https://arxiv.org/html/2508.21107v2](https://arxiv.org/html/2508.21107v2)
27. Process Supervision-Guided Policy Optimization for Code Generation [https://arxiv.org/html/2410.17621v2](https://arxiv.org/html/2410.17621v2)
28. ECLAIR: A Treat for the Enterprise - Hazy Research [https://hazyresearch.stanford.edu/blog/2024-05-18-eclair](https://hazyresearch.stanford.edu/blog/2024-05-18-eclair)
29. ECLAIR: A Treat for the Enterprise - Hazy Research [https://hazyresearch.stanford.edu/blog/2024-05-18-eclair](https://hazyresearch.stanford.edu/blog/2024-05-18-eclair)
30. ECLAIR: A Treat for the Enterprise - Hazy Research [https://hazyresearch.stanford.edu/blog/2024-05-18-eclair](https://hazyresearch.stanford.edu/blog/2024-05-18-eclair)
31. Automating the Enterprise with Foundation Models [https://arxiv.org/html/2405.03710v1](https://arxiv.org/html/2405.03710v1)
32. ECLAIR: A Treat for the Enterprise - Hazy Research [https://hazyresearch.stanford.edu/blog/2024-05-18-eclair](https://hazyresearch.stanford.edu/blog/2024-05-18-eclair)
33. Evaluating AI in context: Which LLM is best for real health care needs? [https://med.stanford.edu/news/insights/2025/04/ai-artificial-intelligence-evaluation-algorithm.html](https://med.stanford.edu/news/insights/2025/04/ai-artificial-intelligence-evaluation-algorithm.html)