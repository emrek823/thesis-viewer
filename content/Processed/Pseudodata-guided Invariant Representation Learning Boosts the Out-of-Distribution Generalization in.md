---
title: Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution
  Generalization in Enzymatic Kinetic Parameter Prediction
authors:
- Haomin Wu
- Zhiwei Nie
- Hongyu Zhang
- Zhixiang Ren
date: '2026-01-12'
categories:
- cs.LG
- cs.AI
- q-bio.QM
pdf_url: https://arxiv.org/pdf/2601.07261v1
arxiv_id: 2601.07261v1
tags:
- paper
- alphaxiv/hot
- topic/cs-LG
- topic/cs-AI
- topic/q-bio-QM
---

# Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution Generalization in Enzymatic Kinetic Parameter Prediction

**Authors:** Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren

**Date:** 2026-01-12 | **Categories:** cs.LG, cs.AI, q-bio.QM

[PDF](https://arxiv.org/pdf/2601.07261v1) | [AlphaXiv](https://alphaxiv.org/abs/2601.07261v1)

## Abstract

Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering.However, existing deep learning-based enzyme-substrate interaction (ESI) predictors often exhibit performance degradation on sequence-divergent, out-of-distribution (OOD) cases, limiting robustness under biologically relevant perturbations.We propose O$^2$DENet, a lightweight, plug-and-play module that enhances OOD generalization via biologically and chemically informed perturbation augmentation and invariant representation learning.O$^2$DENet introduces enzyme-substrate perturbations and enforces consistency between original and augmented enzyme-substrate-pair representations to encourage invariance to distributional shifts.When integrated with representative ESI models, O$^2$DENet consistently improves predictive performance for both $k_{cat}$ and $K_m$ across stringent sequence-identity-based OOD benchmarks, achieving state-of-the-art results among the evaluated methods in terms of accuracy and robustness metrics.Overall, O$^2$DENet provides a general and effective strategy to enhance the stability and deployability of data-driven enzyme kinetics predictors for real-world enzyme engineering applications.

## Notes

