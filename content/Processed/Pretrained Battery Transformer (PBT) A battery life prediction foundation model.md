---
title: 'Pretrained Battery Transformer (PBT): A battery life prediction foundation
  model'
authors:
- Ruifeng Tan
- Weixiang Hong
- Jia Li
- Jiaqiang Huang
- Tong-Yi Zhang
date: '2025-12-18'
source: arxiv
categories:
- cs.LG
- cs.AI
pdf_url: https://arxiv.org/pdf/2512.16334v1
tags:
- paper
- source/arxiv
- topic/cs-LG
- topic/cs-AI
arxiv_id: 2512.16334v1
---

# Pretrained Battery Transformer (PBT): A battery life prediction foundation model

**Authors:** Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang

**Date:** 2025-12-18

**Source:** arxiv | **Categories:** cs.LG, cs.AI

ðŸ“„ [PDF](https://arxiv.org/pdf/2512.16334v1)

## Abstract

Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.

## Notes

<!-- Add your notes here -->
