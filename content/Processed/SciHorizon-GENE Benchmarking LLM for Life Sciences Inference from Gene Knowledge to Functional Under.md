---
title: 'SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge
  to Functional Understanding'
authors:
- Xiaohan Huang
- Meng Xiao
- Chuan Qin
- Qingqing Long
- Jinmiao Chen
- Yuanchun Zhou
- Hengshu Zhu
date: '2026-01-19'
categories:
- q-bio.GN
- cs.AI
- cs.CL
pdf_url: https://arxiv.org/pdf/2601.12805v2
arxiv_id: 2601.12805v2
tags:
- paper
- alphaxiv/hot
- topic/q-bio-GN
- topic/cs-AI
- topic/cs-CL
---

# SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding

**Authors:** Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen...

**Date:** 2026-01-19 | **Categories:** q-bio.GN, cs.AI, cs.CL

[PDF](https://arxiv.org/pdf/2601.12805v2) | [AlphaXiv](https://alphaxiv.org/abs/2601.12805v2)

## Abstract

Large language models (LLMs) have shown growing promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding, a core requirement for knowledge-enhanced cell atlas interpretation, remains largely underexplored. To address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric benchmark constructed from authoritative biological databases. The benchmark integrates curated knowledge for over 190K human genes and comprises more than 540K questions covering diverse gene-to-function reasoning scenarios relevant to cell type annotation, functional interpretation, and mechanism-oriented analysis. Motivated by behavioral patterns observed in preliminary examinations, SciHorizon-GENE evaluates LLMs along four biologically critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence, explicitly targeting failure modes that limit the safe adoption of LLMs in biological interpretation pipelines. We systematically evaluate a wide range of state-of-the-art general-purpose and biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning capabilities and persistent challenges in generating faithful, complete, and literature-grounded functional interpretations. Our benchmark establishes a systematic foundation for analyzing LLM behavior at the gene scale and offers insights for model selection and development, with direct relevance to knowledge-enhanced biological interpretation.

## Notes

