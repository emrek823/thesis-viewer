---
title: Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs
authors:
- Ziyu Zhou
- Haozhe Luo
- Mohammad Reza Hosseinzadeh Taher
- Jiaxuan Pang
- Xiaowei Ding
- Michael B. Gotway
- Jianming Liang
date: '2025-12-28'
categories:
- cs.CV
pdf_url: https://arxiv.org/pdf/2512.22872v1
arxiv_id: 2512.22872v1
tags:
- paper
- alphaxiv/hot
- topic/cs-CV
---

# Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs

**Authors:** Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding...

**Date:** 2025-12-28 | **Categories:** cs.CV

[PDF](https://arxiv.org/pdf/2512.22872v1) | [AlphaXiv](https://alphaxiv.org/abs/2512.22872v1)

## Abstract

Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

## Notes

