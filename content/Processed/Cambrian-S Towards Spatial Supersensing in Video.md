---
title: "Cambrian-S: Towards Spatial Supersensing in Video"
source: "https://arxiv.org/html/2511.04670v1"
published:
created: 2025-12-16
description:
tags:
  - "clippings"
---
HTML conversions [sometimes display errors](https://info.dev.arxiv.org/about/accessibility_html_error_messages.html) due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.

- failed: cambrian.cls
- failed: datetime.sty
- failed: mdframed.sty

Authors: achieve the best HTML results from your LaTeX submissions by following these [best practices](https://info.arxiv.org/help/submit_latex_best_practices.html).

arXiv:2511.04670v1 \[cs.CV\] 06 Nov 2025

Shusheng Yang <sup>1</sup> <sup><span>∗</span></sup>   Jihan Yang <sup>1</sup> <sup><span>∗</span></sup>   Pinzhi Huang <sup>1</sup> <sup><span>†</span></sup>   Ellis Brown <sup>1</sup> <sup><span>†</span></sup>   Zihao Yang <sup>1</sup>  Yue Yu <sup>1</sup>  Shengbang Tong <sup>1</sup>  Zihan Zheng <sup>1</sup>  Yifan Xu <sup>1</sup>  Muhan Wang <sup>1</sup>  Daohan Lu <sup>1</sup>  Rob Fergus <sup>1</sup>  Yann LeCun <sup>1</sup>  Li Fei-Fei <sup>2</sup>  Saining Xie <sup>1</sup>  
<sup>1</sup> New York University   <sup>2</sup> Stanford University

###### Abstract

We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of *supersensing*. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-Super, a two-part benchmark: VSR (long-horizon v isual s patial r ecall) and VSC (continual v isual s patial c ounting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian- *S*, achieving $+30\%$ absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-Super remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose *predictive sensing* as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages *surprise* (prediction error) to drive memory and event segmentation. On VSI-Super, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.

<sup>1</sup> <sup>1</sup> footnotetext: SY led the project, JY and SY contributed equally.<sup>2</sup> <sup>2</sup> footnotetext: Core contributor.

|  | Website | [https://cambrian-mllm.github.io](https://cambrian-mllm.github.io/) |
| --- | --- | --- |
|  | Code | [https://github.com/cambrian-mllm/cambrian-s](https://github.com/cambrian-mllm/cambrian-s) |
|  | Cambrian- *S* Models | [https://hf.co/collections/nyu-visionx/cambrian-s](https://hf.co/collections/nyu-visionx/cambrian-s-models-68ed68c1caded228ae5a6f85) |
|  | VSI-590K | [https://hf.co/datasets/nyu-visionx/vsi-590k](https://hf.co/datasets/nyu-visionx/vsi-590k) |
|  | VSI-Super | [https://hf.co/collections/nyu-visionx/vsi-super](https://hf.co/collections/nyu-visionx/vsi-super) |

Contents

## 1 Introduction

![Refer to caption](https://arxiv.org/html/x6.png)

Figure 1: From pixels to predictive mind. We look beyond linguistic-only understanding to envision multimodal intelligence that sees, remembers, and reasons as part of a continuous, lived world. It begins with semantic perception: naming and describing what is seen. Streaming event cognition goes further, enabling always-on sensing across continuous input streams, integrating memory, and supporting proactive responses. Spatial cognition captures the implicit 3D structure of video, enabling reasoning about objects, configurations, and metrics. Finally, a predictive world model emerges, one that learns passively from experience, updates through prediction and surprise, and retains information for future use. Lower illustration: Video serves as the ideal experimental domain. Models must advance from frame-level Q&A to constructing implicit world models that enable deeper spatial reasoning, scale to unbounded horizons, and achieve supersensing that rivals, and ultimately surpasses, human visual intelligence.

A video is not just a sequence of frames in isolation. It is a continual, high-bandwidth projection of a hidden, evolving 3D world onto pixels [^46]. Although multimodal large language models (MLLMs) have advanced rapidly by pairing strong image encoders with language models [^1], most video extensions [^137] remain fundamentally constrained. They still treat video as sparse frames, underrepresent spatial structure and dynamics [^148], and lean heavily on textual recall [^168], thus overlooking what makes the video modality uniquely powerful.

In this paper, we argue that advancing toward true multimodal intelligence requires a shift from language-centric perception toward spatial *supersensing:* the capacity not only to see, but also to construct, update and predict with an implicit model of the 3D world from continual sensory experience. We do not claim to realize supersensing here; rather, we take an initial step toward it by articulating the developmental path that could lead in this direction and by demonstrating early prototypes along that path:

1. (Linguistic-only understanding): no sensory capabilities; reasoning confined to text and symbols. Current MLLMs have progressed beyond this stage, yet still retain traces of its bias.
2. Semantic perception: parsing pixels into objects, attributes, and relations. This corresponds to the strong multimodal *“show and tell”* capabilities present in MLLMs.
3. Streaming event cognition: processing live, unbounded streams while proactively interpreting and responding to ongoing events. This aligns with efforts to make MLLMs real-time assistants.
4. Implicit 3D spatial cognition: understanding video as projections of a 3D world. Agents must know what is present, where, how things relate, and how configurations change over time. Today’s video models remain limited here.
5. Predictive world modeling: the brain makes *unconscious inferences* [^130] by predicting latent world states based on prior expectations. When these predictions are violated, surprise guides attention, memory, and learning [^41]. However, current multimodal systems lack an internal model that anticipates future states and uses surprise to organize perception for memory and decision making.

Our paper unfolds in three parts.First (§ [2](https://arxiv.org/html/2511.04670v1#S2 "2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")), we re-examine existing benchmarks through the lens of our supersensing hierarchy. We find that most benchmarks map to the first few stages, while some, such as VSI-Bench [^148], begin to probe spatial reasoning. However, none sufficiently address the final crucial stage of predictive world modeling. To make this gap concrete and motivate a shift in approach, we introduce VSI-Super (VSI stands for visual-spatial intelligence), a two-part benchmark for spatial supersensing: VSI-Super Recall (VSR) targets long-horizon spatial observation and recall, while VSI-Super Count (VSC) tests continual counting across changing viewpoints and scenes. Built from arbitrarily long spatiotemporal videos, these tasks are deliberately resistant to the predominant multimodal recipe; they require perception to be *selective* and *structured* rather than indiscriminately accumulated. We show that even the best long-context commercial models struggle on VSI-Super.

Second (§ [3](https://arxiv.org/html/2511.04670v1#S3 "3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")), we investigate whether spatial supersensing is simply a data problem. We curate *VSI-590K*, a spatially focused instruction-tuning corpus over images and videos, which we use to train *Cambrian- *S**, a family of spatially-grounded video MLLMs. Under the current paradigm, careful data design and training push Cambrian- *S* to state-of-the-art spatial cognition on VSI-Bench (> $30\%$ absolute gain) without sacrificing general capabilities. Nevertheless, Cambrian- *S* still falls short on VSI-Super, indicating that while scale lays crucial groundwork, it alone is not sufficient for spatial supersensing.

This motivates the third and final part (§ [4](https://arxiv.org/html/2511.04670v1#S4 "4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")), where we propose *predictive sensing* as a first step toward a new paradigm. We present a proof-of-concept solution built upon self-supervised next-latent-frame prediction. Here, we leverage the model’s prediction error, or “surprise,” for two key functions: (1) managing memory by allocating resources to unexpected events, and (2) event segmentation, breaking unbounded streams into meaningful chunks. We demonstrate that this approach, though simple, significantly outperforms strong long-context baselines such as Gemini-2.5 on our two new tasks. Although not a final solution, this result provides compelling evidence that the path to true supersensing requires models that not only *see* but actively predict and learn from the world.

Our work makes the following contributions. (1) We define a hierarchy for spatial supersensing and introduce VSI-Super, a supersensing benchmark that reveals the limitations of the current paradigm. (2) We develop Cambrian- *S*, a state-of-the-art model that pushes the limits of spatial cognition. Cambrian- *S* serves as a powerful new baseline, and, by delimiting the boundaries of current methods on our new benchmark, paves the path for a new paradigm. (3) We propose predictive sensing as a promising new direction for MLLMs, showing that leveraging model surprise is more effective for long-horizon spatial reasoning than passive context expansion.

## 2 Benchmarking Spatial Supersensing

To ground our pursuit of spatial supersensing, we first establish how to measure it. This section undertakes a two-part investigation into benchmarking this capability. We begin by auditing a suite of popular video MLLM benchmarks, where our analysis ([Fig.˜ 3](https://arxiv.org/html/2511.04670v1#S2.F3 "In 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")) reveals that they overwhelmingly focus on linguistic understanding and semantic perception while neglecting the more advanced spatial and temporal reasoning required for supersensing ([Section ˜ 2.1](https://arxiv.org/html/2511.04670v1#S2.SS1 "2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). To address this critical gap, we then introduce VSI-Super, a new benchmark specifically designed to probe these harder, continual aspects of spatial intelligence in arbitrarily long streaming scenarios ([Section ˜ 2.2](https://arxiv.org/html/2511.04670v1#S2.SS2 "2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). We use this benchmark to test the limits of the current MLLM paradigm throughout the rest of the paper.

\\begin{overpic}\[width=433.62pt\]{figs/benchmark\_analysis\_jy\_v5.pdf} \\put(100.0,20.0){\\makebox\[0.0pt\]\[r\]{\\fcolorbox{white}{white}{\\parbox{0.23\\linewidth}{\\footnotesize\\begin{tabular}{@{}l@{\\hspace{2pt}}l@{}} \\textbf{MME}: & VideoMME~\\cite{fu2025video} \\\\ \\textbf{ES}: & EgoSchema~\\cite{mangalam2023egoschema} \\\\ \\textbf{VM}: & VideoMMMU~\\cite{hu2025video} \\\\ \\textbf{LV}: & LongVideoBench~\\cite{wu2024longvideobench} \\\\ \\textbf{TM}: & Tomato~\\cite{shangguan2024tomato} \\\\ \\textbf{MV}: & MVBench~\\cite{li2024mvbench} \\\\ \\textbf{PT}: & Perception Test~\\cite{patraucean2023perception} \\\\ \\textbf{HV}: & HourVideo~\\cite{chandrasegaran2024hourvideo} \\\\ \\textbf{VSI}: & VSIBench~\\cite{yang2024think} \\\\ \\textbf{\\vso}: & \\vsisuper Rec. \\\\ \\textbf{\\vsc}: & \\vsisuper Cnt. \\\\ \\end{tabular} } }}} \\end{overpic}

Figure 2: [^148]

### 2.1 Deconstructing Existing Video Benchmarks

Recent advances in MLLMs have led to a surge of Video-QA benchmarks. However, a critical question remains: *to what extent do existing video benchmarks truly examine visual sensing capabilities rather than simply testing language priors?* Our diagnostic tests disentangle the model’s reliance on visual sensing versus linguistic priors by varying the richness of visual input and the informativeness of textual cues. Benchmarks solvable with text-only inputs (*e.g.*, captions or a blind MLLM) are skewed towards examining linguistic understanding. In contrast, benchmark questions that can only be answered with multi-frame inputs require genuine visual sensing. We use an image-based multimodal large language model Cambrian-1 [^124] for evaluation, which allows us to probe the underlying task demands without conflating them with the capabilities of video-specific architectures and post-training recipes.

We establish several experimental conditions for feeding video input to a Cambrian-1 [^124] model:

- [^65]
- Single Frame: The model processes only the middle frame of a given video clip. This condition tests the reliance on minimal, contextually-central visual information.
- Frame Captions: Instead of video frames, the model receives captions corresponding to the same 32 uniformly-sampled frames. This condition is designed to reveal how solvable a task is *without* low-level perceptual grounding. We use the Gemini-2.0-Flash API to re-caption video frames.
	To contextualize the performance under these conditions, we introduce two other baselines:
	- Blind Test: The model attempts the task using solely the task’s question. *All visual input is ignored*, no visual captions are used. This baseline measures the model’s performance based on its pre-existing knowledge, language priors, and any potential biases in the benchmark questions.
	- Chance Acc: This represents the accuracy achievable by randomly guessing for the specific task format (*e.g.*, multiple-choice questions), serving as a floor for performance.
		We conduct a fine-grained analysis of each benchmark’s characteristics by comparing performance across these conditions and baselines. We focus on the following key comparisons (diff(A,B) $=$ A-B):
		- diff( $\bf x$ , Blind), $\bf x\in\big\{$ Multiple, Single, Captions $\big\}$ to quantify the uplift provided by different input modalities over the blind baseline;
		- diff( $\bf x$ , Chance), $\bf x\in\big\{$ Multiple, Single, Captions $\big\}$ to measure performance gains over chance;
		- diff(Multiple, Captions) to understand the performance gap between the current mainstream practice and a strong language-only baseline
			Results presented in [Fig.˜ 2](https://arxiv.org/html/2511.04670v1#S2.F2 "In 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") (a-c) demonstrate that Cambrian-1 [^124], an image-based MLLM without any video post-training, can attain reasonable performance across many benchmarks, in some instances surpassing chance-level accuracy by 10-30% (see [Fig.˜ 2](https://arxiv.org/html/2511.04670v1#S2.F2 "In 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-g,h). This suggests that much of the knowledge these benchmarks target is accessible via standard single-image instruction-tuning pipelines. Nevertheless, on two existing datasets, VSI-Bench [^148] and Tomato [^116], the model’s performance falls below chance-level. For VSI-Bench, this is largely because its spatial understanding questions require true video sensing and targeted data curation and training. For Tomato, this underperformance is expected: the benchmark demands understanding of fine-grained details from higher frame-rate video, rendering the largely temporally-subsampled single-frame and 32-frame inputs inadequate.
			Employing textual captions in place of visual inputs also yields notable performance improvements, surpassing chance accuracy by more than 20% on benchmarks such as EgoSchema [^87], VideoMME [^42], LongVideoBench [^140], VideoMMMU [^53], Perception Test [^103], and MVBench [^71] ([Fig.˜ 2](https://arxiv.org/html/2511.04670v1#S2.F2 "In 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-i). Similar conclusions can be drawn when comparing benchmark performance against blind test results ([Fig.˜ 2](https://arxiv.org/html/2511.04670v1#S2.F2 "In 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-d,f). Such performance implies that these benchmarks primarily probe abilities inferable from textual summaries of video content. Interpreting the performance difference between using ‘‘ multiple frames’’ and ‘‘ frame captions’’ ([Fig.˜ 2](https://arxiv.org/html/2511.04670v1#S2.F2 "In 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-j), a significantly positive margin (in favor of multi-frame inputs) signifies a benchmark’s demand for nuanced visual sensing. Conversely, a small or negative margin (more in favor ‘‘ frame captions’’) suggests a more language-centric nature. Our analysis places VideoMMMU, EgoSchema, VideoMME, Perception Test, and LongVideoBench in this latter category, indicating their potential reliance on *linguistic understanding* rather than visual cues. A notable exception is VSC, which is so challenging for current MLLMs that all three input conditions yield near-zero performance, precluding any meaningful comparison between them.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="83.54" id="S2.I1.i3.I1.i2.I1.i3.p4.pic1" overflow="visible" version="1.1" viewBox="0 0 600 83.54" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,83.54) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp13"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 75.52 C 0 79.95 3.59 83.54 8.03 83.54 L 591.97 83.54 C 596.41 83.54 600 79.95 600 75.52 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp13)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 70.2 C 3.54 75.62 7.93 80 13.34 80 L 593.75 80 C 599.16 80 603.54 75.62 603.54 70.2 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 75.52 C 0 79.95 3.59 83.54 8.03 83.54 L 591.97 83.54 C 596.41 83.54 600 79.95 600 75.52 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 75.52 C 1.11 79.34 4.2 82.44 8.03 82.44 L 591.97 82.44 C 595.8 82.44 598.89 79.34 598.89 75.52 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 75.52 C 1.11 79.34 4.2 82.44 8.03 82.44 L 591.97 82.44 C 595.8 82.44 598.89 79.34 598.89 75.52 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="61.96" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:4.28em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 59.27)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ Existing benchmarks overwhelmingly focus on linguistic understanding and semantic perception while neglecting the more advanced spatial and temporal reasoning required for supersensing.</span></span></span></span></span></foreignObject></g></g></g></svg>
			We hope to emphasize the inherent challenges in benchmarking and the impracticality of creating a single, all-encompassing benchmark to evaluate every capability. For example, reliance on language priors should not be viewed merely as a drawback, as access to rich world knowledge and its effective retrieval is undoubtedly beneficial in many scenarios. We argue that video benchmarks should not be treated as measuring a single, uniform notion of ‘‘video understanding.’’ Instead, their design and evaluation should be grounded in the specific capabilities they aim to assess. The preceding analyses are therefore intended to guide the development of tasks that more effectively drive progress towards *spatial supersensing*, which will be the central focus of the rest of the paper.
			![Refer to caption](https://arxiv.org/html/x7.png)
			Figure 3: 42
			### 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs
			Referring to [Fig.˜ 1](https://arxiv.org/html/2511.04670v1#S1.F1 "In 1 Introduction ‣ Cambrian-S: Towards Spatial Supersensing in Video"), spatial supersensing requires MLLMs to have four key capabilities: *semantic perception*, *streaming event cognition*, *implicit 3D spatial cognition*, and *predictive world modeling*. However, as outlined by our analysis in [Fig.˜ 2](https://arxiv.org/html/2511.04670v1#S2.F2 "In 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), most existing video QA benchmarks mainly evaluate the linguistic understanding and semantic perception aspects, which are more reactive and driven by specific tasks [^42]. While recent research has begun to address streaming event cognition through continual sensing, memory architectures, and proactive answering [^24], this capability is often engineered at test time rather than being a native model skill. Furthermore, although spatial reasoning occasionally appears as a category in existing benchmarks, these tasks seldom reach the level of true spatial cognition, and are far from probing the world-modeling capacity that defines supersensing ([Fig.˜ 3](https://arxiv.org/html/2511.04670v1#S2.F3 "In 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). Although VSI-Bench [^148] takes an initial step toward examining spatial cognition, its videos remain short-form and single-scene, and it neither formalizes the problem nor evaluates the essential capability of predictive modeling of the world.
			To illuminate the gap between current MLLMs and spatial supersensing, we introduce VSI-Super, a two-part benchmark for continual spatial sensing. The tasks are intuitive and generally easy for humans, where one simply watches and keeps track of what happens, but they remain surprisingly challenging for machines. They demand selective filtering and structured accumulation of visual information across unbounded spatial videos to maintain coherent understanding and answer questions. Importantly, they are resistant to brute-force context expansion, exposing the need for true spatial reasoning. We detail the two components below.
			![Refer to caption](https://arxiv.org/html/x8.png)
			Figure 4: Illustration of the VSR benchmark’s construction process and format. We use generative models to edit videos by inserting surprising or out-of-place objects into the space. The core task then challenges models to recall the spatial placements of these objects in the correct order of their appearance across arbitrarily long videos.
			#### VSI-Super Recall: Long-horizon spatial observation and recall.
			The VSR benchmark requires MLLMs to observe long-horizon spatiotemporal videos, and sequentially recall the locations of an unusual object. As shown in [Fig.˜ 4](https://arxiv.org/html/2511.04670v1#S2.F4 "In 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), to construct this benchmark, human annotators use an image editing model (*i.e.*, Gemini [^30]) to insert surprising or out-of-place objects (*e.g.*, a Teddy Bear) into four distinct frames (and spatial location) of a video capturing a walkthrough of an indoor environment [^33]. This edited video is then concatenated with other similar room-tour videos to create an arbitrarily long and continuous visual stream. This task parallels the needle-in-a-haystack (NIAH) test commonly used in the language domain to stress test the long-context capabilities of LLMs [^79]. Similar NIAH setups have also been proposed for long-video evaluation [^162]. However, unlike benchmarks that insert unrelated text segments or frames, VSR preserves the realism of the ‘‘needle’’ through in-frame editing. It further extends the challenge by requiring sequential recall, effectively a multi-hop reasoning task, and remains arbitrarily scalable in video length. To thoroughly evaluate model performance across different time scales, the benchmark is provided in five durations: 10, 30, 60, 120, and 240 minutes. Further details on the VSR benchmark construction are provided in [Appendix ˜ B](https://arxiv.org/html/2511.04670v1#A2 "Appendix B VSI-Super Benchmark ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			#### VSI-Super Count: Continual counting under changing viewpoints and scenes.
			Here we test the capacity of MLLMs to continuously accumulate information in long-form spatial videos. To build VSC, we concatenate multiple room-tour video clips from VSI-Bench [^148] and task models with counting the *total* number of target objects across all rooms (see [Fig.˜ 5](https://arxiv.org/html/2511.04670v1#S2.F5 "In VSI-Super Count: Continual counting under changing viewpoints and scenes. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). This setting is challenging because the model must handle viewpoint shifts, repeat sightings, and scene transitions, all while maintaining a consistent cumulative count. For humans, counting is an intuitive and generalizable process. Once the concept of ‘‘one’’ is understood, extending it to larger quantities is natural. In contrast, as we later demonstrate, current MLLMs lack true spatial cognition and depend excessively on learned statistical patterns.
			In addition to standard evaluations (*i.e.*, ask question at the end of video), we query the model at multiple timestamps to assess its performance in streaming settings, where the correct answer in VSC evolves dynamically over time. To examine long-term consistency, VSC includes four video durations: 10, 30, 60, and 120 minutes. For this quantitative task, we report results using the mean relative accuracy ( $\mathcal{MRA}$ ) metric, consistent with the VSI-Bench evaluation protocol [^148].
			![Refer to caption](https://arxiv.org/html/x9.png)
			Figure 5: Overview of the VSC benchmark. The benchmark evaluates counting capabilities on long-horizon, multi-room videos composed of concatenated scenes. Queries are posed at various time points to simulate a streaming question-answering setting.
			#### State-of-the-art models struggle on VSI-Super.
			To test whether VSI-Super poses a real challenge for frontier MLLMs, we evaluate the latest Gemini-2.5-Flash [^122]. As shown in [Table ˜ 1](https://arxiv.org/html/2511.04670v1#S2.T1 "In State-of-the-art models struggle on VSI-Super. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), the model reaches its context limit when handling two-hour videos, despite a context length of 1,048,576 tokens. This highlights the *open-ended* nature of video understanding, where continuous streams effectively require an ‘‘infinite-in, infinite-out’’ context and can grow arbitrarily long, suggesting that simply scaling up tokens, context length, or model size may not suffice. Though synthetic, our benchmark reflects a real challenge in spatial supersensing: humans effortlessly integrate and retain information from ongoing sensory experiences that unfold over hours or years, yet current models lack comparable mechanisms for sustained perception and memory. Gemini-2.5-Flash demonstrates strong performance on semantic-perception and linguistic-understanding-focused video benchmarks such as VideoMME [^42] and VideoMMMU [^53], achieving around 80% accuracy. However, even for 60-minute videos in VSI-Super that fall well within its context window, performance on VSR and VSC remains limited---only 41.5 and 10.9, respectively. As shown in [Fig.˜ 6](https://arxiv.org/html/2511.04670v1#S2.F6 "In State-of-the-art models struggle on VSI-Super. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), the model’s predicted object counts fail to scale with video length or the true number of objects, instead saturating at a small constant value, suggesting a lack of generalization in counting ability and a reliance on training distribution priors.
			<table><tbody><tr><td rowspan="2"><span>Model</span></td><td rowspan="2"><sup><a href="https://arxiv.org/html/#fn:42">42</a></sup></td><td rowspan="2"><sup><a href="https://arxiv.org/html/#fn:53">53</a></sup></td><td rowspan="2"><sup><a href="https://arxiv.org/html/#fn:148">148</a></sup></td><td colspan="2"><span>VSR</span></td><td colspan="2"><span>VSC</span></td></tr><tr><td><span>60 min</span></td><td><span>120 min</span></td><td><span>60 min</span></td><td><span>120 min</span></td></tr><tr><td><span>Gemini-2.5-Flash</span></td><td><span>81.5</span></td><td><span>79.2</span></td><td><span>45.7</span></td><td><span>41.5</span></td><td><span>Out of Ctx.</span></td><td><span>10.9</span></td><td><span>Out of Ctx.</span></td></tr></tbody></table>
			Table 1: Gemini-2.5-Flash results. As a state-of-the-art video understanding model with long-context capabilities, Gemini demonstrates strong performance on general video benchmarks but shows clear limitations towards spatial supersensing.
			![Refer to caption](https://arxiv.org/html/x10.png)
			Figure 6: Visualization of Gemini-2.5-Flash’s predictions v.s. ground truth on VSC. The model’s predicted object counts saturate at small constant values and fail to scale with video length or true object counts, indicating limited generalization in counting and reliance on training distribution priors.
			#### How VSI-Super challenges the current paradigm.
			Although the task setup is simple, the challenge posed by VSI-Super goes beyond just spatial reasoning and reveals fundamental limitations of the current MLLM paradigm.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="50.34" id="S2.SS2.SSS0.Px4.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 600 50.34" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,50.34) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp14"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 42.31 C 0 46.74 3.59 50.34 8.03 50.34 L 591.97 50.34 C 596.41 50.34 600 46.74 600 42.31 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp14)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 37 C 3.54 42.41 7.93 46.79 13.34 46.79 L 593.75 46.79 C 599.16 46.79 603.54 42.41 603.54 37 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 42.31 C 0 46.74 3.59 50.34 8.03 50.34 L 591.97 50.34 C 596.41 50.34 600 46.74 600 42.31 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 42.31 C 1.11 46.13 4.2 49.23 8.03 49.23 L 591.97 49.23 C 595.8 49.23 598.89 46.13 598.89 42.31 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 42.31 C 1.11 46.13 4.2 49.23 8.03 49.23 L 591.97 49.23 C 595.8 49.23 598.89 46.13 598.89 42.31 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="28.75" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:1.88em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 26.06)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ </span><span>VSI-Super</span> <span>tasks challenge the belief that scaling alone guarantees progress.</span></span></span></span></span></foreignObject></g></g></g></svg>
			By allowing arbitrarily long video inputs that emulate the dynamics of streaming cognition, VSI-Super is intentionally constructed to exceed any fixed context window. This design suggests that frame-by-frame tokenization and processing are unlikely to be computationally viable as a long-term solution. Humans address such problems efficiently and adaptively by selectively attending to and retaining only a small fraction of sensory input [^62], often unconsciously [^40]. This predictive and selective mechanism, core to human cognition, remains absent in current MLLMs but is fundamental to a predictive world model.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="50.34" id="S2.SS2.SSS0.Px4.p4.pic1" overflow="visible" version="1.1" viewBox="0 0 600 50.34" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,50.34) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp15"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 42.31 C 0 46.74 3.59 50.34 8.03 50.34 L 591.97 50.34 C 596.41 50.34 600 46.74 600 42.31 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp15)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 37 C 3.54 42.41 7.93 46.79 13.34 46.79 L 593.75 46.79 C 599.16 46.79 603.54 42.41 603.54 37 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 42.31 C 0 46.74 3.59 50.34 8.03 50.34 L 591.97 50.34 C 596.41 50.34 600 46.74 600 42.31 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 42.31 C 1.11 46.13 4.2 49.23 8.03 49.23 L 591.97 49.23 C 595.8 49.23 598.89 46.13 598.89 42.31 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 42.31 C 1.11 46.13 4.2 49.23 8.03 49.23 L 591.97 49.23 C 595.8 49.23 598.89 46.13 598.89 42.31 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="28.75" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:1.88em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 26.06)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ </span><span>VSI-Super</span> <span>tasks demand generalization to new temporal and spatial scales at test time.</span></span></span></span></span></foreignObject></g></g></g></svg>
			For example, VSC requires counting in arbitrarily long videos, similar to how humans, who understand the concept of counting, can extend it to any number. The key is not maintaining an extremely long context window, humans do not retain every visual detail from extended visual experiences, but rather learning the process of counting itself. Predictive sensing facilitates this by *segmenting* continuous visual streams into coherent events, using moments of ‘‘surprise’’ to impose temporal structure. This segmentation acts as a divide-and-conquer mechanism that allows the model to decide when to start, continue, or reset behaviors in dynamically changing scenes.
			Together, these challenges, which span computational efficiency, generalization, and cognitive mechanisms such as unconscious inference and predictive sensing, call for a paradigm shift. Rather than relying solely on scaling data, parameters, or context length, future models should learn internal world models capable of perceiving and predicting within an endlessly unfolding visual world across space and time.
			To further motivate this paradigm shift, the next section investigates the extent to which progress remains possible within the current paradigm through improved engineering and targeted data curation. We assess whether the existing MLLM framework can be adapted to address the challenges posed by VSI-Super. These efforts, while operating within the limits of the present framework, are indispensable for building the data and empirical foundations of the next generation of spatial supersensing models.
			## 3 Spatial Sensing Under the Current Paradigm
			As demonstrated in the previous section, Gemini-2.5-Flash exhibits subpar performance on spatial sensing tasks (see [Table ˜ 1](https://arxiv.org/html/2511.04670v1#S2.T1 "In State-of-the-art models struggle on VSI-Super. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). This observation raises a key question: *Is limited spatial sensing simply a data issue?* It is a valid question to ask, as current video MLLMs do not explicitly prioritize spatial-focused videos during training, and it remains s whether existing pre-training and post-training designs are well-suited for our target tasks. We begin by enhancing Cambrian-1 [^124] with a series of architectural and training improvements to establish a stronger image MLLM as our base model ([Section ˜ 3.1](https://arxiv.org/html/2511.04670v1#S3.SS1 "3.1 Base Model Training: Upgraded Cambrian-1 ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). We proceed to construct a large-scale, spatial-focused instruction-tuning dataset, VSI-590K ([Section ˜ 3.2](https://arxiv.org/html/2511.04670v1#S3.SS2 "3.2 Spatial Video Data Curation: VSI-590K ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). The dataset is curated from diverse sources and carefully annotated. As such data does not currently exist publicly, VSI-590K is intended to provide a strong data foundation for spatial sensing. Finally, with a refined training recipe ([Section ˜ 3.3](https://arxiv.org/html/2511.04670v1#S3.SS3 "3.3 Post-Training Recipe for Spatial Sensing ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")), we introduce the spatially-grounded Cambrian- *S* model family ([Section ˜ 3.4](https://arxiv.org/html/2511.04670v1#S3.SS4 "3.4 Cambrian-S: Spatially-Grounded MLLMs ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")).
			The Cambrian- *S* model family demonstrates strong performance on established spatial reasoning benchmarks such as VSI-Bench [^148] and offers valuable insights into base model design, data curation, and training strategies for spatial supersensing. However, despite these advances, this approach does not directly address the continual sensing challenges of VSI-Super ([Section ˜ 3.5](https://arxiv.org/html/2511.04670v1#S3.SS5 "3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")); instead, it provides a crucial foundation that motivates the new paradigm introduced in ([Section ˜ 4](https://arxiv.org/html/2511.04670v1#S4 "4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")).
			### 3.1 Base Model Training: Upgraded Cambrian-1
			We begin by developing an image-based MLLM base model, as robust semantic perception forms the foundation for higher-level spatial cognition. We follow the two-stage training pipeline of Cambrian-1 [^124]. We upgrade the visual encoder to SigLIP2-SO400m [^128] and the language model to the instruction-tuned Qwen2.5 [^145]. For the vision-language connector, we adopt a simple two-layer MLP primarily for its computational efficiency. Other training components from Cambrian-1, including hyperparameters and the data recipe, remain unchanged. Full implementation details are provided in [Appendix ˜ D](https://arxiv.org/html/2511.04670v1#A4 "Appendix D Cambrian-S Implementation Details ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			### 3.2 Spatial Video Data Curation: VSI-590K
			![Refer to caption](https://arxiv.org/html/x11.png)
			Figure 7: VSI-590K data curation pipeline. We collect data from 3D-annotated real and simulated video sources, as well as from pseudo-annotated frames extracted from web videos. We then use diverse templates to automatically generate question–answer pairs for instruction tuning.
			Table 2: Data statistics for VSI-590K. We collect data from 10 sources with different video types and annotations to improve diversity.
			| Dataset | \# Videos | \# Images | \# QA Pairs |
			| --- | --- | --- | --- |
			| Annotated Real Videos |  |  |  |
			| S3DIS [^4] | 199 | \- | 5,187 |
			| Aria Digital Twin [^102] | 183 | \- | 60,207 |
			| ScanNet [^33] | 1,201 | \- | 92,145 |
			| ScanNet++ V2 [^153] | 856 | \- | 138,701 |
			| ARKitScenes [^12] | 2,899 | \- | 57,816 |
			| Simulated Data |  |  |  |
			| ProcTHOR [^36] | 625 | \- | 20,092 |
			| Hypersim [^113] | \- | 5,113 | 176,774 |
			| Unannotated Real Videos |  |  |  |
			| YouTube Room Tour | \- | 20,100 | 20,100 |
			| Open X-Embodiment [^100] | \- | 14,801 | 14,801 |
			| AgiBot-World [^16] | \- | 4,844 | 4,844 |
			| Total | 5,963 | 44,858 | 590,667 |
			It is well recognized that data quality and diversity play a critical role in the training of MLLMs [^124]. We hypothesize that the performance gap on VSI-Bench [^148] comes mainly from the lack of high-quality, spatially grounded data in current instruction-tuning datasets [^161]. To fill this gap, we build VSI-590K, a large-scale instruction-tuning dataset designed to improve visual-spatial understanding.
			#### Data curation and processing.
			We construct VSI-590K from a diverse span of data sources and types (*i.e.*, simulated and real). See [Table ˜ 2](https://arxiv.org/html/2511.04670v1#S3.T2 "In 3.2 Spatial Video Data Curation: VSI-590K ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") for the data sources and for dataset statistics on the number of videos, images, and QA pairs from each dataset. We find that this yields a dataset substantially more robust than one of comparable size derived from a single source. Below, we detail the data processing procedure.
			- *Annotated real videos.*Multimodal visual–spatial reasoning relies on a solid understanding of 3D geometry and spatial relationships. Following VSI-Bench, we repurpose the training splits of existing indoor scan and first-person video datasets that provide 3D instance-level annotations, including S3DIS [^4], ScanNet [^33], ScanNet++ V2 [^153], ARKitScenes [^12], and ADT [^102]. For each dataset, annotations are consolidated into a meta-information file capturing scene-level attributes such as object counts by category, object bounding boxes, room dimensions, and related metadata. Question templates are then automatically instantiated to generate corresponding questions.
			- *Simulated data.*Due to the limited availability of 3D-annotated data, constructing a large-scale and diverse 3D-annotated SFT dataset solely from real annotated videos is challenging. Following SIMS-V [^13], we utilize embodied simulators to procedurally generate spatially grounded video trajectories and QA pairs, rendering 625 video traversals within ProcTHOR [^36] scenes featuring diverse layouts, object configurations, and visual appearances. We apply the same methodology to Hypersim [^113], sampling 5,113 images from 461 indoor scenes. Using instance-level bounding boxes, we generate question-answer pairs consistent with our annotated real-video setup.
			- *Unannotated real videos.*Although web-sourced videos lack explicit annotations, they offer rich diversity in indoor environment types, geographical regions, and spatial layouts. We collected approximately 19K room tour videos from YouTube and additionally incorporated videos from robotic learning datasets, including Open-X-Embodiment [^100] and AgiBot-World [^16]. Since these videos do not contain the 3D annotations required for constructing spatial instruction-tuning data, we develop a pseudo-annotation pipeline. As illustrated in [Fig.˜ 7](https://arxiv.org/html/2511.04670v1#S3.F7 "In 3.2 Spatial Video Data Curation: VSI-590K ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), we subsample and filter video frames, applying object detection [^80], segmentation model [^109], and 3D reconstruction model [^133] to generate pseudo-annotated images following the approach of SpatialVLM [^21]. We choose to generate annotations at the image level rather than across full videos, as full-video pseudo-annotations derived from recognition and reconstruction models tend to be too noisy for training.
			#### Question type definition and template augmentation.
			We define 12 question types within a spatiotemporal taxonomy to construct a comprehensive and diverse set of questions for instruction tuning. We define five main question types---size, direction, count, distance, and appearance order---broadly categorized as measuring configuration, measurement, or spatiotemporal capabilities following [^148]. Except for the appearance order type, each question category includes both relative and absolute variants, reflecting the importance of these complementary forms of reasoning in visual–spatial understanding [^148]. For example, for size, we ask for both size comparison between two objects (relative) and the metric dimensions of an object (absolute). To enhance diversity, we vary the perspective used in formulating direction and distance questions. For instance, a distance question may ask which of two objects is closer to the camera or which object is closer to a third reference object. We also diversify the dataset through variations in question wording and in measurement units (*e.g.*, meters versus feet). Additional details of the dataset are provided in [Appendix ˜ C](https://arxiv.org/html/2511.04670v1#A3 "Appendix C VSI-590K Dataset ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			Table 3: Contributions of Different Data Sources in the VSI-590K Mixture. This table illustrates the impact of different data sources on VSI-Bench performance. The combined dataset, VSI-590K Full Mix, achieves the best overall results. Among individual sources, annotated real video datasets contribute the most significant improvements, followed by simulated videos, and then pseudo-annotated images.
			<table><tbody><tr><td></td><td colspan="3"><span>Image</span></td><td colspan="9"><span>VSI-Bench (Video)</span></td></tr><tr><td><span>VSI Data Mixture</span></td><td><p><span></span></p><p><span>MMVP</span></p><p></p></td><td><p><span></span></p><p><span>3DSR</span></p><p></p></td><td><p><span></span></p><p><span>CV-B</span></p><p></p></td><td><p><span></span></p><p><span>Avg</span></p><p></p></td><td><p><span></span></p><p><span>Obj Ct</span></p><p></p></td><td><p><span></span></p><p><span>Abs Dst</span></p><p></p></td><td><p><span></span></p><p><span>Obj Sz</span></p><p></p></td><td><p><span></span></p><p><span>Rm Sz</span></p><p></p></td><td><p><span></span></p><p><span>Rel Dst</span></p><p></p></td><td><p><span></span></p><p><span>Rel Dir</span></p><p></p></td><td><p><span></span></p><p><span>Rte Pln</span></p><p></p></td><td><p><span></span></p><p><span>Ap Ord</span></p><p></p></td></tr><tr><td><span>Baseline</span></td><td><span>52.7</span></td><td><span>54.5</span></td><td><span>73.5</span></td><td><span>28.5</span></td><td><span>18.1</span></td><td><span>20.0</span></td><td><span>36.0</span></td><td><span>22.2</span></td><td><span>42.9</span></td><td><span>31.3</span></td><td><span>24.6</span></td><td><span>33.0</span></td></tr><tr><td><span>Real Videos</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>+ S3DIS</span></td><td><span>54.0</span></td><td><span>54.9</span></td><td><span>75.3</span></td><td><span>41.6</span></td><td><span>63.8</span></td><td><span>21.0</span></td><td><span>44.9</span></td><td><span>37.0</span></td><td><span>43.8</span></td><td><span>47.4</span></td><td><span>34.0</span></td><td><span>41.1</span></td></tr><tr><td><span>+ ADT</span></td><td><span>50.6</span></td><td><span>56.5</span></td><td><span>77.5</span></td><td><span>41.0</span></td><td><span>51.0</span></td><td><span>29.8</span></td><td><span>52.5</span></td><td><span>40.2</span></td><td><span>42.3</span></td><td><span>38.8</span></td><td><span>34.0</span></td><td><span>39.8</span></td></tr><tr><td><span>+ ARKitScenes</span></td><td><span>50.0</span></td><td><span>56.7</span></td><td><span>77.3</span></td><td><span>51.0</span></td><td><span>70.2</span></td><td><span>32.7</span></td><td><span>64.5</span></td><td><span>60.0</span></td><td><span>55.1</span></td><td><span>45.2</span></td><td><span>37.1</span></td><td><span>43.5</span></td></tr><tr><td><span>+ ScanNet</span></td><td><span>54.7</span></td><td><span>57.7</span></td><td><span>77.5</span></td><td><span>56.3</span></td><td><span>70.9</span></td><td><span>37.9</span></td><td><span>67.5</span></td><td><span>59.3</span></td><td><span>57.0</span></td><td><span>46.7</span></td><td><span>35.1</span></td><td><span>76.1</span></td></tr><tr><td><span>+ ScanNet++ V2</span></td><td><span>52.7</span></td><td><span>57.3</span></td><td><span>77.5</span></td><td><span>56.3</span></td><td><span>72.5</span></td><td><span>40.7</span></td><td><span>65.7</span></td><td><span>56.9</span></td><td><span>59.7</span></td><td><span>47.1</span></td><td><span>31.4</span></td><td><span>76.2</span></td></tr><tr><td><span>Simulated Videos</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>+ ProcThor</span></td><td><span>53.3</span></td><td><span>55.7</span></td><td><span>74.9</span></td><td><span>36.4</span></td><td><span>21.0</span></td><td><span>29.7</span></td><td><span>49.3</span></td><td><span>3.8</span></td><td><span>52.3</span></td><td><span>45.7</span></td><td><span>30.4</span></td><td><span>58.7</span></td></tr><tr><td><span>+ HyperSim</span></td><td><span>52.0</span></td><td><span>56.0</span></td><td><span>79.7</span></td><td><span>45.6</span></td><td><span>67.8</span></td><td><span>32.0</span></td><td><span>59.3</span></td><td><span>36.4</span></td><td><span>53.2</span></td><td><span>47.0</span></td><td><span>32.5</span></td><td><span>36.6</span></td></tr><tr><td><span>Pseudo-Annotated Images</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>+ YTB RoomTour</span></td><td><span>55.3</span></td><td><span>52.6</span></td><td><span>75.0</span></td><td><span>32.5</span></td><td><span>43.4</span></td><td><span>25.8</span></td><td><span>24.2</span></td><td><span>27.3</span></td><td><span>38.7</span></td><td><span>31.4</span></td><td><span>28.4</span></td><td><span>40.9</span></td></tr><tr><td><span>+ OXE &amp; AGIBot</span></td><td><span>56.0</span></td><td><span>54.4</span></td><td><span>72.5</span></td><td><span>30.6</span></td><td><span>40.3</span></td><td><span>23.1</span></td><td><span>27.9</span></td><td><span>26.6</span></td><td><span>38.0</span></td><td><span>22.8</span></td><td><span>32.0</span></td><td><span>33.8</span></td></tr><tr><td><span>Full Mix</span></td><td><span>54.7</span></td><td><span>54.0</span></td><td><span>77.9</span></td><td><span>63.2</span></td><td><span>73.5</span></td><td><span>49.4</span></td><td><span>71.4</span></td><td><span>70.1</span></td><td><span>66.9</span></td><td><span>61.5</span></td><td><span>36.6</span></td><td><span>76.6</span></td></tr></tbody></table>
			#### VSI-590K data source ablation.
			To evaluate the effectiveness of our proposed VSI-590K dataset, we perform an ablation study by finetuning the improved Cambrian-1 MLLM described in [Section ˜ 3.1](https://arxiv.org/html/2511.04670v1#S3.SS1 "3.1 Base Model Training: Upgraded Cambrian-1 ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") with part of the video instruction tuning samples from LLaVA-Video-178K [^161]. This model serves as the baseline in [Table ˜ 3](https://arxiv.org/html/2511.04670v1#S3.T3 "In Question type definition and template augmentation. ‣ 3.2 Spatial Video Data Curation: VSI-590K ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"). The contribution of each data source is evaluated by fine-tuning the model on individual datasets as well as their combination. The VSI-590K Full Mix achieves the highest overall performance on video spatial reasoning tasks, outperforming both the baseline and all single-source counterparts. All data sources contribute positively after fine-tuning, though their effectiveness varies.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="66.94" id="S3.SS2.SSS0.Px3.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 600 66.94" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,66.94) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp16"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp16)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 53.6 C 3.54 59.01 7.93 63.4 13.34 63.4 L 593.75 63.4 C 599.16 63.4 603.54 59.01 603.54 53.6 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="45.35" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:3.08em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 42.66)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ Data effectiveness ranks as: annotated real videos </span><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" data-latex="&gt;"><semantics><mo>&gt;</mo> <annotation>&gt;</annotation></semantics></math> <span>simulated data </span><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" data-latex="&gt;"><semantics><mo>&gt;</mo> <annotation>&gt;</annotation></semantics></math> <span>pseudo-annotated images.</span></span></span></span></span></foreignObject></g></g></g></svg>
			This indicates that videos are inherently more informative than static images for spatial reasoning, as training exclusively on video data yields superior performance on *both* video- and image-based spatial reasoning benchmarks. These findings support the intuition that the temporal continuity and multi-view diversity of videos are key to developing robust spatial representations.
			### 3.3 Post-Training Recipe for Spatial Sensing
			We further analyze and ablate our video instruction-tuning pipeline, focusing on the roles of the pretrained base video model and the instruction-tuning dataset mixture. As shown in [Table ˜ 4](https://arxiv.org/html/2511.04670v1#S3.T4 "In 3.3 Post-Training Recipe for Spatial Sensing ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), we begin with four *base models* that represent a progressive increase in video understanding capability:
			- A1 is trained only with image-text alignment on Cambrian-1 alignment data. The language model is identical to base QwenLM as it is frozen during training.
			- A2 is finetuned with image instruction tuning on top of A1, essentially our improved Cambrian-1.
			- A3 is initialized from A2 and finetuned on 429K video instruction tuning data.
			- A4 is initialized from A2 and finetuned on 3M video instruction tuning data.
			We then finetune these models using two different data recipes: (1) VSI-590K only, and (2) VSI-590K mixed with a similar amount of general video instruction tuning data.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="66.94" id="S3.SS3.p4.pic1" overflow="visible" version="1.1" viewBox="0 0 600 66.94" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,66.94) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp17"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp17)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 53.6 C 3.54 59.01 7.93 63.4 13.34 63.4 L 593.75 63.4 C 599.16 63.4 603.54 59.01 603.54 53.6 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="45.35" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:3.08em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 42.66)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ A stronger base model with greater exposure to general video data leads to improved spatial sensing after SFT.</span></span></span></span></span></foreignObject></g></g></g></svg>
			As shown in [Table ˜ 4](https://arxiv.org/html/2511.04670v1#S3.T4 "In 3.3 Post-Training Recipe for Spatial Sensing ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), SFT with a stronger base model, one that performs well on general video benchmarks such as VideoMME [^42] and EgoSchema [^87], leads to enhanced spatial understanding. This highlights the importance of broad exposure to general video data during base model training.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="50.34" id="S3.SS3.p6.pic1" overflow="visible" version="1.1" viewBox="0 0 600 50.34" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,50.34) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp18"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 42.31 C 0 46.74 3.59 50.34 8.03 50.34 L 591.97 50.34 C 596.41 50.34 600 46.74 600 42.31 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp18)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 37 C 3.54 42.41 7.93 46.79 13.34 46.79 L 593.75 46.79 C 599.16 46.79 603.54 42.41 603.54 37 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 42.31 C 0 46.74 3.59 50.34 8.03 50.34 L 591.97 50.34 C 596.41 50.34 600 46.74 600 42.31 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 42.31 C 1.11 46.13 4.2 49.23 8.03 49.23 L 591.97 49.23 C 595.8 49.23 598.89 46.13 598.89 42.31 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 42.31 C 1.11 46.13 4.2 49.23 8.03 49.23 L 591.97 49.23 C 595.8 49.23 598.89 46.13 598.89 42.31 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="28.75" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:1.88em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 26.06)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ Mixing general video data prevents the generalization loss caused by in-domain SFT.</span></span></span></span></span></foreignObject></g></g></g></svg>
			Furthermore, while in-domain SFT solely on VSI-590K achieves the highest performance on VSI-Bench, it results in a noticeable decline on general video benchmarks. However, this performance drop can be effectively mitigated by training on a data mix that includes general videos.
			Table 4: Post-training exploration for spatial sensing. We examine four base models with progressively increasing exposure to visual data, from image-only training to extensive video training, and analyze their distinct trends during spatial sensing tuning under two different data recipes. A1: only the connector is trained for image–language alignment; A2: A1 *w/.* Cambrian-7M image instruction-tuning data; A3: A2 further finetuned on 429K video instruction-tuning samples; A4: A2 further finetuned on 3M video instruction-tuning samples. From A1 to A4, the models show a monotonic improvement in video understanding ability. I-IT and V-IT denote instruction finetuning on image and video data, respectively. Finally, we show that stronger base models yield better SFT performance on spatial sensing tasks.
			<table><tbody><tr><td><span>Model</span></td><td><span>VSI-Bench</span></td><td><span>VideoMME</span></td><td><span>EgoSchema</span></td><td><span>Perception Test</span></td></tr><tr><td colspan="5"><span>Different Base Models</span></td></tr><tr><td><span>A1 </span><span>(<em>w/o.</em> I-IT, <em>i.e.</em> QwenLM)</span></td><td><span>21.4</span></td><td><span>44.2</span></td><td><span>42.9</span></td><td><span>44.5</span></td></tr><tr><td><span>A2 </span><span>(A1 + I-IT, <em>i.e.</em> Cambrian-1)</span></td><td><span>25.8</span></td><td><span>53.7</span></td><td><span>48.1</span></td><td><span>55.4</span></td></tr><tr><td><span>A3 </span><span>(A2 + V-IT, 429K data)</span></td><td><span>28.9</span></td><td><span>61.2</span></td><td><span>50.3</span></td><td><span>66.3</span></td></tr><tr><td><span>A4 </span><span>(A2 + V-IT, 3M data)</span></td><td><span>35.7</span></td><td><span>62.6</span></td><td><span>77.0</span></td><td><span>70.9</span></td></tr><tr><td colspan="5"><span>SFT <em>w/.</em> VSI-590K</span></td></tr><tr><td><span>from A1</span></td><td><span>57.2</span></td><td><span>40.3</span></td><td><span>38.7</span></td><td><span>52.3</span></td></tr><tr><td><span>from A2</span></td><td><span>66.8</span></td><td><span>46.7</span></td><td><span>47.2</span></td><td><span>52.3</span></td></tr><tr><td><span>from A3</span></td><td><span>68.8</span></td><td><span>52.3</span></td><td><span>48.4</span></td><td><span>55.8</span></td></tr><tr><td><span>from A4</span></td><td><span>69.2</span></td><td><span>54.1</span></td><td><span>55.2</span></td><td><span>59.2</span></td></tr><tr><td colspan="5"><span>SFT <em>w/.</em> VSI-590K &amp; general V-IT data mixture</span></td></tr><tr><td><span>from A1</span></td><td><span>61.3</span></td><td><span>60.5</span></td><td><span>52.8</span></td><td><span>65.0</span></td></tr><tr><td><span>from A2</span></td><td><span>63.2</span></td><td><span>62.6</span></td><td><span>52.9</span></td><td><span>65.6</span></td></tr><tr><td><span>from A3</span></td><td><span>64.0</span></td><td><span>61.0</span></td><td><span>54.9</span></td><td><span>66.8</span></td></tr><tr><td><span>from A4</span></td><td><span>65.1</span></td><td><span>61.9</span></td><td><span>77.3</span></td><td><span>71.2</span></td></tr></tbody></table>
			### 3.4 Cambrian-S: Spatially-Grounded MLLMs
			Building on all the previous insights, we develop Cambrian- *S*, a family of spatially-grounded models with varying LLM scales: 0.5B, 1.5B, 3B, and 7B parameters. These models are built through a four-stage training pipeline specifically designed to first establish general semantic perception and then develop specialized spatial sensing skills, as illustrated in [Fig.˜ 8](https://arxiv.org/html/2511.04670v1#S3.F8 "In 3.4 Cambrian-S: Spatially-Grounded MLLMs ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			The first two stages adhere to the Cambrian-1 framework to develop strong image understanding capabilities. In stage 3, we extend the models to video by conducting general video instruction tuning on CambrianS-3M, a curated dataset composed of 3 million samples (see detailed composition in [Fig.˜ 16](https://arxiv.org/html/2511.04670v1#A4.F16 "In D.2 Training Data Mixture ‣ Appendix D Cambrian-S Implementation Details ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). This stage establishes a solid foundation for general video understanding prior to introducing specialized skills. In the final and crucial stage 4, the models are trained for spatial sensing. Here, we finetune the models on a blended corpus combining our specialized VSI-590K with a proportional subset of the general video data used in stage 3, following the setup described in [Table ˜ 4](https://arxiv.org/html/2511.04670v1#S3.T4 "In 3.3 Post-Training Recipe for Spatial Sensing ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"). Complete training details are provided in [Section ˜ D.3](https://arxiv.org/html/2511.04670v1#A4.SS3 "D.3 Training Recipe ‣ Appendix D Cambrian-S Implementation Details ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			![Refer to caption](https://arxiv.org/html/x12.png)
			Figure 8: Overall Cambrian- S training pipeline. Stages 1 and 2 enhance image understanding, stage 3 improves general video understanding, and stage 4 strengthens spatial sensing capability.
			Table 5: Comparison of Cambrian- *S* with other leading MLLMs. Cambrian- *S* outperforms both proprietary and open-source models across a range of image and video visual–spatial benchmarks and model sizes. For video evaluation, we uniformly sample 128 frames as input. Detailed evaluation settings are provided in [Appendix ˜ E](https://arxiv.org/html/2511.04670v1#A5 "Appendix E Cambrian-S Additional Results ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			<table><tbody><tr><td></td><td></td><td colspan="10"><span>Video</span></td><td colspan="3"><span>Image</span></td></tr><tr><td><span>Model</span></td><td><span>Base LM</span></td><td><p><span></span></p><p><span>VSI-Bench</span></p><p></p></td><td><p><span></span></p><p><span>VSI-Bench</span> <sup><span>Debiased</span></sup></p><p></p></td><td><p><span></span></p><p><span>Tomato</span></p><p></p></td><td><p><span></span></p><p><span>HourVideo</span></p><p></p></td><td><p><span></span></p><p><span>Video</span> <sup><span>MME</span></sup> <span>†</span></p><p></p></td><td><p><span></span></p><p><span>EgoSchema </span><span>†</span></p><p></p></td><td><p><span></span></p><p><span>Video</span> <sup><span>MMMU</span></sup> <span>†</span></p><p></p></td><td><p><span></span></p><p><span>LongVBench </span><span>†</span></p><p></p></td><td><p><span></span></p><p><span>MVBench </span><span>†</span></p><p></p></td><td><p><span></span></p><p><span>Percept. Test </span><span>†</span></p><p></p></td><td><p><span></span></p><p><span>MMVP</span></p><p></p></td><td><p><span></span></p><p><span>3DSR</span></p><p></p></td><td><p><span></span></p><p><span>CV-Bench</span></p><p></p></td></tr><tr><td><span>Proprietary Models</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>Claude-3.5-sonnet</span></td><td><span>UNK.</span></td><td><span>-</span></td><td><span>-</span></td><td><span>27.8</span></td><td><span>-</span></td><td><span>62.9</span></td><td><span>-</span></td><td><span>65.8</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>48.2</span></td><td><span>-</span></td></tr><tr><td><span>GPT-4o</span></td><td><span>UNK.</span></td><td><span>34.0</span></td><td><span>-</span></td><td><span>37.7</span></td><td><span>37.2</span></td><td><span>71.9</span></td><td><span>-</span></td><td><span>61.2</span></td><td><span>66.7</span></td><td><span>-</span></td><td><span>-</span></td><td><span>66.0</span></td><td><span>44.2</span></td><td><span>-</span></td></tr><tr><td><span>Gemini-1.5-Pro</span></td><td><span>UNK.</span></td><td><span>45.4</span></td><td><span>40.1</span></td><td><span>36.1</span></td><td><span>37.3</span></td><td><span>75.0</span></td><td><span>72.2</span></td><td><span>53.9</span></td><td><span>64.0</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Gemini-2.5 Pro</span></td><td><span>UNK.</span></td><td><span>51.5</span></td><td><span>49.1</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>83.6</span></td><td><span>67.4</span></td><td><span>-</span></td><td><span>-</span></td><td><span>51.3</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Open-Source Models</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>LLaVA-Video-7B</span></td><td><span>Qwen2-7B</span></td><td><span>35.6</span></td><td><span>30.7</span></td><td><span>22.5</span></td><td><span>28.6</span></td><td><span>63.3</span></td><td><span>57.3</span></td><td><span>36.1</span></td><td><span>58.2</span></td><td><span>58.6</span></td><td><span>67.9</span></td><td><span>-</span></td><td><span>-</span></td><td><span>75.7</span></td></tr><tr><td><span>LLaVA-One-Vision-7B</span></td><td><span>Qwen2-7B</span></td><td><span>32.4</span></td><td><span>28.5</span></td><td><span>25.5</span></td><td><span>28.3</span></td><td><span>58.2</span></td><td><span>60.1</span></td><td><span>33.9</span></td><td><span>56.4</span></td><td><span>56.7</span></td><td><span>57.1</span></td><td><span>54.7</span></td><td><span>-</span></td><td><span>74.3</span></td></tr><tr><td><span>Qwen-VL-2.5-7B</span></td><td><span>Qwen2.5-7B</span></td><td><span>33.5</span></td><td><span>29.6</span></td><td><span>-</span></td><td><span>-</span></td><td><span>65.1</span></td><td><span>65.0</span></td><td><span>47.4</span></td><td><span>56.0</span></td><td><span>69.6</span></td><td><span>-</span></td><td><span>56.7</span></td><td><span>48.4</span></td><td><span>-</span></td></tr><tr><td><span>InternVL2.5-8B</span></td><td><span>InternLM2.5-7B</span></td><td><span>34.6</span></td><td><span>24.9</span></td><td><span>-</span></td><td><span>-</span></td><td><span>64.2</span></td><td><span>50.6</span></td><td><span>-</span></td><td><span>60.0</span></td><td><span>72.0</span></td><td><span>-</span></td><td><span>55.3</span></td><td><span>50.9</span></td><td><span>-</span></td></tr><tr><td><span>InternVL3.5-8B</span></td><td><span>Qwen3-8B</span></td><td><span>56.3</span></td><td><span>49.7</span></td><td><span>-</span></td><td><span>-</span></td><td><span>66.0</span></td><td><span>61.2</span></td><td><span>49.0</span></td><td><span>62.1</span></td><td><span>72.1</span></td><td><span>-</span></td><td><span>56.0</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Cambrian- <em>S</em> -7B</span></td><td><span>Qwen2.5-7B</span></td><td><span>67.5</span></td><td><span>59.9</span></td><td><span>27.0</span></td><td><span>36.5</span></td><td><span>63.4</span></td><td><span>76.8</span></td><td><span>38.6</span></td><td><span>59.4</span></td><td><span>64.5</span></td><td><span>69.9</span></td><td><span>60.0</span></td><td><span>54.8</span></td><td><span>76.9</span></td></tr><tr><td><span>VILA1.5-3B</span></td><td><span>Sheared-LLaMA-2.7B</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>42.2</span></td><td><span>-</span></td><td><span>-</span></td><td><span>42.9</span></td><td><span>-</span></td><td><span>49.1</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Qwen2.5-VL-3B</span></td><td><span>Qwen2.5-3B</span></td><td><span>26.8</span></td><td><span>22.7</span></td><td><span>-</span></td><td><span>-</span></td><td><span>61.5</span></td><td><span>-</span></td><td><span>-</span></td><td><span>54.2</span></td><td><span>-</span></td><td><span>66.9</span></td><td><span>39.3</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Cambrian- <em>S</em> -3B</span></td><td><span>Qwen2.5-3B</span></td><td><span>57.3</span></td><td><span>49.7</span></td><td><span>25.4</span></td><td><span>36.8</span></td><td><span>60.2</span></td><td><span>73.5</span></td><td><span>25.2</span></td><td><span>52.3</span></td><td><span>60.2</span></td><td><span>65.9</span></td><td><span>50.0</span></td><td><span>50.9</span></td><td><span>75.2</span></td></tr><tr><td><span>SmolVLM2-2.2B</span></td><td><span>SmolLM2-1.7B</span></td><td><span>27.0</span></td><td><span>22.3</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>34.1</span></td><td><span>-</span></td><td><span>-</span></td><td><span>48.7</span></td><td><span>51.1</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>InternVL2.5-2B</span></td><td><span>InternLM2.5-1.8B</span></td><td><span>25.8</span></td><td><span>20.7</span></td><td><span>-</span></td><td><span>-</span></td><td><span>51.9</span></td><td><span>47.4</span></td><td><span>-</span></td><td><span>52.0</span></td><td><span>68.8</span></td><td><span>-</span></td><td><span>45.3</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>InternVL3.5-2B</span></td><td><span>Qwen3-1.7B</span></td><td><span>51.5</span></td><td><span>46.1</span></td><td><span>-</span></td><td><span>-</span></td><td><span>58.4</span></td><td><span>50.8</span></td><td><span>-</span></td><td><span>57.4</span></td><td><span>65.9</span></td><td><span>-</span></td><td><span>44.0</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Cambrian- <em>S</em> -1.5B</span></td><td><span>Qwen2.5-1.5B</span></td><td><span>54.8</span></td><td><span>47.5</span></td><td><span>22.5</span></td><td><span>31.4</span></td><td><span>55.6</span></td><td><span>68.8</span></td><td><span>24.9</span></td><td><span>50.0</span></td><td><span>58.1</span></td><td><span>63.2</span></td><td><span>42.7</span></td><td><span>51.9</span></td><td><span>69.6</span></td></tr><tr><td><span>SmolVLM2-0.5B</span></td><td><span>SmolLM2-360M</span></td><td><span>26.1</span></td><td><span>23.1</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>20.3</span></td><td><span>-</span></td><td><span>-</span></td><td><span>43.7</span></td><td><span>44.8</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>LLaVA-One-Vision-0.5B</span></td><td><span>Qwen2-0.5B</span></td><td><span>28.5</span></td><td><span>20.6</span></td><td><span>-</span></td><td><span>-</span></td><td><span>44.0</span></td><td><span>26.8</span></td><td><span>-</span></td><td><span>45.8</span></td><td><span>45.5</span></td><td><span>49.2</span></td><td><span>28.7</span></td><td><span>-</span></td><td><span>55.5</span></td></tr><tr><td><span>InternVL2.5-1B</span></td><td><span>Qwen2.5-0.5B</span></td><td><span>22.5</span></td><td><span>17.5</span></td><td><span>-</span></td><td><span>-</span></td><td><span>50.3</span></td><td><span>39.8</span></td><td><span>-</span></td><td><span>47.9</span></td><td><span>64.3</span></td><td><span>-</span></td><td><span>33.3</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>InternVL3.5-1B</span></td><td><span>Qwen3-0.6B</span></td><td><span>49.9</span></td><td><span>41.8</span></td><td><span>-</span></td><td><span>-</span></td><td><span>51.0</span></td><td><span>41.5</span></td><td><span>33.0</span></td><td><span>53.0</span></td><td><span>61.0</span></td><td><span>-</span></td><td><span>32.0</span></td><td><span>-</span></td><td><span>-</span></td></tr><tr><td><span>Cambrian- <em>S</em> -0.5B</span></td><td><span>Qwen2.5-0.5B</span></td><td><span>50.6</span></td><td><span>42.2</span></td><td><span>23.4</span></td><td><span>27.9</span></td><td><span>44.0</span></td><td><span>62.4</span></td><td><span>15.7</span></td><td><span>44.0</span></td><td><span>51.8</span></td><td><span>56.0</span></td><td><span>26.0</span></td><td><span>48.5</span></td><td><span>59.8</span></td></tr></tbody></table>
			### 3.5 Empirical Results: Improved Spatial Cognition
			We next evaluate the Cambrian- *S* multimodal models to assess both the strengths and limitations of our data-driven approach.
			Table 6: VSI-Bench sub-task breakdown. Best results are bolded. Notably, even without any route planning data in training, Cambrian- *S* -7B outperforms Gemini-1.5-Pro on this task.
			<table><tbody><tr><td></td><td></td><td><p><span></span></p><p><span>Obj. Count</span></p><p></p></td><td><p><span></span></p><p><span>Abs. Dist.</span></p><p></p></td><td><p><span></span></p><p><span>Obj. Size</span></p><p></p></td><td><p><span></span></p><p><span>Room Size</span></p><p></p></td><td><p><span></span></p><p><span>Rel. Dist.</span></p><p></p></td><td><p><span></span></p><p><span>Rel. Dir.</span></p><p></p></td><td><p><span></span></p><p><span>Route Plan</span></p><p></p></td><td><p><span></span></p><p><span>Appr. Order</span></p><p></p></td></tr><tr><td><span>Methods</span></td><td><span>Avg.</span></td><td colspan="4"><span>Numerical Answer</span></td><td colspan="4"><span>Multiple-Choice Answer</span></td></tr><tr><td><span>Statistics</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>Chance Level (Random)</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>-</span></td><td><span>25.0</span></td><td><span>36.1</span></td><td><span>28.3</span></td><td><span>25.0</span></td></tr><tr><td><span>Chance Level (Frequency)</span></td><td><span>34.0</span></td><td><span>62.1</span></td><td><span>32.0</span></td><td><span>29.9</span></td><td><span>33.1</span></td><td><span>25.1</span></td><td><span>47.9</span></td><td><span>28.4</span></td><td><span>25.2</span></td></tr><tr><td><span>Proprietary Models (API)</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>GPT-4o</span></td><td><span>34.0</span></td><td><span>46.2</span></td><td><span>5.3</span></td><td><span>43.8</span></td><td><span>38.2</span></td><td><span>37.0</span></td><td><span>41.3</span></td><td><span>31.5</span></td><td><span>28.5</span></td></tr><tr><td><span>Gemini-1.5 Flash</span></td><td><span>42.1</span></td><td><span>49.8</span></td><td><span>30.8</span></td><td><span>53.5</span></td><td><span>54.4</span></td><td><span>37.7</span></td><td><span>41.0</span></td><td><span>31.5</span></td><td><span>37.8</span></td></tr><tr><td><span>Gemini-1.5 Pro</span></td><td><span>45.4</span></td><td><span>56.2</span></td><td><span>30.9</span></td><td><span>64.1</span></td><td><span>43.6</span></td><td><span>51.3</span></td><td><span>46.3</span></td><td><span>36.0</span></td><td><span>34.6</span></td></tr><tr><td><span>Gemini-2.5 Pro</span></td><td><span>51.5</span></td><td><span>43.8</span></td><td><span>34.9</span></td><td><span>64.3</span></td><td><span>42.8</span></td><td><span>61.1</span></td><td><span>47.8</span></td><td><span>45.9</span></td><td><span>71.3</span></td></tr><tr><td><span>Open-source Models</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span>Cambrian-</span> <em>S</em> <span>-7B</span></td><td><span>67.5</span></td><td><span>73.2</span></td><td><span>50.5</span></td><td><span>74.9</span></td><td><span>72.2</span></td><td><span>71.1</span></td><td><span>76.2</span></td><td><span>41.8</span></td><td><span>80.1</span></td></tr><tr><td><span>Cambrian-</span> <em>S</em> <span>-3B</span></td><td><span>57.3</span></td><td><span>70.7</span></td><td><span>40.6</span></td><td><span>68.0</span></td><td><span>46.3</span></td><td><span>64.8</span></td><td><span>61.9</span></td><td><span>27.3</span></td><td><span>78.8</span></td></tr><tr><td><span>Cambrian-</span> <em>S</em> <span>-1.5B</span></td><td><span>54.8</span></td><td><span>68.4</span></td><td><span>40.0</span></td><td><span>61.5</span></td><td><span>50.1</span></td><td><span>62.4</span></td><td><span>48.9</span></td><td><span>29.9</span></td><td><span>77.5</span></td></tr><tr><td><span>Cambrian-</span> <em>S</em> <span>-0.5B</span></td><td><span>50.6</span></td><td><span>67.9</span></td><td><span>35.4</span></td><td><span>52.2</span></td><td><span>52.5</span></td><td><span>52.3</span></td><td><span>46.5</span></td><td><span>25.8</span></td><td><span>72.2</span></td></tr></tbody></table>
			#### Improved spatial cognition.
			As shown in [Table ˜ 5](https://arxiv.org/html/2511.04670v1#S3.T5 "In 3.4 Cambrian-S: Spatially-Grounded MLLMs ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), our models achieve state-of-the-art performance in visual-spatial understanding in video. Cambrian- *S* \-7B achieves 67.5% on VSI-Bench, significantly outperforming all open-source models and surpassing the proprietary Gemini-2.5-Pro by over 16 absolute points. Since our work in this section can be viewed as a data scaling effort, a natural question is: *are the performance improvements simply due to broader data coverage (including more diverse visual configurations and question–answer pairs), or has the model actually developed stronger spatial cognition?* First, we emphasize that there is no data overlap between VSI-590K and the benchmark datasets. Although some datasets originate from the same sources (*e.g.*from ScanNet), we only use the training split, while the benchmarks use validation and test splits. Moreover, we observe clear signs of generalization in spatial reasoning. For example, in the challenging ‘‘Route Planning’’ subtask, whose question types are absent from VSI-590K because of the high annotation cost, Cambrian- *S* \-7B still performs strongly, showing pronounced scaling behavior with increasing model size too (see [Table ˜ 6](https://arxiv.org/html/2511.04670v1#S3.T6 "In 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")).
			Furthermore, our training approach proves highly effective even with smaller model sizes: our smallest *0.5B model* achieves performance comparable to Gemini-1.5 Pro on VSI-Bench. Importantly, this emphasis on spatial reasoning does not come at the expense of general capabilities: Cambrian- *S* continues to deliver competitive results on standard video benchmarks such as Perception Test [^103] and EgoSchema [^87] (see [Table ˜ 14](https://arxiv.org/html/2511.04670v1#A5.T14 "In E.2 Detailed Performance on Image and Video Benchmarks ‣ Appendix E Cambrian-S Additional Results ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") for complete results).
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="83.54" id="S3.SS5.SSS0.Px1.p3.pic1" overflow="visible" version="1.1" viewBox="0 0 600 83.54" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,83.54) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp19"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 75.52 C 0 79.95 3.59 83.54 8.03 83.54 L 591.97 83.54 C 596.41 83.54 600 79.95 600 75.52 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp19)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 70.2 C 3.54 75.62 7.93 80 13.34 80 L 593.75 80 C 599.16 80 603.54 75.62 603.54 70.2 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 75.52 C 0 79.95 3.59 83.54 8.03 83.54 L 591.97 83.54 C 596.41 83.54 600 79.95 600 75.52 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 75.52 C 1.11 79.34 4.2 82.44 8.03 82.44 L 591.97 82.44 C 595.8 82.44 598.89 79.34 598.89 75.52 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 75.52 C 1.11 79.34 4.2 82.44 8.03 82.44 L 591.97 82.44 C 595.8 82.44 598.89 79.34 598.89 75.52 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="61.96" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:4.28em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 59.27)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ Cambrian-</span> <em>S</em> <span>achieves state-of-the-art spatial sensing performance with robust generalization to unseen spatial question types, while staying competitive in general video understanding.</span></span></span></span></span></foreignObject></g></g></g></svg>
			#### Robust spatial reasoning on VSI-Bench-Debiased.
			A recent study [^14] reveals that models can rely on strong language priors for spatial reasoning tasks. For instance, when asked to estimate a table’s length, a model might leverage natural world knowledge about typical table sizes (*e.g.*, 120--180 cm) rather than analyzing the visual evidence. To investigate whether Cambrian- *S* learns to reason visually, we evaluate it on VSI-Bench-Debiased [^14], a benchmark specifically designed to eliminate language shortcuts through debiasing. As shown in [Table ˜ 5](https://arxiv.org/html/2511.04670v1#S3.T5 "In 3.4 Cambrian-S: Spatially-Grounded MLLMs ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), although performance decreases by about 8% compared to standard VSI-Bench, our models still outperform proprietary counterparts, demonstrating robust visual-spatial reasoning capabilities and confirming that our training extends beyond language-based learning.
			#### Results on VSI-Super: limitations in continual spatial sensing.
			Despite its strong performance on spatial reasoning tasks in short, pre-segmented videos from VSI-Bench, Cambrian- *S* isn’t well-equipped for continual spatial sensing. This limitation is evident in two ways. First, its performance deteriorates significantly on long videos. As shown in [Table ˜ 7](https://arxiv.org/html/2511.04670v1#S3.T7 "In Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), when evaluated on VSI-Super with 1 FPS sampling in a streaming-style setup, scores drop steadily from 38.3% to 6.0% as video length increases from 10 to 60 minutes, and the model fails completely on videos longer than 60 minutes. Second, the model has difficulty generalizing to new test scenarios. Although trained on multi-room house tour videos, it fails to handle unseen examples with just a few additional rooms. This issue isn’t simply about context length: performance drops even on short 10-minute videos that fit comfortably within model’s context window. These results highlight that a purely data-driven approach within the current MLLM framework, no matter how much data or engineering effort is invested, faces fundamental limits. Addressing these limitations calls for a paradigm shift toward AI systems that can actively model and anticipate the world while organizing their experiences more efficiently, which we explore next.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="66.94" id="S3.SS5.SSS0.Px3.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 600 66.94" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,66.94) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp20"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp20)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 53.6 C 3.54 59.01 7.93 63.4 13.34 63.4 L 593.75 63.4 C 599.16 63.4 603.54 59.01 603.54 53.6 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="45.35" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:3.08em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 42.66)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ Scaling data and models is essential, but alone it cannot unlock true spatial supersensing.</span></span></span></span></span></foreignObject></g></g></g></svg>
			Table 7: Cambrian- *S* -7B results on VSI-Super. Despite strong performance on VSI-Bench, accuracy on VSR drops sharply from 38.3% (10 min) to 0.0% (>60 min), and VSC completely fails. Note that VSI-Super focuses on continual, streaming evaluation, where uniform sampling 128 frames across the entire video does not align with the online setting; results shown in gray are provided for reference only.
			<table><tbody><tr><td></td><td colspan="5"><span>VSR</span></td><td colspan="4"><span>VSC</span></td></tr><tr><td><span>Eval Setup</span></td><td><span>10 min</span></td><td><span>30 min</span></td><td><span>60 min</span></td><td><span>120 min</span></td><td><span>240 min</span></td><td><span>10 mins</span></td><td><span>30 min</span></td><td><span>60 min</span></td><td><span>120 min</span></td></tr><tr><td><span>Uni. Sampling, 128F</span></td><td><span>26.7</span></td><td><span>21.7</span></td><td><span>23.3</span></td><td><span>30.0</span></td><td><span>28.2</span></td><td><span>16.0</span></td><td><span>0.0</span></td><td><span>0.0</span></td><td><span>0.0</span></td></tr><tr><td><span>FPS Sampling, 1FPS</span></td><td><span>38.3</span></td><td><span>35.0</span></td><td><span>6.0</span></td><td><span>0.0</span></td><td><span>0.0</span></td><td><span>0.6</span></td><td><span>0.0</span></td><td><span>0.0</span></td><td><span>0.0</span></td></tr></tbody></table>
			## 4 Predictive Sensing as a New Paradigm
			Performance of both Gemini-2.5-Flash ([Table ˜ 1](https://arxiv.org/html/2511.04670v1#S2.T1 "In State-of-the-art models struggle on VSI-Super. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")) and Cambrian- *S* ([Table ˜ 7](https://arxiv.org/html/2511.04670v1#S3.T7 "In Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")) drops sharply on VSI-Super, revealing a fundamental paradigm gap: scaling data and context alone is insufficient for supersensing. We propose *predictive sensing* as a path forward, where models learn to anticipate their sensory input and construct internal world models to handle unbounded visual streams. This design is inspired by theories of human cognition. Unlike current video multimodal models that tokenize and process entire data streams, human perception (and memory) is highly selective, retaining only a fraction of sensory input [^130]. The brain continuously updates internal models to predict incoming stimuli, compressing or discarding predictable inputs that contribute no novel information [^29]. In contrast, unexpected sensory information that violates predictions generates ‘‘surprise’’ and drives increased attention and memory encoding [^115]. We prototype this concept via a self-supervised next-latent-frame prediction approach ([Section ˜ 4.1](https://arxiv.org/html/2511.04670v1#S4.SS1 "4.1 Predictive Sensing via Latent Frame Prediction ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). The resulting prediction error serves as a control signal for two key capabilities: memory management to selectively retain important information ([Section ˜ 4.2](https://arxiv.org/html/2511.04670v1#S4.SS2 "4.2 Case Study I: Surprise-driven Memory Management System for VSI-Super Recall. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")), and event segmentation to partition unbounded streams into meaningful chunks ([Section ˜ 4.3](https://arxiv.org/html/2511.04670v1#S4.SS3 "4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). We demonstrate through two case studies on VSI-Super that this approach substantially outperforms strong long-context and streaming video model baselines.
			### 4.1 Predictive Sensing via Latent Frame Prediction
			We implement our predictive sensing paradigm through a lightweight, self-supervised module called the Latent Frame Prediction (LFP) head, which is trained jointly with the primary instruction-tuning objective. This is achieved by modifying the stage 4 training recipe as follows:
			- Latent frame prediction head. We introduce an LFP Head, a two-layer MLP that operates in parallel with the language head, to predict the latent representation of the subsequent video frame. This architecture is illustrated in the top left of [Fig.˜ 9](https://arxiv.org/html/2511.04670v1#S4.F9 "In Inference: Estimating surprise via prediction error. ‣ 4.1 Predictive Sensing via Latent Frame Prediction ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			- Learning objectives. To optimize the LFP head, we introduce two auxiliary losses, mean squared error (MSE) and cosine distance, which measure the discrepancy between the predicted latent feature and the ground truth feature of the next frame. A weighting coefficient balances the LFP loss against the primary instruction-tuning next token prediction objective.
			- Data for LFP training. We augment stage 4 data with a 290K video subset from VSI-590K used exclusively for the LFP objective. Unlike instruction tuning, these videos are sampled at a constant rate of 1 FPS to ensure uniform temporal spacing for latent frame prediction.
			During this modified stage 4 finetuning, we train the connectors, language model, and both the language and LFP heads jointly in an end-to-end manner, while keeping the SigLIP vision encoder frozen. All other training settings remain consistent with the original stage 4 configuration. For brevity, we still denote the model jointly optimized with the LFP objective as Cambrian- *S* in subsequent experiments.
			#### Inference: Estimating surprise via prediction error.
			During inference, we leverage the trained LFP head to evaluate the ‘‘surprise’’ for every incoming visual sensory input. In psychology, this framework is often described as the Violation-of-Expectation (VoE) paradigm [^17]. Specifically, during inference, video frames are fed into Cambrian- *S* at a constant sampling rate. Unless otherwise noted, the videos in the following experiments are sampled at 1 FPS before being input into the model. As the model receives incoming video frames, it continuously predicts the latent features of the next frame. We then measure the cosine distance between the model’s prediction and the actual ground truth feature of that incoming frame. This distance serves as a quantitative measure of surprise: a larger value indicating a greater deviation from the model’s learned expectations. This surprise score acts as a powerful, self-supervised guidance signal for the downstream tasks explored next.
			![Refer to caption](https://arxiv.org/html/x13.png)
			Figure 9: Training and inference pipeline for the latent frame prediction (LFP) approach. Our model employs a Latent Frame Prediction (LFP) head to predict the next frame in latent space. During training, the LFP head predicts the latent representation of the subsequent video frame. During inference, the model measures surprise by computing the cosine distance between the LFP head’s prediction and the actual latent features of the subsequent frame. The surprise signal exhibits distinct spikes for events such as the sudden appearance of unusual objects and abrupt scene changes. Our predictive-sensing prototype allows Cambrian- S to generalize to longer videos on VSI-Super, outperforming frontier models ( e.g., Gemini-2.5-Flash) that rely solely on context length expansion.
			![Refer to caption](https://arxiv.org/html/x14.png)
			Figure 10: Surprise-driven memory management framework design. The proposed memory system (a) encodes incoming visual streams, compressing frames with low surprise; (b) performs consolidation when memory is full by dropping or merging the least surprising frames; and (c) retrieves relevant frames during query answering. Color shading (dark → \\rightarrow light) reflects the degree of surprise, with hatched boxes denoting compressed frames and solid boxes representing uncompressed ones.
			### 4.2 Case Study I: Surprise-driven Memory Management System for VSI-Super Recall.
			Most current MLLMs treat all video frames equally, storing every frame without selective compression or forgetting, which limits efficiency and scalability. In this case study, we explore augmenting MLLMs with a surprise–driven memory management framework to support continual spatial-sensing question answering over long-duration videos. We show that through the surprise-guided compression, Cambrian- *S* maintains consistent accuracy and stable GPU memory footprints, independent of video length.
			#### Surprise-driven memory management system.
			Our memory management system dynamically compresses and consolidates visual streams based on the estimate of ‘‘surprise’’. As shown in [Fig.˜ 10](https://arxiv.org/html/2511.04670v1#S4.F10 "In Inference: Estimating surprise via prediction error. ‣ 4.1 Predictive Sensing via Latent Frame Prediction ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-a, we encode incoming frames using sliding window attention with fixed window size. The latent frame prediction module then measures a ‘‘surprise level’’ and assigns it to each frame’s KV caches. Frames with a surprise level below a predefined threshold undergo $2\times$ compression before being pushed into long-term memory. To maintain a stable GPU memory footprint, this long-term memory is constrained to a fixed size by a consolidation function that, once again, operates based on *surprise*: dropping or merging frames according to their surprise scores (see [Fig.˜ 10](https://arxiv.org/html/2511.04670v1#S4.F10 "In Inference: Estimating surprise via prediction error. ‣ 4.1 Predictive Sensing via Latent Frame Prediction ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-b). Finally, upon receiving a user query, the system retrieves the top- $K$ most relevant frames from the long-term memory by calculating the cosine similarity between the query and the stored frame features (see [Fig.˜ 10](https://arxiv.org/html/2511.04670v1#S4.F10 "In Inference: Estimating surprise via prediction error. ‣ 4.1 Predictive Sensing via Latent Frame Prediction ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") \-c). See [Section ˜ F.2](https://arxiv.org/html/2511.04670v1#A6.SS2 "F.2 Memory Framework Design for VSI-Super Recall ‣ Appendix F Predictive Sensing ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") for more design details. While prior works have explored memory system designs for long videos [^119], our focus is on exploring prediction errors (*i.e.*, surprise) as guiding signals.
			![Refer to caption](https://arxiv.org/html/x15.png)
			(a) VSR results
			#### Results.
			We compare Cambrian- *S* with and without the surprise-based memory system against two advanced proprietary models, Gemini-1.5-Flash [^122] and Gemini-2.5-Flash [^30], on the VSR benchmark. As shown in [Fig.˜ 11(a)](https://arxiv.org/html/2511.04670v1#S4.F11.sf1 "In Figure 11 ‣ Surprise-driven memory management system. ‣ 4.2 Case Study I: Surprise-driven Memory Management System for VSI-Super Recall. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), Cambrian- *S* (*w/* Mem.) outperforms both Gemini-1.5-Flash and Cambrian- *S* (*w/o.* Mem.) at all video lengths, demonstrating consistent spatial sensing performance across video durations. Although Gemini-2.5-Flash yields strong results for videos within an hour, it fails to process longer inputs. In addition to maintaining high accuracy, Cambrian- *S* (*w/* Mem.) also maintains stable GPU memory usage across different video lengths ([Fig.˜ 11(b)](https://arxiv.org/html/2511.04670v1#S4.F11.sf2 "In Figure 11 ‣ Surprise-driven memory management system. ‣ 4.2 Case Study I: Surprise-driven Memory Management System for VSI-Super Recall. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). This demonstrates that surprise-based memory effectively compresses redundant data without losing critical information. We include two long-video baselines, MovieChat [^119] and Flash-VStream [^159], for comparison in [Table ˜ 17](https://arxiv.org/html/2511.04670v1#A6.T17 "In F.4 Comparisons with Existing Long-video Methods ‣ Appendix F Predictive Sensing ‣ Acknowledgments ‣ 6 Conclusion ‣ Predictive Modeling ‣ 5 Related Work ‣ Summary. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video").
			#### Ablation on surprise measurement.
			Central to our surprise-based memory system is the mechanism for measuring surprise, which dictates how frames are compressed or consolidated in a passive sensing manner---without assuming any prior knowledge of future queries. Here, we compare our design, prediction error as surprise, to another straightforward baseline: adjacent-frame visual-feature similarity. Specifically, we use SigLIP2 as the vision encoder and directly compare the frame feature difference (cosine distance) between two adjacent frames. If the difference exceeds a threshold, we treat the later frame as a surprise frame. We compare these two methods across all VSR variants. For each VSR duration, we keep the experimental setup identical except for the surprise threshold, which we tune for both methods. As shown in [Fig.˜ 11(c)](https://arxiv.org/html/2511.04670v1#S4.F11.sf3 "In Figure 11 ‣ Surprise-driven memory management system. ‣ 4.2 Case Study I: Surprise-driven Memory Management System for VSI-Super Recall. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), using prediction error as the surprise measurement consistently outperforms adjacent-frame similarity across different video durations.
			<svg xmlns="http://www.w3.org/2000/svg" class="ltx_picture" height="66.94" id="S4.SS2.SSS0.Px3.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 600 66.94" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,66.94) matrix(1 0 0 -1 0 0)"><clipPath id="pgfcp21"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z"></path></clipPath><g clip-path="url(#pgfcp21)"><g color="#BFBFBF" fill="#BFBFBF" fill-opacity="0.5" stroke="#BFBFBF" stroke-opacity="0.5" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M 3.54 6.25 L 3.54 53.6 C 3.54 59.01 7.93 63.4 13.34 63.4 L 593.75 63.4 C 599.16 63.4 603.54 59.01 603.54 53.6 L 603.54 6.25 C 603.54 0.84 599.16 -3.54 593.75 -3.54 L 13.34 -3.54 C 7.93 -3.54 3.54 0.84 3.54 6.25 Z" style="stroke:none"></path></g><g></g></g><g fill-rule="evenodd"><g fill="#004D4D" fill-opacity="1.0" style="--ltx-fill-color:#004D4D;"><path d="M 0 8.03 L 0 58.91 C 0 63.35 3.59 66.94 8.03 66.94 L 591.97 66.94 C 596.41 66.94 600 63.35 600 58.91 L 600 8.03 C 600 3.59 596.41 0 591.97 0 L 8.03 0 C 3.59 0 0 3.59 0 8.03 Z M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.11 8.03 L 1.11 58.91 C 1.11 62.74 4.2 65.83 8.03 65.83 L 591.97 65.83 C 595.8 65.83 598.89 62.74 598.89 58.91 L 598.89 8.03 C 598.89 4.2 595.8 1.11 591.97 1.11 L 8.03 1.11 C 4.2 1.11 1.11 4.2 1.11 8.03 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.86 13.48)"><foreignObject color="#000000" height="45.35" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.2em;--fo_height:3.08em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 42.66)" width="556.28"><span xmlns="http://www.w3.org/1999/xhtml"><span><span><span><span>Ψ Predictive sensing provides a more principled approach to modeling the spatiotemporal dynamics of video data than static similarity measures based on per-frame features.</span></span></span></span></span></foreignObject></g></g></g></svg>
			While our current system employs a simple predictive head as an initial prototype, future integration of a more capable world model could produce richer and more reliable surprise signals, ultimately enabling broader advances in spatial supersensing.
			### 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count.
			While VSR focuses on evaluating the long-term observation and recall abilities of MLLMs, a more challenging test of supersensing would involve testing a model’s capacity to interpret its sensory input, navigate across varied environments, and perform cumulative, multihop reasoning. For example, the model might need to complete a task in one environment, move to another, and ultimately integrate information from all experiences to reach a final decision.
			![Refer to caption](https://arxiv.org/html/x18.png)
			Figure 12: Illustration of our surprise-driven event segmentation framework for VSC. The model continuously accumulates frame features in an event buffer. When a high-surprise frame is detected, the buffered features are summarized to produce a segment-level answer, and the buffer is cleared to start a new segment. This process repeats until the end of the video, after which all segment answers are aggregated to form the final output. Color shading (dark → \\rightarrow light) reflects the degree of surprise.
			#### Surprise-driven event segmentation.
			An *event* can be understood as a spatiotemporally coherent segment of experience [^64]. In the context of spatial supersensing, an event corresponds to a continuous experience of being situated within a specific space and sensing its environment. This definition emphasizes that real sensory experience is typically organized into locally coherent segments---episodes where perceptual, spatial, and temporal features remain relatively stable or consistent. Event segmentation, then, is the process of parsing a continuous stream of sensory input into discrete, meaningful units based on changes in this coherence. Such segmentation is essential for reasoning and behavior [^37]: it allows an agent (biological or artificial) to form structured representations of experience, detect boundaries where significant change occurs, and update predictions about the environment accordingly. Recent studies highlight that prediction error and changes in working memory/context are two possible mechanisms driving segmentation [^98].
			In the VSI-Super Count (VSC) benchmark, we examine a simple setting where surprise is used to segment continuous visual input, identifying scene changes as natural breakpoints that divide the video stream into *spatially coherent* segments. This approach also parallels human problem-solving: when counting objects across a large area, people typically focus on one section at a time before combining the results. This behavior is also related to the ‘‘doorway effect’’ [^106], in which passing through a doorway or entering a new room creates a natural boundary in memory. As illustrated in [Fig.˜ 12](https://arxiv.org/html/2511.04670v1#S4.F12 "In 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), the model continuously accumulates frame features in an event buffer. When a high-surprise frame is detected, the buffered features are summarized to produce a segment-level answer, and the buffer is cleared to start a new segment. This cycle repeats until the end of the video, after which all segment answers are aggregated to form the final output.
			![Refer to caption](https://arxiv.org/html/x19.png)
			(a) VSC results
			![Refer to caption](https://arxiv.org/html/x22.png)
			Figure 14: Cambrian- S scales to higher ground truth object counts whereas Gemini saturates. Predicted counts are plotted against ground-truth counts for videos of different lengths (10, 30, 60, and 120 minutes). Using surprise-driven segmentation, Cambrian- ’s predicted counts grow approximately linearly with the ground-truth, tracking the y = x y=x perfect-count line (gray dashed), whereas Gemini-2.5-Flash’s predicted counts remain clustered near small values and fail to increase with ground-truth count, indicating early saturation and poor extrapolation to larger counts.
			#### Results.
			Gemini-1.5-Flash attains near-zero performance on VSC ([Fig.˜ 13(a)](https://arxiv.org/html/2511.04670v1#S4.F13.sf1 "In Figure 13 ‣ Surprise-driven event segmentation. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")), showing the task’s difficulty. Although Gemini-2.5-Flash yields much better results on 10-minute videos, its performance declines rapidly on longer videos. In contrast, the surprise-driven event segmentation approach used by Cambrian- *S* (*w/* Surprise Seg.) achieves higher and more stable performance across all video lengths. When the video is segmented using ground-truth scene transitions (*i.e.*, Cambrian- *S* *w/* GT Seg.), performance improves further, representing an approximate upper bound. A deeper analysis in [Fig.˜ 14](https://arxiv.org/html/2511.04670v1#S4.F14 "In Surprise-driven event segmentation. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video") reveals that Gemini-2.5-Flash’s predictions are confined to a limited range and do not scale as more objects appear in the video. In contrast, Cambrian- *S* (*w/* Surprise Seg.) produces counts that, while not yet fully accurate, exhibit a stronger correlation with the true object numbers, indicating better generalization.
			#### Ablation on surprise measurement.
			We compare our surprise-driven approach with a baseline using adjacent-frame feature similarity ([Fig.˜ 13(b)](https://arxiv.org/html/2511.04670v1#S4.F13.sf2 "In Figure 13 ‣ Surprise-driven event segmentation. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video")). For both methods, we report the best results after hyperparameter tuning. Consistent with our observations in VSR, using prediction error as a measure of surprise consistently outperforms appearance similarity across all video durations by a notable margin.
			#### Evaluation in streaming setup.
			As the correct answer in VSC evolves throughout the video, we create a streaming QA setup where the same question is asked at 10 different timestamps. The final performance is averaged across all queries. We benchmark against commercial MLLMs marketed for live visual input. As shown in [Fig.˜ 13(c)](https://arxiv.org/html/2511.04670v1#S4.F13.sf3 "In Figure 13 ‣ Surprise-driven event segmentation. ‣ 4.3 Case Study II: Surprise-driven continual video segment for VSI-Super Count. ‣ 4 Predictive Sensing as a New Paradigm ‣ Results on VSI-Super: limitations in continual spatial sensing. ‣ 3.5 Empirical Results: Improved Spatial Cognition ‣ 3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMs ‣ 3rd item ‣ 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"), although Gemini-Live and GPT-Realtime are intended for streaming scenarios, they achieve under 15% MRA on 10-minute videos and their performance declines to near zero on 120-minute streams. Cambrian- *S*, however, shows stronger performance, reaching 38% MRA on 10-minute streams and maintaining around 28% at 120 minutes.
			#### Summary.
			Across both VSR recall and VSC counting tasks, predictive sensing through surprise-driven memory and event segmentation enables Cambrian- *S* to overcome the fixed-context limitations described in [Section ˜ 3](https://arxiv.org/html/2511.04670v1#S3 "3 Spatial Sensing Under the Current Paradigm ‣ How VSI-Super challenges the current paradigm. ‣ 2.2 VSI-Super: Towards Benchmarking Spatial Supersensing in Multimodal LLMsIn 2nd item ‣ 3rd item ‣ 2.1 Deconstructing Existing Video Benchmarks ‣ 2 Benchmarking Spatial Supersensing ‣ Cambrian-S: Towards Spatial Supersensing in Video"). Although this remains an early prototype, it highlights the potential for building AI systems that not only see but also anticipate, select, and organize experience. Such systems move beyond frame-level Q&A toward constructing implicit world models that support deeper spatial reasoning, scale across unbounded temporal horizons, and achieve supersensing that rivals and ultimately surpasses human visual intelligence.
			## 5 Related Work
			#### Video Multimodal Large Language Models
			The strong linguistic understanding capabilities of pretrained LLMs [^15], combined with the representational power of vision foundation models used as feature extractors [^105], have driven significant advances in extending these models beyond text to achieve semantic perception of visual content, primarily in the image domain [^56]. This momentum has spurred growing research into video-based MLLMs [^74], which are seen as a key step toward connecting multimodal intelligence with real-world applications such as embodied agents [^61]. As emphasized throughout this paper, developing a truly capable supersensing system requires rethinking several core aspects, including how progress is benchmarked, what constitutes the right data, which architectural designs are most effective, and what modeling objectives best align with the system’s goals.
			#### Streaming Video Understanding
			Video is a continuous and potentially infinite stream of visual signals. While humans process it effortlessly, its unbounded nature challenges video MLLMs because token lengths increase with duration, causing rising computational and storage costs. Recent work has explored several approaches to address this problem: Efficient architectural design. The quadratic cost of self-attention makes it hard to handle long videos. Recent methods [^70] use simpler, faster architectures [^135] that reduce computation and work better with longer inputs. Context window expansion. The fixed context length in pre-trained LLMs limits their understanding of long-term content. Recent work [^26] extends this window by careful system design, enabling models to handle and reason over longer video sequences. Retrieval-augmented video understanding. To process long videos, some approaches retrieve only the most relevant segments from a larger collection [^63] and use them as context for further analysis.Visual token reduction or compression. Other methods shorten the input by reducing visual tokens across or within frames [^117], making it easier to handle long video sequences. While these methods improve performance, they largely treat continuous videos as standard sequence modeling problems, similar to text. We believe future MLLMs should build internal predictive models to efficiently process continuous visual streams, as humans do.
			#### Visual Spatial Intelligence
			Understanding spatial relationships from visual inputs is crucial for perceiving and interacting with the physical world. As multimodal models become more physically grounded, interest in spatial intelligence has surged, leading to new benchmarks [^148] and research focused on enhancing models’ spatial reasoning capabilities [^151]. In this paper, we study visual spatial intelligence through the concept of spatial supersensing in videos and explore ways to strengthen MLLMs’ spatial reasoning by refining data curation, optimizing training strategies, and introducing new paradigms.
			#### Predictive Modeling
			A learned internal predictive model [^31] allows an intelligent agent to represent and simulate aspects of its environment, enabling more effective planning and decision-making. Model predictive control (MPC) [^43] applies similar principles in control theory, leveraging internal forward models to anticipate future trajectories and select optimal actions in real time. This concept draws inspiration from how humans form mental models of the world [^108] and how these internal representations influence behavior (*e.g.*, *unconscious inference* [^130]), serving as simplified abstractions of reality that enable prediction and efficient action. A growing body of work has explored the idea of predictive modeling through self-supervised representation learning [^5], and text- or action-conditioned video generation [^164]. In this paper, motivated by how humans leverage internal world models to process unbounded sensory input efficiently and effectively, we investigate how to equip MLLMs with a similar predictive sensing capability.
			## 6 Conclusion
			We highlight the importance of and propose a hierarchy for spatial *supersensing* capabilities in videos, arguing that achieving superintelligence requires AI systems to move beyond text-based knowledge and semantic perception, the current focus of most MLLMs, to also develop spatial cognition and predictive world models. To measure progress, we introduce VSI-Super and find that current MLLMs struggle with it. To test whether current progress is limited by data, we curate VSI-590K and train our spatially grounded MLLM, Cambrian- *S*, on it. Although Cambrian- *S* performs well on standard benchmarks, its results on VSI-Super reveal the limitations of the current MLLM paradigm. We prototype predictive sensing, using latent frame prediction and surprise estimation to handle unbounded visual streams. It improves Cambrian- *S* performance on VSI-Super and marks an early step toward spatial supersensing.
			Limitations. Our goal is to present a conceptual framework that encourages the community to reconsider the importance of developing spatial supersensing. As a long-term research direction, our current benchmark, dataset, and model design remain limited in quality, scale, and generalizability, and the prototype serves only as a proof of concept. Future work should explore more diverse and embodied scenarios and build stronger connections with recent advances in vision, language, and world modeling.
			## Acknowledgments
			We are grateful to Cambrian-1 [^124] for the excellent codebase, which served as the launching point for our research. Thanks to the TorchXLA team for helpful discussions on TPU, TorchXLA, and JAX distributed training infrastructure. We also thank Anjali Gupta, Sihyun Yu, Oscar Michel, Boyang Zheng, Xichen Pan, Weiyang Jin, and Arijit Ray for reviewing this manuscript and providing constructive feedback. This work was primarily supported by the Google TPU Research Cloud (TRC) program and the Google Cloud Research Credits program (GCP19980904). E.B. is supported by the DoD NDSEG Fellowship Program. S.X. acknowledges support from the MSIT IITP grant (RS-2024-00457882) and the NSF award IIS-2443404.
			## References

[^1]: Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXiv preprint arXiv:2303.08774, 2023.

[^2]: Triantafyllos Afouras, Effrosyni Mavroudi, Tushar Nagarajan, Huiyu Wang, and Lorenzo Torresani.Ht-step: Aligning instructional articles with how-to videos.In NeurIPS, 2023.

[^3]: Anthropic.Introducing claude 3.5 sonnet.[https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet), 2024.

[^4]: Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese.3d semantic parsing of large-scale indoor spaces.In CVPR, 2016.

[^5]: Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas.Self-supervised learning from images with a joint-embedding predictive architecture.In CVPR, 2023.

[^6]: Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al.V-jepa 2: Self-supervised video models enable understanding, prediction and planning.arXiv preprint arXiv:2506.09985, 2025.

[^7]: Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.Qwen technical report.arXiv preprint arXiv:2309.16609, 2023.

[^8]: Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.Qwen-vl: A frontier large vision-language model with versatile abilities.arXiv preprint arXiv:2308.12966, 2023.

[^9]: Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al.Qwen2.5-vl technical report.arXiv preprint arXiv:2502.13923, 2025.

[^10]: Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, and Jitendra Malik.Whole-body conditioned egocentric video prediction.arXiv preprint arXiv:2506.21552, 2025.

[^11]: Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun.Navigation world models.In CVPR, 2025.

[^12]: Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman.ARKitscenes - a diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data.In NeurIPS, 2021.

[^13]: Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, and Saining Xie.SIMS-V: Simulated instruction-tuning for spatial video understanding.arXiv preprint, 2025.

[^14]: Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, and Saining Xie.Benchmark designers should ‘‘train on the test set’’ to expose exploitable non-visual shortcuts.arXiv preprint, 2025.

[^15]: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners.In NeurIPS, 2020.

[^16]: Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Xindong He, Xu Huang, et al.Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems.In IROS, 2025.

[^17]: Judee K Burgoon and Jerold L Hale.Nonverbal expectancy violations: Model elaboration and application to immediacy behaviors.Communications Monographs, 55(1):58--79, 1988.

[^18]: Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao.Spatialbot: Precise spatial understanding with vision language models.In ICRA, 2025.

[^19]: Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher D Manning.Auroracap: Efficient, performant video detailed captioning and a new benchmark.In ICLR, 2025.

[^20]: Keshigeyan Chandrasegaran, Agrim Gupta, Lea M Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li.Hourvideo: 1-hour video-language understanding.In NeurIPS, 2024.

[^21]: Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia.Spatialvlm: Endowing vision-language models with spatial reasoning capabilities.In CVPR, 2024.

[^22]: Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre, and Sungjin Ahn.Simple hierarchical planning with diffusion.In ICLR, 2024.

[^23]: Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et al.Gui-world: A video benchmark and dataset for multimodal gui-oriented understanding.In ICLR, 2025.

[^24]: Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou.Videollm-online: Online video large language model for streaming video.In CVPR, 2024.

[^25]: Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al.Scaling rl to long videos.In NeurIPS, 2025.

[^26]: Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al.Longvila: Scaling long-context visual language models for long videos.In ICLR, 2025.

[^27]: Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.In CVPR, 2024.

[^28]: An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu.Spatialrgpt: Grounded spatial reasoning in vision-language models.In NeurIPS, 2024.

[^29]: Andy Clark.Whatever next? predictive brains, situated agents, and the future of cognitive science.Behavioral and brain sciences, 2013.

[^30]: Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al.Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.arXiv preprint arXiv:2507.06261, 2025.

[^31]: Kenneth James Williams Craik.The nature of explanation.CUP Archive, 1967.

[^32]: Erfei Cui, Yinan He, Zheng Ma, Zhe Chen, Hao Tian, Weiyun Wang, Kunchang Li, Yi Wang, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, Yali Wang, Limin Wang, Yu Qiao, and Jifeng Dai.Sharegpt-4o: Comprehensive multimodal annotations with gpt-4o, 2024.

[^33]: Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner.Scannet: Richly-annotated 3d reconstructions of indoor scenes.In CVPR, 2017.

[^34]: Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.Flashattention: Fast and memory-efficient exact attention with io-awareness.In NeurIPS, 2022.

[^35]: Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier.Language modeling with gated convolutional networks.In ICML, 2017.

[^36]: Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi.Procthor: Large-scale embodied ai using procedural generation.In NeurIPS, 2022.

[^37]: Peter Ford Dominey.Narrative event segmentation in the cortical reservoir.PLOS Computational Biology, 17(10):e1008993, 2021.

[^38]: Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei.Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models.In ACL, 2024.

[^39]: David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al.Scaling language-free visual representation learning.In ICCV, 2025.

[^40]: Li Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona.What do we perceive in a glance of a real-world scene?Journal of vision, 2007.

[^41]: Karl Friston.The free-energy principle: a unified brain theory?Nature reviews neuroscience, 2010.

[^42]: Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al.Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.In CVPR, 2025.

[^43]: Carlos E Garcia, David M Prett, and Manfred Morari.Model predictive control: Theory and practice—a survey.Automatica, 25(3):335--348, 1989.

[^44]: Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, and Yann LeCun.Intuitive physics understanding emerges from self-supervised pretraining on natural videos.arXiv preprint arXiv:2502.11831, 2025.

[^45]: Samuel J Gershman, Marie-H Monfils, Kenneth A Norman, and Yael Niv.The computational nature of memory modification.Elife, 2017.

[^46]: James J Gibson.The ecological approach to visual perception: classic edition.Psychology press, 2014.

[^47]: Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.Ego4d: Around the world in 3,000 hours of egocentric video.In CVPR, 2022.

[^48]: Albert Gu and Tri Dao.Mamba: Linear-time sequence modeling with selective state spaces.In COLM, 2024.

[^49]: David Ha and Jürgen Schmidhuber.World models.arXiv preprint arXiv:1803.10122, 2018.

[^50]: Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.Masked autoencoders are scalable vision learners.In CVPR, 2022.

[^51]: D Hendrycks.Gaussian error linear units (gelus).arXiv preprint arXiv:1606.08415, 2016.

[^52]: Jakob Hohwy.The predictive mind.OUP Oxford, 2013.

[^53]: Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu.Video-MMMU: Evaluating knowledge acquisition from multi-discipline professional videos.arXiv preprint arXiv:2501.13826, 2025.

[^54]: Zi-Yuan Hu, Shuo Liang, Duo Zheng, Yanyang Li, Yeyao Tao, Shijia Huang, Wei Feng, Jia Qin, Jianguang Yu, Jing Huang, et al.Nemo: Needle in a montage for video-language understanding.arXiv preprint arXiv:2509.24563, 2025.

[^55]: Drew A Hudson and Christopher D Manning.Gqa: A new dataset for real-world visual reasoning and compositional question answering.In CVPR, 2019.

[^56]: Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.Gpt-4o system card.arXiv preprint arXiv:2410.21276, 2024.

[^57]: Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, et al.Token-efficient long video understanding for multimodal llms.arXiv preprint arXiv:2503.04130, 2025.

[^58]: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.Transformers are rnns: Fast autoregressive transformers with linear attention.In ICML, 2020.

[^59]: Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.A diagram is worth a dozen images.In ECCV, 2016.

[^60]: Nicholas GW Kennedy, Jessica C Lee, Simon Killcross, R Fred Westbrook, and Nathan M Holmes.Prediction error determines how memories are organized in the brain.Elife, 2024.

[^61]: Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al.Openvla: An open-source vision-language-action model.arXiv preprint arXiv:2406.09246, 2024.

[^62]: Kristin Koch, Judith McLean, Ronen Segev, Michael A Freed, Michael J Berry, Vijay Balasubramanian, and Peter Sterling.How much the eye tells the brain.Current biology, 2006.

[^63]: Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, and Federico Tombari.Text-conditioned resampler for long form video understanding.In ECCV, 2024.

[^64]: Christopher A Kurby and Jeffrey M Zacks.Segmentation in the perception and memory of events.Trends in cognitive sciences, 12(2):72--79, 2008.

[^65]: Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al.Llava-onevision: Easy visual task transfer.TMLR, 2025.

[^66]: Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan.Seed-bench: Benchmarking multimodal large language models.In CVPR, 2024.

[^67]: Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, and Ivan Vulić.Topviewrs: Vision-language models as top-view spatial reasoners.In EMNLP, 2024.

[^68]: Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.In ICML, 2023.

[^69]: KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.Videochat: Chat-centric video understanding.arXiv preprint arXiv:2305.06355, 2023.

[^70]: Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao.Videomamba: State space model for efficient video understanding.In ECCV, 2024.

[^71]: Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.MVbench: A comprehensive multi-modal video understanding benchmark.In CVPR, 2024.

[^72]: Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie.Lion-fs: Fast & slow video-language thinker as online video assistant.In CVPR, 2025.

[^73]: Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al.Videochat-flash: Hierarchical compression for long-context video modeling.arXiv preprint arXiv:2501.00574, 2024.

[^74]: Yanwei Li, Chengyao Wang, and Jiaya Jia.Llama-vid: An image is worth 2 tokens in large language models.In ECCV, 2024.

[^75]: Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao.Sti-bench: Are mllms ready for precise spatial-temporal world understanding?In ICCV, 2025.

[^76]: Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, and Ranjay Krishna.Coarse correspondences boost spatial-temporal reasoning in multimodal language model.In CVPR, 2025.

[^77]: Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.Improved baselines with visual instruction tuning.In CVPR, 2024.

[^78]: Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning.In NeurIPS, 2023.

[^79]: Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.Lost in the middle: How language models use long contexts.In ACL, 2024.

[^80]: Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al.Grounding dino: Marrying dino with grounded pre-training for open-set object detection.In ECCV, 2024.

[^81]: Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al.Mmbench: Is your multi-modal model an all-around player?In ECCV, 2024.

[^82]: Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai.Ocrbench: on the hidden mystery of ocr in large multimodal models.SCIS, 2024.

[^83]: Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.In ICLR, 2024.

[^84]: Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, and Alan Yuille.Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning.In NeurIPS, 2025.

[^85]: Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan.Videogpt+: Integrating image and video encoders for enhanced video understanding.arXiv preprint arXiv:2406.09418, 2024.

[^86]: Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al.Openeqa: Embodied question answering in the era of foundation models.In CVPR, 2024.

[^87]: Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik.Egoschema: A diagnostic benchmark for very long-form video language understanding.In NeurIPS, 2023.

[^88]: Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Ghosh, Arjun Karpur, Koert Chen, Ye Xia, Bingyi Cao, Daniel Salz, Guangxing Han, Jan Dlabal, et al.Tips: Text-image pretraining with spatial awareness.In ICLR, 2024.

[^89]: Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf.Smolvlm: Redefining small and efficient multimodal models.arXiv preprint arXiv:2504.05299, 2025.

[^90]: David Marr.Vision: A computational investigation into the human representation and processing of visual information.MIT press, 2010.

[^91]: Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.Chartqa: A benchmark for question answering about charts with visual and logical reasoning.In ACL, 2022.

[^92]: Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.Docvqa: A dataset for vqa on document images.In WACV, 2021.

[^93]: Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al.Mm1: methods, analysis and insights from multimodal llm pre-training.In ECCV, 2024.

[^94]: Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.Howto100m: Learning a text-video embedding by watching hundred million narrated video clips.In ICCV, 2019.

[^95]: Beren Millidge, Tommaso Salvatori, Yuhang Song, Rafal Bogacz, and Thomas Lukasiewicz.Predictive coding: Towards a future of deep learning beyond backpropagation?In IJCAI, 2022.

[^96]: Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, and Federico Tombari.Silc: Improving vision language pretraining with self-distillation.In ECCV, 2024.

[^97]: Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al.Ovo-bench: How far is your video-llms from real-world online video understanding?In CVPR, 2025.

[^98]: Sophie Nolden, Gözem Turan, Berna Güler, and Eren Günseli.Prediction error and event segmentation in episodic memory.Neuroscience & Biobehavioral Reviews, 157:105533, 2024.

[^99]: Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun.Spacer: Reinforcing mllms in video spatial reasoning.arXiv preprint arXiv:2504.01805, 2025.

[^100]: Abby O’Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al.Open x-embodiment: Robotic learning datasets and rt-x models.In ICRA, 2024.

[^101]: Junwen Pan, Rui Zhang, Xin Wan, Yuan Zhang, Ming Lu, and Qi She.Timesearch: Hierarchical video search with spotlight and reflection for human-like long video understanding.arXiv preprint arXiv:2504.01407, 2025.

[^102]: Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren.Aria digital twin: A new benchmark dataset for egocentric 3d machine perception.In ICCV, 2023.

[^103]: Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al.Perception test: A diagnostic benchmark for multimodal video models.In NeurIPS, 2023.

[^104]: Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang.Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction.In CVPR, 2025.

[^105]: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.Learning transferable visual models from natural language supervision.In ICML, 2021.

[^106]: Gabriel A Radvansky, Sabine A Krawietz, and Andrea K Tamplin.Walking through doorways causes forgetting: Further explorations.Quarterly journal of experimental psychology, 2011.

[^107]: Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun.Does spatial cognition emerge in frontier models?In ICLR, 2025.

[^108]: Rajesh PN Rao and Dana H Ballard.Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.Nature neuroscience, 1999.

[^109]: Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al.Sam 2: Segment anything in images and videos.In ICLR, 2025.

[^110]: Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko.SAT: Spatial Aptitude Training for Multimodal Language Models.In COLM, 2025.

[^111]: Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou.Timechat: A time-sensitive multimodal large language model for long video understanding.In CVPR, 2024.

[^112]: Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen.Vamba: Understanding hour-long videos with hybrid mamba-transformers.In ICCV, 2025.

[^113]: Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M Susskind.Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding.In ICCV, 2021.

[^114]: Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya.Scienceqa: A novel resource for question answering on scholarly articles.IJDL, 2022.

[^115]: Wolfram Schultz and Anthony Dickinson.Neuronal coding of prediction errors.Annual review of neuroscience, 2000.

[^116]: Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan.Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models.In ICLR, 2025.

[^117]: Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al.Longvu: Spatiotemporal adaptive compression for long video-language understanding.In ICML, 2025.

[^118]: Sunjae Shim, Franck B Mugisho, Lila Davachi, and Christopher Baldassano.Generating event boundaries in memory without prediction error.PsyArXiv Preprints, 2024.

[^119]: Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al.Moviechat: From dense token to sparse memory for long video understanding.In CVPR, 2024.

[^120]: Aimee E Stahl and Lisa Feigenson.Observing the unexpected enhances infants’ learning and exploration.Science, 2015.

[^121]: Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al.Gemini: a family of highly capable multimodal models.arXiv preprint arXiv:2312.11805, 2023.

[^122]: Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al.Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.arXiv preprint arXiv:2403.05530, 2024.

[^123]: Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al.Gemini robotics: Bringing ai into the physical world.arXiv preprint arXiv:2503.20020, 2025.

[^124]: Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri Iyer, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie.Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs.In NeurIPS, 2024.

[^125]: Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.Eyes wide shut? exploring the visual shortcomings of multimodal llms.In CVPR, 2024.

[^126]: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023.

[^127]: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.

[^128]: Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al.Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features.arXiv preprint arXiv:2502.14786, 2025.

[^129]: Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, and Vittorio Ferrari.Connecting vision and language with video localized narratives.In CVPR, 2023.

[^130]: Hermann Von Helmholtz.Handbuch der physiologischen Optik.L. Voss, 1867.

[^131]: Bo Wan, Michael Tschannen, Yongqin Xian, Filip Pavetic, Ibrahim M Alabdulmohsin, Xiao Wang, André Susano Pinto, Andreas Steiner, Lucas Beyer, and Xiaohua Zhai.Locca: Visual pretraining with location-aware captioners.In NeurIPS, 2024.

[^132]: Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang, and Mike Zheng Shou.Cosmo: Contrastive streamlined multimodal model with interleaved pre-training.arXiv preprint arXiv:2401.00849, 2024.

[^133]: Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny.Vggt: Visual geometry grounded transformer.In CVPR, 2025.

[^134]: Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al.Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.arXiv preprint arXiv:2409.12191, 2024.

[^135]: Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.Linformer: Self-attention with linear complexity.arXiv preprint arXiv:2006.04768, 2020.

[^136]: Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy.Videoagent: Long-form video understanding with large language model as agent.In ECCV, 2024.

[^137]: Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al.Internvideo2: Scaling foundation models for multimodal video understanding.In ECCV, 2024.

[^138]: Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al.Videorope: What makes for good video rotary position embedding?In ICML, 2025.

[^139]: Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Vivian Chen, and Hung-yi Lee.Streambench: Towards benchmarking continuous improvement of language agents.In NeurIPS, 2024.

[^140]: Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li.Longvideobench: A benchmark for long-context interleaved video-language understanding.In NeurIPS, 2024.

[^141]: xAI.Grok-1.5 Vision Preview.[https://x.ai/blog/grok-1-5v](https://x.ai/blog/grok-1-5v), April 2024.RealworldQA, Blog post, Announced on April 12, 2024.

[^142]: Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin J Liang.Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models.arXiv preprint arXiv:2505.17015, 2025.

[^143]: Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al.Gspmd: general and scalable parallelization for ml computation graphs.arXiv preprint arXiv:2105.04663, 2021.

[^144]: Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo.Advancing high-resolution video-language representation with large-scale video transcriptions.In CVPR, 2022.

[^145]: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, et al.Qwen2.5 technical report.arXiv preprint arXiv:2412.15115, 2024.

[^146]: Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao.Vript: A video is worth thousands of words.In NeurIPS, 2024.

[^147]: Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie.V-IRL: Grounding virtual intelligence in real life.In ECCV, 2024.

[^148]: Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie.Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces.In CVPR, 2024.

[^149]: Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, et al.Egolife: Towards egocentric life assistant.In CVPR, 2025.

[^150]: Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel.Learning interactive real-world simulators.In ICLR, 2024.

[^151]: Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, and Chuang Gan.Mindjourney: Test-time scaling with world models for spatial reasoning.arXiv preprint arXiv:2507.12508, 2025.

[^152]: Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Ruoyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma.Seeing from another perspective: Evaluating multi-view understanding in mllms.arXiv preprint arXiv:2504.15280, 2025.

[^153]: Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai.Scannet++: A high-fidelity dataset of 3d indoor scenes.In ICCV, 2023.

[^154]: Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al.Spatial mental modeling from limited views.arXiv preprint arXiv:2506.21458, 2025.

[^155]: Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.A survey on multimodal large language models.National Science Review, 2024.

[^156]: Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al.Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.In CVPR, 2024.

[^157]: Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.Sigmoid loss for language image pre-training.In ICCV, 2023.

[^158]: Hang Zhang, Xin Li, and Lidong Bing.Video-llama: An instruction-tuned audio-visual language model for video understanding.In EMNLP, 2023.

[^159]: Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin.Flash-vstream: Memory-based real-time understanding for long video streams.In ICCV, 2025.

[^160]: Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu.Long context transfer from language to vision.arXiv preprint arXiv:2406.16852, 2024.

[^161]: Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.Video instruction tuning with synthetic data.TMLR, 2025.

[^162]: Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing Liu.Needle in a video haystack: A scalable synthetic evaluator for video mllms.arXiv preprint arXiv:2406.09367, 2024.

[^163]: Jieyu Zheng and Markus Meister.The unbearable slowness of being: Why do we live at 10 bits/s?Neuron, 2025.

[^164]: Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto.Dino-wm: World models on pre-trained visual features enable zero-shot planning.In ICML, 2025.

[^165]: Luowei Zhou, Chenliang Xu, and Jason Corso.Towards automatic learning of procedures from web instructional videos.In AAAI, 2018.

[^166]: Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu.Llava-3d: A simple yet effective pathway to empowering lmms with 3d-awareness.In ICCV, 2025.

[^167]: Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al.Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models.arXiv preprint arXiv:2504.10479, 2025.

[^168]: Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al.Apollo: An exploration of video understanding in large multimodal models.In CVPR, 2025.