---
title: 'HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with
  Continuous Structure Tokens'
authors:
- Yi Zhou
- Haohao Qu
- Yunqing Liu
- Shanru Lin
- Le Song
- Wenqi Fan
date: '2025-12-17'
source: arxiv
categories:
- cs.CE
- cs.AI
pdf_url: https://arxiv.org/pdf/2512.15133v1
tags:
- paper
- source/arxiv
- topic/cs-CE
- topic/cs-AI
arxiv_id: 2512.15133v1
---

# HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens

**Authors:** Yi Zhou, Haohao Qu, Yunqing Liu, Shanru Lin, Le Song (+1 more)

**Date:** 2025-12-17

**Source:** arxiv | **Categories:** cs.CE, cs.AI

ðŸ“„ [PDF](https://arxiv.org/pdf/2512.15133v1)

## Abstract

Proteins inherently possess a consistent sequence-structure duality. The abundance of protein sequence data, which can be readily represented as discrete tokens, has driven fruitful developments in protein language models (pLMs). A key remaining challenge, however, is how to effectively integrate continuous structural knowledge into pLMs. Current methods often discretize protein structures to accommodate the language modeling framework, which inevitably results in the loss of fine-grained information and limits the performance potential of multimodal pLMs. In this paper, we argue that such concerns can be circumvented: a sequence-based pLM can be extended to incorporate the structure modality through continuous tokens, i.e., high-fidelity protein structure latents that avoid vector quantization. Specifically, we propose a hybrid diffusion protein language model, HD-Prot, which embeds a continuous-valued diffusion head atop a discrete pLM, enabling seamless operation with both discrete and continuous tokens for joint sequence-structure modeling. It captures inter-token dependencies across modalities through a unified absorbing diffusion process, and estimates per-token distributions via categorical prediction for sequences and continuous diffusion for structures. Extensive empirical results show that HD-Prot achieves competitive performance in unconditional sequence-structure co-generation, motif-scaffolding, protein structure prediction, and inverse folding tasks, performing on par with state-of-the-art multimodal pLMs despite being developed under limited computational resources. It highlights the viability of simultaneously estimating categorical and continuous distributions within a unified language model architecture, offering a promising alternative direction for multimodal pLMs.

## Notes

<!-- Add your notes here -->
