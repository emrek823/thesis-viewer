---
source_pdf: "https://drive.google.com/file/d/1p0gd6y6EkZHCpQBSNhELAN4L1kD2MYkF/view"
drive_folder: "Research"
type: research

ingested: 2025-12-28
original_filename: "2505.21908v1.pdf"
---

> **Original:** [View Original PDF](https://drive.google.com/file/d/1p0gd6y6EkZHCpQBSNhELAN4L1kD2MYkF/view)

# Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding

**Hanyin Wang$^{1,2}$ Zhenbang Wu$^2$ Gururaj Kolar$^3$ Hariprasad Korsapati$^1$ Brian Bartlett$^1$ Bryan Hull Jimeng Sun$^2$**
$^1$Mayo Clinic Health System
$^2$University of Illinois Urbana-Champaign
$^3$Mayo Clinic Rochester Mayo Clinic Phoenix
wang.hanyin@mayo.edu

arXiv:2505.21908v1 [cs.LG] 28 May 2025

## Abstract

Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-SAPPHIRE, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-SAPPHIRE introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.

### Figure 1: Main Results.
(A) Accuracy of DRG coding on the MIMIC-IV test set (N=26,244). DRG-SAPPHIRE outperforms proprietary reasoning models and the previous SOTA model, DRG-LLaMA. Notably, classification models could not generate reasoning for DRG code assignments.
(B) Best RL performance increases linearly with the logarithm of the SFT sample sizes. Dashed line marks where 50% of training data was used for SFT. Best results from vanilla GRPO runs are shown.

#### A. DRG-Sapphire vs Baselines (Accuracy %)

| Model Name                | Accuracy (%) | Model Type         |
| :------------------------ | :----------- | :----------------- |
| DRG-Sapphire (ours)       | 54.8         | Generative Model   |
| DRG-LLAMA-7B              | 53.9         | Generative Model   |
| ClinicalBERT              | 50.2         | Classification Model |
| CAML                      | 44.7         | Classification Model |
| GPT-4o                    | 8.8          | Generative Model   |
| 03-Mini                   | 3.3          | Generative Model   |
| R1-Distill-Qwen-32B       | 0.5          | Generative Model   |

#### B. Best RL Accuracy vs SFT Sample Size (Accuracy %)

| SFT Sample Size before RL (log2 scale) | SFT Accuracy (%) | GRPO Accuracy (%) | Full Data, 50% SFT Accuracy (%) |
| :------------------------------------- | :--------------- | :---------------- | :------------------------------ |
| 2^11 (2048)                            | ~35              | ~40               | N/A                             |
| 2^12 (4096)                            | ~38              | ~42               | N/A                             |
| 2^13 (8192)                            | ~42              | ~45               | N/A                             |
| 2^14 (16384)                           | ~45              | ~48               | N/A                             |
| 2^15 (32768)                           | ~47              | ~50               | N/A                             |
| 2^16 (65536)                           | ~49              | ~52               | ~50.5                           |
| 2^17 (131072)                          | ~51              | ~53               | ~52.5                           |
| 2^18 (262144)                          | ~52              | ~54.8             | ~54.8                           |

Our code is available at https://github.com/hanyin88/DRG-Sapphire.

Preprint. Under review.

## 1 Introduction

Medical codes such as DRG play pivotal roles in modern healthcare. DRG codes are fundamental to the inpatient prospective payment system, directly influencing hospital reimbursement and key quality metrics [28]. Currently, assigning DRG codes from clinical notes remains a costly and labor-intensive task, performed manually by highly trained coding specialists.

With the emergence of LLMs, there has been growing interest in leveraging these models for auto- mated medical coding [9, 31, 34, 22, 40]. However, DRG coding remains a particularly challenging task for LLMs (Figure 1 A), with prior attempts yielding limited success [34, 31]. A primary difficulty arises because DRG coding represents an **out-of-distribution (OOD)** task for off-the-shelf LLMs. Due to the private nature of medical records, most LLMs likely have minimal exposure to patient notes or billing data during pretraining. Additionally, DRG coding is inherently challenging due to: (1) a high-dimensional search space with over 700 DRG codes; (2) advanced clinical reasoning required to link diagnoses with hospital resource use and disease severity; and (3) strict hierarchical rules governing DRG assignment.

Recent advances in reasoning models, such as OpenAI-01 [16] and DeepSeek-R1 [13], have intro- duced a paradigm shift in LLM post-training. By leveraging large-scale RL with verifiable rewards, these models exhibit test-time scaling through extended chain-of-thought (CoT) reasoning, achieving state-of-the-art (SOTA) performance on complex tasks like competitive mathematics. Despite this progress, the design of optimal RL algorithms for scalable training remain an open challenge [41, 25]. In the healthcare domain, RL applications using verifiable rewards are still in their early stages, with prior work primarily focused on medical knowledge benchmarks [5, 20, 21].

In this paper, we present a comprehensive exploration of large-scale, reasoning-oriented RL training for automated DRG coding from unstructured clinical notes. In theory, training towards a reasoning model is well-suited for this task: (1) it promotes the development of complex reasoning skills required for accurate code assignment; and (2) more importantly, it generates transparent rationales through CoT reasoning—a key requirement for trust and explainablity in real-world clinical applications.

Through this work, we aim to further derive insights into applying RL to challenging OOD tasks with off-the-shelf LLMs. Using Qwen2.5-7B model and GRPO with DRG-rule-based rewards, we systematically investigate the prerequisites for successful RL, the allocation of data between SFT and GRPO under a fixed data budget, and the impact of scaling SFT data. We also explore a series of RL algorithmic enhancements and adaptive learning strategies. Our core contributions are as follows:

1.  We introduce DRG-SAPPHIRE, a novel model developed through large-scale RL, achieving SOTA performance in automated DRG coding. Unlike prior methods, DRG-SAPPHIRE generates clinically helpful, physician-validated reasoning, significantly improving explainability.
2.  We demonstrate that the performance ceiling of RL in this OOD task is bounded by the model's capabilities before RL training. Specifically, we observe that RL performance increases linearly with the logarithm of the number of SFT examples, suggesting that scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.
3.  We propose a series of algorithmic enhancements and identify unique challenges in applying RL to DRG coding that distinguish it from mathematical domains—such as a preference for an Answer-First cognitive pattern, and sensitivity to KL divergence for stable training.

## 2 Related Work

**Automated DRG Coding** Given their critical role in hospital operations and reimbursement, there is significant interest in automating DRG coding and enabling early DRG prediction [24, 14, 34, 11]. The prior SOTA method, DRG-LLaMA, fine-tunes a LLaMA model as a sequence classifier by replacing its generation head with a classification head [34]. Most existing approaches similarly frame DRG coding as a multilabel classification task, offering limited insight into the rationale behind code assignments. While methods like DRGCoder provide input-level weight visualizations [14], their interpretability remains insufficient for real-world clinical deployment, where transparency and explainability are critical.

**Replication Efforts of Deepseek-R1** Recent studies have actively explored replicating the RL recipes of DeepSeek-R1, particularly in mathematical and coding domains, with varying degrees of success [43, 15, 36]. One line of work has proposed approaches to address biases and improve sample efficiency in the original GRPO algorithm [41, 25, 23]. Another active research area focuses on curriculum and staged learning strategies during reasoning-oriented RL [44, 32, 37, 17, 4].

## 3 Large-scale RL for Automated DRG Coding

### 3.1 Problem Formulation

We aim to automate the hierarchical assignment of Medicare Severity Diagnosis-Related Group (MS-DRG) codes using LLMs. The MS-DRG system classifies each hospitalization into a single DRG code based on clinical complexity and resource utilization (see Appendix A.1 for details). Given a hospitalization represented by a set of clinical documents D, the DRG coding process applies an extraction function h to identify the principal diagnosis w$_d$ or procedure w$_p$, and the presence of Complications or Comorbidities (CC) or Major Complications or Comorbidities (MCC). A hierarchical mapping function f then determines the final DRG code. Formally, the MS-DRG assignment is defined as:

(Wd, Wp, CC, MCC) = h(D), g = f(wa, wp, CC, MCC),

where g is the assigned DRG code. In this paper, we use an LLM to automate this complex process.

### 3.2 Preliminary: GRPO

Compared to Proximal Policy Optimization [29], GRPO eliminates the value function and estimates the advantage using relative rewards within a group [30]. For each question q, GRPO samples a group of outputs {o$_1$, o$_2$, ..., o$_G$} from the old policy π$_{θold}$ and then optimizes the target policy π$_θ$. In this paper, we enforce π$_{θold}$ = π$_θ$ to ensure strict on-policy learning. Under this setting, we maximizing the following objective:

JGRPO(θ) = E[q ~ P(Q), {o$_i$}$^G_{i=1}$ ~ π$_{θold}$(O|q)]

= (1/G) * Σ$^G_{i=1}$ Σ$^{|o_i|}_{t=1}$ [Â$_{i,t}$ - β * ( (π$_{ref}$(o$_{i,t}$|q, o$_{i,<t}$) / π$_θ$(o$_{i,t}$|q, o$_{i,<t}$) ) * log( (π$_{ref}$(o$_{i,t}$|q, o$_{i,<t}$) / π$_θ$(o$_{i,t}$|q, o$_{i,<t}$) ) - 1 )] (1)

where β is the coefficient for the KL divergence penalty, π$_{ref}$ is the reference policy, and Â$_{i,t}$ is the advantage, computed based on the relative rewards within each group {r$_i$}$^G_{i=1}$ as:

Â$_{i,t}$ = (r$_i$ - mean({r$_i$}$^G_{i=1}$)) / std({r$_i$}$^G_{i=1}$) (2)

Here, r$_i$ denotes the reward assigned to output o$_i$ for prompt q. The gradient of J$_{GRPO}$(θ) is:

∇$_{θ}$J$_{GRPO}$(θ) = E[q ~ P(Q), {o$_i$}$^G_{i=1}$ ~ π$_{θold}$(O|q)]
(1/G) * Σ$^G_{i=1}$ Σ$^{|o_i|}_{t=1}$ [Â$_{i,t}$ + β * ( (π$_{ref}$(o$_{i,t}$|o$_{i,<t}$) / π$_θ$(o$_{i,t}$|o$_{i,<t}$) ) - 1 ) * ∇$_θ$ log π$_θ$(o$_{i,t}$|q, o$_{i,<t}$)] (3)

### 3.3 Improving GRPO Beyond the Baseline

We propose a set of strategies to address key limitations of GRPO.

**Dynamic Resampling for Advantage Preservation** Existing RL algorithms suffer from the gradient-diminishing problem. In GRPO, if all completions {o$_i$}$_G$ for a prompt q receive the same reward value, the resulting advantage for this group becomes zero. As training progresses, this issue becomes more pronounced due to policy optimization and accompanying entropy collapse [41], as more prompts yield completions with no reward variance—either because all completions are perfectly correct or uniformly incorrect. This leads to a progressive decrease in the learning signal from the reward-based advantage.

To address this, we propose a dynamic resampling strategy (Equation 4). For each prompt q, if sampled completions yield zero reward variance, we resample up to N$_{max}$ times until nonzero variance is observed. Optionally, we enforce that at least one completion receives a positive reward, guiding gradient updates toward high-reward trajectories.

JGRPO(θ) = E[q ~ P(Q), {o$_i$}$^G_{i=1}$ ~ π$_{θold}$(O|q)]
= (1/G) * Σ$^G_{i=1}$ Σ$^{|o_i|}_{t=1}$ [Â$_{i,t}$ - β * ( (π$_{ref}$(o$_{i,t}$|q, o$_{i,<t}$) / π$_θ$(o$_{i,t}$|q, o$_{i,<t}$) ) * log( (π$_{ref}$(o$_{i,t}$|q, o$_{i,<t}$) / π$_θ$(o$_{i,t}$|q, o$_{i,<t}$) ) - 1 )] (4)
s.t. Var({r$_i$}$^G_{i=1}$) > 0 within N$_{max}$, optionally: |{o$_i$ | r$_i$ > 0}| > 0.

Our approach differs from the dynamic sampling strategy in DAPO [41], which discards prompts that yield uniformly correct or incorrect completions. Given the data scarcity in clinical domains, we instead maximize the utility of each training example by resampling rather than discarding.

**Intervening on Cognitive Behaviors** Cognitive behaviors, such as verification and backtracking, are critical for effective reasoning-oriented RL [12]. We explored additional reward functions and a specialized SFT dataset (detailed in Section A.3) to incentivize three cognitive patterns in CoT reasoning, as shown in Figure 2. These are: (1) Answer-First, where the model outputs the DRG code before CoT; (2) CoT-First, where the model generates CoT reasoning before the DRG code; and (3) Differential Thinking, where the model evaluates three potential DRG codes before selecting the most appropriate.

### Figure 2: Examples of Cognitive Behaviors.

**Answer-First**
<think>
Based on the provided discharge summary, the most likely Medicare Severity Diagnosis-Related Group (MS-DRG) is "OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC."
### Reasoning:
1.  **Principal Diagnosis**:
2.  **CC (Complication or Comorbidity)**: ...
3.  **MCC (Major Complication or Comorbidity)**: ...
4.  ** discharged with specific medications and follow-up plans**:
### Conclusion:
The DRG code "OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC" is assigned because the patient's primary condition, despite being initially concerning, did not meet the criteria for an MCC...
</think>
<answer>
OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC
</answer>

**CoT-First**
<think>
To determine the most likely MS-DRG for this patient, let's break down the key elements from the discharge summary:
1.  **Principal Diagnosis**: ...
2.  **Secondary Diagnoses and Procedures**....
3.  **Complications and Comorbidities (CC and MCC)**:...
**Reasoning for the DRG Assignment**:...
**DRG Code Assignment**:
The most likely MS-DRG for this patient, based on the provided information, would be ***OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC.*** This DRG code is assigned because...
</think>
<answer>
OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC
</answer>

**Differential Thinking**
<think>
#### Principal Diagnosis: ...
#### Secondary Diagnoses:
#### Procedures Performed:
#### Other Relevant Factors:...
### Possible DRG Codes
1.  **[Heart failure and shock without CC/MCC]**
2.  **[Other circulatory system diagnoses with CC]**
3.  **[Other circulatory system diagnoses with MCC]**
### Analysis for Each DRG Code:
### Conclusion
Most Likely DRG Code: [Other circulatory system diagnoses with CC], as it best reflects...
</think>
<answer>
OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC
</answer>

**KL Divergence Decay** The KL divergence term in the GRPO objective (Equation 1) regularizes the divergence between the target policy π$_θ$ and the reference policy π$_{θref}$. However, this term exacerbates the gradient-diminishing problem discussed in Section 3.3: as training progresses and more prompts yield zero-variance responses, the gradient, per Equation 3, becomes dominated by the KL term, pulling π$_θ$ toward π$_{θref}$. This drives over-regularization toward the reference policy and risks policy degradation. Recent work suggests that removing the KL penalty enhances reasoning capabilities in mathematical domains [41, 25, 15]. Motivated by this, we explored two setups: (1) completely removing the KL divergence term from the objective, and (2) applying a cosine decay schedule to the KL term's coefficient β, smoothly reducing it to zero during training (see Section A.4 for details).

**GRPO Variants** In Equation 1, dividing by $|o_i|$ during group-level advantage normalization introduces a length bias, diminishing the influence of longer completions on the policy gradient. To address this, DAPO [41] uses Σ$^{|o_i|}_{t=1}$ as the denominator, while Dr. GRPO [25] adopts a constant normalization factor. Additionally, Dr. GRPO removes the division by std({r$_i$}$^G_{i=1}$) in Equation 2 to mitigate question-level difficulty bias. We systematically evaluated these three strategies. Due to the strict on-policy nature of our setting (π$_{θold}$ = π$_θ$), we did not explore other modifications, such as clip-higher [41].

**Reward Shaping** We implemented two straightforward yet robust rule-based reward components: Format Reward and Accuracy Reward (detailed in the Section A.2). For the Accuracy Reward, we investigated three distinct strategies: Dense Reward, Balanced Reward, and Strict Reward. These reward functions were designed to provide varying levels of reward signal sparsity, contingent on the correctness of the DRG code, its associated principal diagnosis, and the CC/MCC status.

### 3.4 Adaptive Learning Strategy

**Curriculum Learning** We investigate whether a curriculum learning strategy, which organizes training cases by difficulty, improves performance compared to a mixed-difficulty baseline. We evaluated four setups, detailed in Appendix A.6: (1) excluding easy cases, (2) excluding hard cases, (3) excluding both easy and hard cases (i.e., using only medium-difficulty cases), and (4) training on easy cases first, then progressing to hard cases.

**Staged Learning** Lastly, we explored a staged learning strategy with three training phases of roughly equal length. After each phase, we identified easy and hard cases and evaluated two approaches: (1) additional SFT on hard cases, and (2) additional DPO on hard cases, before advancing to the next stage. As detailed in Appendix A.7, these approaches aim to improve the model's handling of challenging cases through targeted learning.

## 4 Implementation Details

**Dataset** We utilized the DRG-LLaMA training and test sets [34], derived from the publicly available MIMIC-IV dataset of real-world medical records [18]. The full training and test sets include 236,192 and 26,244 cases, respectively. Each case uses the “brief hospital course" section from the discharge summary as input, with MS-DRG codes consolidated to version 34.0.

**Training Pipeline and Scaling Strategy** An overview of the training pipeline is shown in Figure 3. We first sampled a reduced dataset termed DRG-Small, comprising 20% of the full data (N=46,758). This subset served as the foundation for extensive experiments on methodological variants and SFT-RL data mixtures, as detailed in Sections 5.2 through 5.3. After identifying the optimal configuration, we scaled training to the full dataset to produce the final DRG-SAPPHIRE model.

### Figure 3: Overview of Pipeline.
We construct a CoT cold-start dataset using Qwen2.5-7B-Instruct, followed by SFT with this dataset and large-scale GRPO.

*   **Step 1:** Clinical Notes and DRG Pairs from MIMIC-IV `+` Bootstrap CoT Reasoning for DRG Assignment Using Qwen2.5-7B -> Cold-Start Dataset
*   **Step 2:** SFT on the Qwen2.5-7B Using Cold-Start Data -> Cold-Start Model Checkpoint
*   **Step 3:** Large-Scale RL with GRPO and Verifiable Rewards -> DRG-Sapphire

**Construction of SFT Dataset** We prompted the Qwen2.5-7B-Instruct model with medical records and ground-truth DRG codes, tasking it to generate reasoning for DRG assignments (prompt provided in Section H). After extensive prompt engineering, manual inspection by domain expert revealed that the dataset exhibits correct reasoning logic (e.g., analyzing principal diagnosis first) but frequently contains factual errors (e.g., misclassifying a condition's CC/MCC status). We also included the complete list of original V34.0 MS-DRG codes in a question-answer format within the SFT dataset.

**Model and RL Training** We selected Qwen2.5-7B-Instructs [39] for the main experiments after evaluating various model size. GRPO training was conducted using the TRL package [33] for one epoch across all experiments.

**Evaluation Metrics** We report model performance on the full test set using Pass@1, Pass@8, and Majority@8 (Maj@8), following prior work in reasoning-oriented RL [30, 42]. Pass@1, reported as the model's accuracy, is the mean accuracy across eight runs. Pass@8 assesses whether the correct DRG code appears among eight generated outputs, while Maj@8 determines if the most frequent output matches the correct DRG code.

## 5 Experiments

### 5.1 Results of DRG-SAPPHIRE

Our best DRG-SAPPHIRE model was achieved using a 90% SFT and 10% RL ratio on the full dataset (see Section 5.2 for SFT vs. RL ratio experiments), incorporating optimal GRPO enhancements and adaptive learning strategies (see Section 5.3 for ablation studies).

**Comparison with Baselines** As shown in Figure 1 A, DRG-SAPPHIRE significantly outperforms proprietary reasoning models, non-reasoning models, and the DeepSeek-distilled Qwen 32B. It achieves new SOTA performance on DRG coding, surpassing the previous best, DRG-LLaMA-7B (54.8% vs. 53.9%). In addition to improved accuracy, DRG-SAPPHIRE provides interpretable reasoning—a compelling advantage over prior models trained purely as classifiers.

**Expert Reader Study Results** Four physicians in hospital leadership roles, actively engaged in DRG-related initiatives (e.g., reducing geometric mean length of stay), evaluated DRG-SAPPHIRE's reasoning across 30 cases. On the dimensions of Helpfulness and Accuracy, DRG-SAPPHIRE received a median rating of 4 out of 5, suggesting significant potential for real-world applications (Figure 4). Quantitative assessments highlighted the explainability of DRG coding as highly valuable for DRG-related initiatives (see Section D.1 for details), despite occasional factual inaccuracies in the reasoning.

### Figure 4: Expert Reader Study.

#### Review Score (1=Very Poor, 2=Poor, 3=Acceptable, 4=Good, 5=Very Good)

| Metric      | Median Score |
| :---------- | :----------- |
| Helpfulness | 4            |
| Accuracy    | 4            |

*(The box plot shows the distribution, with median at 4 for both Helpfulness and Accuracy. The boxes span 3-5 for Helpfulness and 3-5 for Accuracy, with no outliers visible from the plot.)*

### 5.2 Optimizing Data Allocation Between SFT and GRPO

### Figure 5: Impact of SFT-GRPO Data Ratios on DRG-Small Subset.
Panels A-E: GRPO consistently improves Pass@1 and Maj@1 across all SFT ratios but reduces Pass@8. Panel F: Training time decreases with higher SFT ratios, as GRPO is more time-consuming.

#### A. Deepseek-R1-Style (5% SFT, 95% RL) (Accuracy %)

| Metric  | SFT   | + GRPO | Change |
| :------ | :---- | :----- | :----- |
| Pass@1  | 25.0  | 38.7   | +54.6% |
| Pass@8  | 58.9  | 54.0   | -8.2%  |
| Maj@8   | 31.4  | 39.6   | +26.0% |

#### B. 25% SFT, 75% RL (Accuracy %)

| Metric  | SFT   | + GRPO | Change |
| :------ | :---- | :----- | :----- |
| Pass@1  | 32.9  | 40.1   | +21.9% |
| Pass@8  | 68.0  | 58.0   | -14.7% |
| Maj@8   | 38.7  | 44.4   | +14.7% |

#### C. 50% SFT, 50% RL (Accuracy %)

| Metric  | SFT   | + GRPO | Change |
| :------ | :---- | :----- | :----- |
| Pass@1  | 36.7  | 46.5   | +26.7% |
| Pass@8  | 70.9  | 57.4   | -19.0% |
| Maj@8   | 43.6  | 47.2   | +8.3%  |

#### D. 75% SFT, 25% RL (Accuracy %)

| Metric  | SFT   | + GRPO | Change |
| :------ | :---- | :----- | :----- |
| Pass@1  | 38.5  | 47.4   | +23.1% |
| Pass@8  | 72.1  | 59.6   | -17.4% |
| Maj@8   | 45.4  | 48.3   | +6.4%  |

#### E. 100% SFT, 0% RL (Accuracy %)

| Metric  | SFT   | + GRPO |
| :------ | :---- | :----- |
| Pass@1  | 40.0  | 46.7   |
| Pass@8  | 72.6  | 59.9   |
| Maj@8   | 46.0  | 48.7   |

#### F. Training Time and Accuracy by SFT % (Accuracy (%) & Training Time (hours))

| SFT Percentage | SFT Accuracy (%) | GRPO Accuracy (%) | Training Time (hours) |
| :------------- | :--------------- | :---------------- | :-------------------- |
| 5%             | ~25              | ~38.5             | ~47.4                 |
| 25%            | ~33              | ~40               | ~46.5                 |
| 50%            | ~37              | ~46.5             | ~43                   |
| 75%            | ~38.5            | ~47.5             | ~40                   |
| 100%           | ~40              | ~46.7             | ~30                   |

**Effect of SFT-GRPO Ratios on DRG-Small** First, we investigated the impact of varying the allocation of a fixed data budget between SFT and GRPO on the DRG-Small subset (N=46,758). This contrasts with Deepseek-R1-style training, where only minimal SFT precedes RL. Across all data splits, GRPO consistently and significantly improved Pass@1 over the SFT baseline by an absolute margin of approximately 10 percentage points (see Figure 5). We observed that this gain is driven by improvements in Maj@8, not Pass@8; in fact, Pass@8 declines with GRPO. This pattern suggests that RL sharpens the model's output distribution toward higher-reward pathways, rather than enhancing its underlying reasoning capabilities. Notably, the decline in Pass@8 during training indicates that RL may limit diverse reasoning pathways in our experiments. These findings align with recent studies [42, 30], which question whether RL improves reasoning beyond the base model's capabilities. Furthermore, the ultimate performance ceiling achievable with GRPO appears to be largely determined by the capacity of the initial SFT model; a stronger SFT foundation generally leads to better post-GRPO results. From a computational perspective, scaling SFT before RL is more efficient, as GRPO involves costly completion generation (see Figure 5 F).

### Figure 6: Results on Full Dataset.
A: Accuracy from the two longest GRPO runs. B-C: Pass@8 and Maj@8 vs. SFT size. Dashed line marks where 50% of training data was used for SFT. Best results from vanilla GRPO runs are shown.

#### A. Accuracy vs. Global Steps (Accuracy %)

| Global Steps | Full Data, 50% SFT | Full Data, 75% SFT |
| :----------- | :----------------- | :----------------- |
| 0            | ~44                | ~46                |
| 250          | ~50                | ~52                |
| 500          | ~52.5              | ~54                |
| 750          | ~53                | ~54.5              |
| 1000         | ~53.5              | ~54.7              |
| 1250         | ~53.7              | ~54.8              |
| 1500         | ~53.8              | ~54.8              |

#### B. Pass@8 vs SFT Sample Size (Pass@8 (%))

| SFT Sample Size before RL (log2 scale) | SFT    | GRPO   | Full Data, 50% SFT |
| :------------------------------------- | :----- | :----- | :----------------- |
| 2^11 (2048)                            | ~60    | ~55    | N/A                |
| 2^12 (4096)                            | ~65    | ~60    | N/A                |
| 2^13 (8192)                            | ~67.5  | ~62    | N/A                |
| 2^14 (16384)                           | ~69    | ~63    | N/A                |
| 2^15 (32768)                           | ~70    | ~64    | N/A                |
| 2^16 (65536)                           | ~71    | ~64.5  | ~70                |
| 2^17 (131072)                          | ~72    | ~65    | ~71                |
| 2^18 (262144)                          | ~73    | ~65.5  | ~72                |

#### C. Maj@8 vs SFT Sample Size (Maj@8 (%))

| SFT Sample Size before RL (log2 scale) | SFT    | GRPO   | Full Data, 50% SFT |
| :------------------------------------- | :----- | :----- | :----------------- |
| 2^11 (2048)                            | ~35    | ~40    | N/A                |
| 2^12 (4096)                            | ~38    | ~42    | N/A                |
| 2^13 (8192)                            | ~42    | ~44    | N/A                |
| 2^14 (16384)                           | ~44    | ~46    | N/A                |
| 2^15 (32768)                           | ~46    | ~48    | N/A                |
| 2^16 (65536)                           | ~48    | ~50    | ~49                |
| 2^17 (131072)                          | ~49.5  | ~51.5  | ~50.5              |
| 2^18 (262144)                          | ~50.5  | ~53.5  | ~52                |

**Log-Linear Scaling of GRPO with Increasing SFT** Next, we scaled our training pipeline to the full dataset (N=236,192). Based on the results above, we started with an SFT-GRPO data ratio of 50%-50% and progressively increased the SFT ratio under a fixed data budget. Plotting these results alongside the DRG-Small subset revealed that both GRPO and SFT performance scale approximately linearly with the logarithm of the number of SFT examples (Figure 1 A). Although the number of GRPO steps varies in Figure 1 A, the benefit of scaling RL appeared limited in our experiments. Figure 6 A illustrates results from our longest GRPO runs, demonstrating modest benefits beyond 500 global steps. Consistent with earlier findings, GRPO reliably improves Pass@1 and Maj@8 while reducing Pass@8 (Figure 6 B and C). As the number of SFT samples increased, the slope of the GRPO curves converged toward that of SFT across all metrics. Additional results from scaling to the full dataset are detailed in Section C.3.

### 5.3 Ablation Studies on GRPO Enhancements and Adapative Learning

We present the results of ablation studies in Table 1 and Figure 12. All ablation studies were conducted on the DRG-Small dataset using Deepseek-R1-style training, with cold-start SFT on 1% of the training data (N=2,362) before RL.

### Figure 7: Dynamic Resampling.
Despite maintaining a high reward variance during training (A), dynamic resampling performs worse than vanilla GRPO from training curves (B) and is significantly more time-consuming to train (C).

#### A. Reward Std (Reward Std)

| Global Steps | Positive Resampling | Neutral Resampling | Vanilla GRPO |
| :----------- | :------------------ | :----------------- | :----------- |
| 0            | ~0.8                | ~0.8               | ~0.6         |
| 100          | ~0.9                | ~0.9               | ~0.7         |
| 200          | ~0.9                | ~0.9               | ~0.7         |
| 300          | ~0.9                | ~0.9               | ~0.7         |
| 400          | ~0.9                | ~0.9               | ~0.7         |
| 500          | ~0.9                | ~0.9               | ~0.7         |
| 600          | ~0.9                | ~0.9               | ~0.7         |
| 700          | ~0.9                | ~0.9               | ~0.7         |

#### B. Training Curve (Reward Score)

| Global Steps | Positive Resampling | Neutral Resampling | Vanilla GRPO |
| :----------- | :------------------ | :----------------- | :----------- |
| 0            | ~0.4                | ~0.4               | ~0.45        |
| 100          | ~0.4                | ~0.4               | ~0.45        |
| 200          | ~0.45               | ~0.45              | ~0.5         |
| 300          | ~0.45               | ~0.45              | ~0.5         |
| 400          | ~0.45               | ~0.45              | ~0.5         |
| 500          | ~0.45               | ~0.45              | ~0.5         |
| 600          | ~0.45               | ~0.45              | ~0.5         |
| 700          | ~0.45               | ~0.45              | ~0.5         |

#### C. Training Time (Training Time (hours))

| Global Steps | Positive Resampling | Neutral Resampling | Vanilla GRPO |
| :----------- | :------------------ | :----------------- | :----------- |
| 0            | 0                   | 0                  | 0            |
| 100          | ~70                 | ~70                | ~25          |
| 200          | ~140                | ~140               | ~50          |
| 300          | ~205                | ~205               | ~75          |
| 400          | ~270                | ~270               | ~100         |
| 500          | ~335                | ~335               | ~125         |
| 600          | ~400                | ~400               | ~150         |
| 700          | ~406                | ~406               | ~175         |

**Dynamic Resampling** Surprisingly, dynamic resampling—with or without a positive reward con- straint-yielded marginally better or even worse performance than vanilla GRPO, despite preserving high reward variance (Figure 7 A). Moreover, dynamic resampling proved computationally inefficient due to the frequent need to regenerate responses (Figure 7 C). We hypothesize that dynamic resam- pling introduces sampling bias by oversampling prompts with zero reward variance, which may skew the batch toward out-of-distribution responses rarely produced by the current policy. Additionally, this approach may inadvertently over-penalize low-reward outputs newly introduced into the batch, further distorting the learning signal.

**Intervening on Cognitive Behaviors** Our SFT dataset includes diverse reasoning styles, notably both Answer-First and CoT-First patterns. Interestingly, during training, the policy frequently converged toward the Answer-First strategy. To encourage CoT-First behavior, we experimented with an additional rule-based reward and adjusted the SFT dataset to explicitly promote Differential- Thinking. While both interventions successfully induced the desired cognitive behaviors, they significantly underperformed compared to the naturally converging Answer-First pattern. This finding is surprising, as CoT-First strategies are often effective in complex reasoning tasks [35]. We hypothesize that DRG coding benefits from a direct prediction strategy, where outputting the DRG code first leverages implicit knowledge in the model's latent space, outperforming explicit CoT-grounded reasoning. These findings also align with recent studies [26, 6], which suggest that CoT and extended reasoning may not always be necessary for reasoning models, and a “no-thinking" pattern can sometimes yield better performance.

**KL Divergence** In our experiments, remov- ing the KL penalty frequently led to model collapse (see Figure 8 A). This contrasts sharply with findings in mathematical reasoning tasks, where the KL term is less critical, underscoring its importance for cross-domain generalization. However, in cases where training successfully completed without the KL penalty, performance surpassed that of vanilla GRPO (see Table 1), consistent with the analysis in Section 3.3. Ad- ditionally, a cosine KL decay schedule appeared beneficial. While it yielded no significant performance gains in small-scale runs, it improved the training curve toward the end, suggesting that a low KL penalty toward the end helps prevent over- regularization toward the reference policy (Figure 8 B). Indeed, KL decay proved beneficial when scaling training on the full dataset, as shown in Table 2.

### Figure 8: KL divergence.
A. Examples of training collapse when removing the KL divergence. B. KL decay appears beneficial late in training.

#### A. Training Collapse w/o KL (Reward Score)

| Global Steps | 1.5B with KL | 1.5B no KL | 7B with KL | 7B no KL |
| :----------- | :----------- | :--------- | :--------- | :------- |
| 0            | ~0.0         | ~0.0       | ~0.0       | ~0.0     |
| 100          | ~0.5         | ~0.0       | ~0.5       | ~0.0     |
| 200          | ~0.5         | ~0.0       | ~0.5       | ~0.0     |
| 300          | ~0.5         | ~0.0       | ~0.5       | ~0.0     |
| 400          | ~0.5         | ~-0.5      | ~0.5       | ~-1.0    |
| 500          | ~0.5         | ~-1.0      | ~0.5       | ~-1.5    |

#### B. Training Curve for Final Steps (Reward Score)

| Global Steps | Vanilla | KL Decay |
| :----------- | :------ | :------- |
| 650          | ~0.90   | ~0.90    |
| 670          | ~0.90   | ~0.92    |
| 690          | ~0.90   | ~0.95    |
| 710          | ~0.90   | ~0.97    |
| 730          | ~0.90   | ~0.98    |

### Table 1: Ablation Study Results.
Rows with a blue background indicate superior Pass@1 perfor- mance compared to Vanilla GRPO + Dense Reward. Bold values denote the highest score for each metric.

| Model                                    | DRG Pass@1 | DRG Pass@8 | DRG Maj@8 | Principal Diagnosis Pass@1 | Principal Diagnosis Pass@8 | Principal Diagnosis Maj@8 | CC/MCC Pass@1 | CC/MCC Pass@8 | CC/MCC Maj@8 |
| :--------------------------------------- | :--------- | :--------- | :-------- | :------------------------- | :------------------------- | :------------------------ | :------------ | :------------ | :----------- |
| **Baseline**                             |            |            |           |                            |                            |                           |               |               |              |
| Vanilla GRPO + Dense Reward              | 38.5       | 48.2       | 39.3      | 52.5                       | 58.5                       | 53.4                      | 47.8          | 60.0          | 49.0         |
| **Dynamic Resampling**                   |            |            |           |                            |                            |                           |               |               |              |
| Neutral Resampling                       | 20.3       | 41.9       | 38.1      | 27.0                       | 52.5                       | 50.5                      | 25.6          | 52.6          | 48.0         |
| Positive Reward Resampling               | 39.2       | 44.8       | 39.6      | 52.9                       | 56.4                       | 53.3                      | 48.3          | 55.6          | 49.0         |
| **Cognitive Behaviors Intervention**     |            |            |           |                            |                            |                           |               |               |              |
| CoT-First                                | 35.5       | 52.2       | 37.4      | 50.9                       | 59.6                       | 52.4                      | 46.3          | 66.7          | 48.4         |
| Differential Thinking                    | 30.2       | 47.3       | 33.9      | 46.7                       | 57.0                       | 50.9                      | 40.6          | 63.0          | 45.2         |
| **GRPO Variants**                        |            |            |           |                            |                            |                           |               |               |              |
| DAPO Loss                                | **40.1**   | 48.0       | **40.6**  | **53.8**                   | 58.5                       | **54.3**                  | **49.4**      | 59.1          | **50.3**     |
| Dr. GRPO Loss                            | 37.5       | 47.6       | 38.1      | 50.9                       | 57.2                       | 51.4                      | 48.8          | 60.7          | 49.8         |
| Dr. GRPO Advantage                       | 38.5       | **51.9**   | 39.6      | 53.4                       | **60.5**                   | 54.3                      | 47.6          | **63.6**      | 49.1         |
| **KL Divergence**                        |            |            |           |                            |                            |                           |               |               |              |
| No KL                                    | 39.8       | 42.4       | 39.9      | 53.6                       | 55.2                       | 53.7                      | 49.1          | 52.3          | 49.3         |
| Kl Decay                                 | 38.2       | 42.0       | 38.3      | 52.2                       | 54.7                       | 52.4                      | 48.8          | 53.7          | 49.0         |
| **Reward Shaping**                       |            |            |           |                            |                            |                           |               |               |              |
| Strict Reward                            | **40.1**   | 49.1       | **40.9**  | 52.8                       | 58.1                       | 53.7                      | 47.6          | 59.0          | 48.8         |
| Balanced Reward                          | 38.1       | **51.3**   | 40.0      | 52.1                       | **60.4**                   | **53.8**                  | **48.2**      | **64.0**      | **50.7**     |
| **Curriculum Learning**                  |            |            |           |                            |                            |                           |               |               |              |
| Remove Easy Cases                        | 35.8       | **51.9**   | 37.6      | 50.3                       | **59.2**                   | 51.7                      | 46.6          | **65.8**      | 48.7         |
| Remove Hard Cases                        | **40.4**   | 46.6       | **40.7**  | **53.2**                   | 56.5                       | **53.7**                  | **49.5**      | 57.2          | **50.1**     |
| Remove Easy and Hard Cases               | 38.7       | 48.2       | 39.4      | 52.9                       | 58.1                       | 53.4                      | 48.3          | 59.9          | 49.3         |
| From Easy to Hard                        | 29.4       | **51.7**   | 32.7      | 43.4                       | **58.6**                   | 46.5                      | 40.8          | **68.5**      | 44.3         |
| **Staged Learning**                      |            |            |           |                            |                            |                           |               |               |              |
| Staged SFT                               | 39.3       | 49.1       | 40.0      | 52.9                       | **59.2**                   | **53.8**                  | 46.0          | 58.6          | 47.1         |
| Staged DPO                               | 29.3       | 46.1       | 31.2      | 43.8                       | 54.3                       | 45.5                      | 43.1          | 64.2          | 45.7         |

**GRPO Variants** Among three GRPO vari- ants, the DAPO loss achieved the highest per- formance, while the Dr. GRPO loss performed the lowest (Figure 9). This finding aligns with recent work reporting that Dr. GRPO does not outperform vanilla GRPO [7]. Across all set- tings, we observed completion length contrac- tion during training: as accuracy improved, out- put lengths sharply decreased before stabilizing. This contrasts with trends observed in mathemat- ical reasoning tasks, where longer completions are often associated with better performance.

### Figure 9: GRPO Variants.
A. Dr. GRPO loss underperforms other GRPO variants from training curve. B. All GRPO variants exhibit similar com- pletion length contraction.

#### A. Training Curve (Reward Score)

| Global Steps | DAPO Loss | Dr. GRPO Loss | Dr. GRPO Advantage | Vanilla GRPO |
| :----------- | :-------- | :------------ | :----------------- | :----------- |
| 0            | ~0.0      | ~0.0          | ~0.0               | ~0.0         |
| 100          | ~0.4      | ~0.4          | ~0.3               | ~0.4         |
| 200          | ~0.5      | ~0.45         | ~0.4               | ~0.45        |
| 300          | ~0.6      | ~0.5          | ~0.45              | ~0.5         |
| 400          | ~0.7      | ~0.55         | ~0.5               | ~0.55        |
| 500          | ~0.75     | ~0.6          | ~0.55              | ~0.6         |
| 600          | ~0.77     | ~0.65         | ~0.6               | ~0.65        |
| 700          | ~0.78     | ~0.67         | ~0.62              | ~0.68        |

#### B. Completion Length (Completion Length)

| Global Steps | DAPO Loss | Dr. GRPO Loss | Dr. GRPO Advantage | Vanilla GRPO |
| :----------- | :-------- | :------------ | :----------------- | :----------- |
| 0            | ~575      | ~575          | ~575               | ~575         |
| 100          | ~550      | ~550          | ~550               | ~550         |
| 200          | ~525      | ~525          | ~525               | ~525         |
| 300          | ~500      | ~500          | ~500               | ~500         |
| 400          | ~500      | ~500          | ~500               | ~500         |
| 500          | ~500      | ~500          | ~500               | ~500         |
| 600          | ~500      | ~500          | ~500               | ~500         |
| 700          | ~500      | ~500          | ~500               | ~500         |

**Reward Shaping** The strict accuracy reward, despite providing the sparsest reward signals, outper- formed both dense and balanced reward variants. Notably, we observed no improvement in pincipal diagnosis or CC/MCC accuracy under the denser reward schemes. We hypothesize that denser rewards may lead the policy to converge prematurely to local optima, trading off global performance for easier-to-optimize intermediate signals.

**Adaptive Learning** We observed benefits from removing easy and hard cases during training. Similarly, recent studies suggest that maintaining medium-level difficulty cases may be most effective for RL training in the math domain [32, 37, 17, 38]. Staged learning with SFT resulted in modest performance gains despite additional compute.

### 5.4 Prerequisites for Effective GRPO Training

We explored prerequisites for effective GRPO training, finding that vanilla Qwen2.5 models (base and instruct) failed to produce correct DRG codes with GRPO alone, despite quickly adopting the target reasoning format (Figure 10 A). Post-SFT, all models showed improved RL performance that generally scaled with model size, though gains from 7B to 14B were modest (Figure 10 B). Higher SFT learning rates (up to 4 × 10$^{-5}$) and extended training epochs further enhanced GRPO performance, with diminishing returns at higher learning rates (Figure 10 C). These results align with recent findings [27] emphasizing the importance of aggressive SFT for reasoning-intensive tasks.

### Figure 10: Prerequisites for GRPO Training.
A. Vanilla models fail to explore. B. GRPO perfor- mance increases with model size post-SFT. C. Higher SFT learning rates boost GRPO performance.

#### A. RL from Out-of-Shelf Models (Reward Score)

| Global Steps | Vanilla Qwen2.5-7B-Base | Vanilla Qwen2.5-7B-Instruct |
| :----------- | :---------------------- | :-------------------------- |
| 0            | ~0.5                    | ~0.5                        |
| 100          | ~-0.5                   | ~-0.5                       |
| 200          | ~-1.0                   | ~-1.0                       |
| 300          | ~-1.5                   | ~-1.5                       |
| 400          | ~-2.0                   | ~-2.0                       |
| 500          | ~-2.0                   | ~-2.0                       |
| 600          | ~-2.0                   | ~-2.0                       |
| 700          | ~-2.0                   | ~-2.0                       |

#### B. RL from SFT Models (by Size) (Reward Score)

| Global Steps | Qwen2.5-14B-Instruct | Qwen2.5-7B-Instruct | Qwen2.5-1.5B-Instruct |
| :----------- | :------------------- | :------------------ | :-------------------- |
| 0            | ~0.5                 | ~0.5                | ~0.5                  |
| 100          | ~0.6                 | ~0.5                | ~0.4                  |
| 200          | ~0.7                 | ~0.6                | ~0.5                  |
| 300          | ~0.8                 | ~0.7                | ~0.6                  |
| 400          | ~0.85                | ~0.75               | ~0.65                 |
| 500          | ~0.9                 | ~0.8                | ~0.7                  |
| 600          | ~0.92                | ~0.82               | ~0.72                 |
| 700          | ~0.93                | ~0.83               | ~0.73                 |

#### C. RL from SFT Models (by LR/Epochs) (Reward Score)

| Global Steps | SFT 3e-6 for 3 epochs | SFT 3e-6 for 9 epochs | SFT 4e-5 for 3 epochs | SFT 4e-5 for 9 epochs |
| :----------- | :-------------------- | :-------------------- | :-------------------- | :-------------------- |
| 0            | ~0.3                  | ~0.3                  | ~0.3                  | ~0.3                  |
| 100          | ~0.5                  | ~0.6                  | ~0.6                  | ~0.7                  |
| 200          | ~0.6                  | ~0.7                  | ~0.7                  | ~0.8                  |
| 300          | ~0.7                  | ~0.75                 | ~0.8                  | ~0.85                 |
| 400          | ~0.75                 | ~0.8                  | ~0.85                 | ~0.9                  |
| 500          | ~0.8                  | ~0.85                 | ~0.9                  | ~0.92                 |
| 600          | ~0.82                 | ~0.87                 | ~0.92                 | ~0.93                 |
| 700          | ~0.83                 | ~0.88                 | ~0.93                 | ~0.94                 |

## 6 Conclusion

In this work, we used DRG coding as an empirical study to explore RL for OOD reasoning in LLMs. Our approach, applying GRPO with verifiable rewards, achieved a new SOTA performance while offering a key advantage over prior methods: the generation of physician-validated explanations through CoT reasoning. Critically, our findings reveal that RL performance on this OOD task is fundamentally constrained by the base model's capacity prior to RL. We observed a logarithmic scaling relationship between the number of SFT examples and subsequent RL performance. Despite extensive experimentation with RL algorithmic enhancements and adaptive learning strategies, these refinements yielded only modest improvements compared to simply initializing RL from stronger SFT baselines—highlighting a “bitter lesson” in applying RL to tasks that fall outside the pretraining distribution of LLMs.

## References

[1] Drg-llama: Tuning llama model to predict diagnosis-related group for hospitalized patients.
[2] MIMIC-IV on physionet.
[3] Responsible use of mimic data with online services like gpt.
[4] S. Bae, J. Hong, M. Y. Lee, H. Kim, J. Nam, and D. Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025.
[5] J. Chen, Z. Cai, K. Ji, X. Wang, W. Liu, R. Wang, J. Hou, and B. Wang. Huatuogpt-01, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024.
[6] Y. Chen, J. Benton, A. Radhakrishnan, J. U. C. Denison, J. Schulman, A. Somani, P. Hase, M. W. F. R. V. Mikulik, S. Bowman, J. L. J. Kaplan, et al. Reasoning models don't always say what they think.
[7] X. Chu, H. Huang, X. Zhang, F. Wei, and Y. Wang. Gpg: A simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025.
[8] CMS. Icd-10-cm/pcs ms-drg v34. 0 definitions manual. https://www.cms.gov/icd10m/ version34-fullcode-cms/fullcode_cms/P0001.html., 2016.
[9] H. Dong, M. Falis, W. Whiteley, B. Alex, J. Matterson, S. Ji, J. Chen, and H. Wu. Automated clinical coding: what, why, and where we are? NPJ digital medicine, 5(1):159, 2022.
[10] H. Face. Open r1: A fully open reproduction of deepseek-r1, January 2025.
[11] Y. Feng. Can large language models replace coding specialists? evaluating gpt performance in medical coding tasks. 2025.
[12] K. Gandhi, A. Chakravarthy, A. Singh, N. Lile, and N. D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025.
[13] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
[14] D. Hajialigol, D. Kaknes, T. Barbour, D. Yao, C. North, J. Sun, D. Liem, and X. Wang. Drgcoder: Explainable clinical coding for the early prediction of diagnostic-related groups. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 373-380, 2023.
[15] J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025.
[16] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai 01 system card. arXiv preprint arXiv:2412.16720, 2024.
[17] Y. Ji, S. Zhao, X. Tian, H. Wang, S. Chen, Y. Peng, H. Zhao, and X. Li. How difficulty- aware staged reinforcement learning enhances llms’ reasoning capabilities: A preliminary experimental study. arXiv preprint arXiv:2504.00829, 2025.
[18] A. E. Johnson, L. Bulgarelli, L. Shen, A. Gayles, A. Shammout, S. Horng, T. J. Pollard, S. Hao, B. Moody, B. Gow, et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023.
[19] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023.
[20] Y. Lai, J. Zhong, M. Li, S. Zhao, and X. Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025.
[21] W. Lan, W. Wang, C. Ji, G. Yang, Y. Zhang, X. Liu, S. Wu, and G. Wang. Clinicalgpt-r1: Pushing reasoning capability of generalist disease diagnosis with large language model. arXiv preprint arXiv:2504.09421, 2025.
[22] R. Li, X. Wang, and H. Yu. Exploring llm multi-agents for icd coding. arXiv preprint arXiv:2406.15363, 2024.
[23] Z. Lin, M. Lin, Y. Xie, and R. Ji. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342, 2025.
[24] J. Liu, D. Capurro, A. Nguyen, and K. Verspoor. Early prediction of diagnostic-related groups and estimation of hospital cost by processing clinical notes. NPJ digital medicine, 4(1):103, 2021.
[25] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: A critical perspective, 2025. URL https://arxiv. org/abs/2503.20783.
[26] W. Ma, J. He, C. Snell, T. Griggs, S. Min, and M. Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025.
[27] G. Penedo, L. Tunstall, A. Lozhkov, H. Kydlicek, E. Beeching, L. B. Allal, Q. Gallouédec, L. von Werra, A. P. Lajarín, and N. Habib. Open r1 update 3: Steady progress and a new technical report, 2024. Hugging Face Blog.
[28] K. Quinn. After the revolution: Drgs at age 30. Annals of internal medicine, 160(6):426-429, 2014.
[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[30] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
[31] A. Soroush, B. S. Glicksberg, E. Zimlichman, Y. Barash, R. Freeman, A. W. Charney, G. N. Nadkarni, and E. Klang. Large language models are poor medical coders—benchmarking of medical code querying. NEJM AI, 1(5):AIdbp2300040, 2024.
[32] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025.
[33] L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, S. Huang, K. Ra- sul, and Q. Gallouédec. Trl: Transformer reinforcement learning. https://github.com/ huggingface/trl, 2020.
[34] H. Wang, C. Gao, C. Dantona, B. Hull, and J. Sun. Drg-llama: tuning llama model to predict diagnosis-related group for hospitalized patients. npj Digital Medicine, 7(1):16, 2024.
[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
[36] T. Xie, Z. Gao, Q. Ren, H. Luo, Y. Hong, B. Dai, J. Zhou, K. Qiu, Z. Wu, and C. Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025.
[37] W. Xiong, J. Yao, Y. Xu, B. Pang, L. Wang, D. Sahoo, J. Li, N. Jiang, T. Zhang, C. Xiong, et al. A minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025.
[38] J. Yan, Y. Li, Z. Hu, Z. Wang, G. Cui, X. Qu, Y. Cheng, and Y. Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025.
[39] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.
[40] Z. Yang, S. S. Batra, J. Stremmel, and E. Halperin. Surpassing gpt-4 medical coding with a two-stage approach. arXiv preprint arXiv:2311.13735, 2023.
[41] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025.
[42] Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, Y. Yue, S. Song, and G. Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025.
[43] W. Zeng, Y. Huang, Q. Liu, W. Liu, K. He, Z. Ma, and J. He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025.
[44] X. Zhang, J. Wang, Z. Cheng, W. Zhuang, Z. Lin, M. Zhang, S. Wang, Y. Cui, C. Wang, J. Peng, et al. Srpo: A cross-domain implementation of large-scale reinforcement learning on llm. arXiv preprint arXiv:2504.14286, 2025.

## Appendix

### Contents

*   A Additional Methods
    *   A.1 Problem Definition of MS-DRG Coding
    *   A.2 Rule-Based Reward Modeling
    *   A.3 Enforcing Cognitive Behaviors
    *   A.4 KL Divergence Decay
    *   A.5 GRPO Variants
    *   A.6 Curriculum Learning
    *   A.7 Staged Learning
*   B Additional Implementation Details
    *   B.1 SFT Training Details
    *   B.2 GRPO Training Details
    *   B.3 Experimental Hyperparameters
    *   B.4 Evaluation Details
    *   B.5 Dynamic Resampling Details
*   C Additional Results
    *   C.1 Experiments with GRPO Hyperparameters
    *   C.2 Accuracy with RL Training in Ablation Studies
    *   C.3 Additional Results from Scaling to the Full Dataset
*   D Additional Discussion
    *   D.1 Clinical Applications of Automated DRG Coding with Reasoning
    *   D.2 Practical Implication of Improved Pass@1 but Not Pass@k.
    *   D.3 DRG vs ICD Coding.
*   E Limitations and Future Work
*   F Data Access
*   G Instruction to Reviewers
*   H Prompts to LLM
*   I Example Outputs from DRG-Sapphire
*   J Example Outputs Demonstrating Different Cognitive Behaviors

### A Addtional Methods

#### A.1 Problem Definition of MS-DRG Coding

Under the Medicare Severity DRG (MS-DRG) system, each hospitalization is assigned a single DRG code based on clinical complexity and resource utilization, following rules established by the Centers for Medicare & Medicaid Services (CMS) [8]. Given a hospital stay D = {d$_1$, d$_2$, ..., d$_n$}, where each d$_i$ represents a clinical document generated during the hospitalization, the DRG assignment process performed by human coders can be mathematically represented as follows:

1.  **Extraction of Diagnoses and Procedures.** From D, extract a set W = {w$_1$, w$_2$, ..., w$_m$}, where each w$_i \in$ W corresponds to a distinct medical diagnosis or procedure managed during the stay.
2.  **Identification of Principal Diagnosis or Procedure.** Select a principal diagnosis w$_a \in$ W (for medical DRGs) or a principal procedure w$_p \in$ W (for surgical DRGs), representing the main reason for admission or the primary surgical intervention. Only one—diagnosis or procedure—is designated as principal depending on the case type.
3.  **Detection of Complications and Comorbidities.** Identify the presence of Complications or Comorbidities (CC) and Major Complications or Comorbidities (MCC) within W, forming subsets:

    CC $\subset$ W, MCC $\subset$ W, CC $\cap$ MCC = $\emptyset$,

    which reflect distinct levels of clinical severity and resource impact.
4.  **Hierarchical Mapping to DRG.** The final MS-DRG code g is determined via:

    (W$_d$, W$_p$, CC, MCC) = h(D), g = f(w$_d$, w$_p$, CC, MCC),

    where h extracts the principal diagnosis or procedure and CC/MCC from D, and f represents the CMS-defined DRG mapping logic.

#### A.2 Rule-Based Reward Modeling

We adopted the following two simple yet rigorous rule-based reward components.

**Format Reward.** The Format Reward enforces a structured response, requiring reasoning con- tent to be enclosed within `<think></think>` tags and the final answer (DRG code) within `<answer></answer>` tags. The reward is defined as:

S$_{format}$ =
[0, if the response format is correct
[-2, otherwise

**Accuracy Reward.** The Accuracy Reward evaluates the correctness of the DRG code, and applied only if the Format Reward condition is satisfied. We explored three reward shaping strategies:

**(a) Dense Reward**
S$_{dense}$ =
[2, if full match
[1.5, if principal diagnosis match only
[0.5, if CC/MCC match only
[-1.5, if invalid DRG

**(a) Balanced Reward**
S$_{balanced}$ =
[2, if full match
[1, if principal diagnosis match only
[1, if CC/MCC match only
[-1.5, if invalid DRG

**(c) Strict Reward**
S$_{strict}$ =
[2, if full match
[0, if partial or no match but valid DRG
[-1.5, if invalid DRG

#### A.3 Enforcing Cognitive Behaviors

To incentivize CoT-first cognitive behaviors, we introduced an additional format penalty. If the model outputs a DRG code within the first 50 tokens of the reasoning, a penalty score of -0.5 is assigned.

To encourage differential thinking, we reconstructed the SFT dataset using the same data. We designed a new prompt for the Qwen2.5-7B-Instruct model to generate three potential DRG codes per case (prompt provided in Section H), each accompanied by reasoning, before selecting the most appropriate DRG code.

#### A.4 KL Divergence Decay

To gradually relax the regularization imposed by the KL penalty, we apply a cosine decay to the KL coefficient β, reducing it from its initial value to zero over the course of training. For global step *t* and total training steps *T*, the decay factor is defined as:

decay\_factor(t) = 0.5 * (1 + cos( (π * t) / T ) )

The decayed coefficient at step *t* is then:

β$_{t}$ = β * decay\_factor(t)

This decay schedule promotes stability in the early stages of training while encouraging exploration in later updates.

#### A.5 GRPO Variants

We implemented the loss functions for different GRPO variants as follows:

```
#Vanilla GRPO loss. This is the original implementation in TRL v0.15.1.
vanila_GRPO_loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.
sum(dim=1)).mean()

#DAPO loss. This is the default implementation in TRL since v0.16.0.
DAPO_GRPO_loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()

#Dr.GRPO loss. We set max_tokens to 1024.
max_tokens = 1024
Dr_GRPO_loss = ((per_token_loss * completion_mask).sum(dim=1) / max_tokens).mean()
```

For the Dr. GRPO advantage, we modified the advantage computation by removing the denominator in Equation 2.

#### A.6 Curriculum Learning

Unlike mathematical problems, the difficulty of DRG coding is not easily defined. High prediction accuracy does not necessarily imply that a DRG code is inherently easy. For instance, the frequently occurring code "Septicemia or Severe Sepsis without MV >96 Hours with MCC” may be straight- forward in most cases but can pose challenges when the clinical narrative emphasizes a different condition, such as a urinary tract infection. Moreover, no standardized benchmark exists to quantify DRG coding difficulty.

To address this, we employed a static online filtering strategy. For each experiment, we first ran the model without filtering to establish a baseline. Easy cases were identified as those with zero reward variance and perfect accuracy scores (2) under both dense and strict rewards. Hard cases were defined as those with zero reward variance, an accuracy score of -0.5 under the dense reward, and 0 under the strict reward. We then reran the experiment from the SFT model after excluding these filtered cases.

#### A.7 Staged Learning

For staged learning, we divided the training process into three stages, each with approximately the same number of global steps. After each stage, we identified hard and easy cases using the methodology described in Section A.6. For hard cases, we prompted the current GRPO model checkpoint to generate reasoning given the case and the correct DRG code. We then performed SFT or DPO on this new dataset. For SFT, we used a learning rate of 4 × 10$^{-5}$ for 3 epochs. For DPO, we designated the original model output as the rejected response, and trained with a learning rate of 3 × 10$^{-6}$ for 3 epochs.

### B Additional Implementation Details

#### B.1 SFT Training Details

We used the SFT trainer from the TRL library for all SFT runs [33]. Training was conducted on 4 H100 or A100 GPUs, depending on availability, using bf16 precision. We set `packing=False` and `max_seq_length` to 12846. A cosine learning rate schedule with a minimum of 10% of the initial rate was applied, along with a warm-up ratio of 0.05. The global batch size was adjusted based on VRAM constraints to roughly match the number of unique cases per step used in GRPO training.

#### B.2 GRPO Training Details

Our implementation of GRPO was based on the Open R1 framework [10], which leverages vLLM [19] for inference and GRPO Trainer from TRL library (v0.15.1) [33] for training. All training was conducted on 3 to 5 H100 or A100 GPUs, depending on availability, using bf16 precision. For all GRPO experiments, we set `num_generations` to 8, `per_device_train_batch_size` to 2 or 4, and `gradient_accumulation_steps` to 32 or 64, ensuring a consistent global batch size of 512 across experiments. Each global step consisted of 64 unique prompts, each with 8 generated completions. We set the `max_prompt_length` to 4096 and the `max_completion_length` to 10240. The temperature of the policy model is set to 1. All other training parameters were kept at their default values, including a KL regularization coefficient of β= 0.04.

All GRPO experiments were run for a single epoch. As we enforced π$_{θold}$ = π$_θ$ to ensure strict on-policy learning, this is equivalent to setting `num_iterations` to 1 in later versions of the TRL library. We adopted the default system prompt from Open R1.

#### B.3 Experimental Hyperparameters

**SFT** For SFT, we experimented with learning rates and training epochs, as detailed in Section 5.4. For all experiments in Section 5.3, we initialized GRPO training with an SFT model trained using a learning rate of 4 × 10$^{-5}$ for 9 epochs. The only exception is the result shown in Figure 8 A, which illustrates training collapse from earlier runs using a learning rate of 3 × 10$^{-6}$. For experiments in Sections 5.2, we used SFT models trained with a learning rate of 4 × 10$^{-5}$ but for 3 epochs, as the SFT data was scaled.

**GRPO** For GRPO, we experimented with learning rate values and schedulers, as detailed in Section C.1. For all experiments in Sections 5.2 to 5.3, we used a GRPO learning rate of 3 × 10$^{-6}$ with a constant learning rate scheduler and a warmup ratio of 0.1.

#### B.4 Evaluation Details

We used vLLM [19] for inference during evaluation. All evaluations were conducted on the full test set (N = 26,244). We set the temperature to 0.6, `top_p` to 0.95, and `max_tokens` to 4096. To compute Pass@8, we set `n` to 8 in SamplingParams, generating eight completions per case. Pass@1 is reported as the mean accuracy across these eight generations. For evaluation, we extracted the DRG code enclosed within `<answer></answer>` tags and computed exact match against the reference code after text normalization. All training curves figures in Section 5.3 are smoothed using a moving average with a window of 50 steps.

#### B.5 Dynamic Resampling Details

For both neutral and positive dynamic resampling, we set the maximum number of regenera- tion attempts to 12. During regeneration, the model randomly selects a temperature from the set {0.7, 0.8, 0.9, 1.0}.

### C Additional Results

#### C.1 Experiments with GRPO Hyperparameters

We performed a limited hyperparameter search to tune the learn- ing rate and scheduler for GRPO. As detailed in Section 5.3, our best configuration combines DAPO loss, strict reward enforce- ment, and KL decay. A learning rate of 3 × 10$^{-6}$ consistently outperformed 1 × 10$^{-6}$, as shown in Figure 11. We also com- pared constant and decaying learning rate schedules and found comparable overall performance. Notably, a constant learning rate was more effective at the lower learning rate, though this advantage diminished at higher rates. Nevertheless, constant learning rates may help mitigate the gradient vanishing issue, as discussed in Section 3.3.

### Figure 11: Training Curve vs GRPO Learning Rates (Reward Score)

| Global Steps | GRPO 3e-6 Decay | GRPO 3e-6 Constant | GRPO 1e-6 Decay | GRPO 1e-6 Constant |
| :----------- | :-------------- | :----------------- | :-------------- | :----------------- |
| 0            | ~0.25           | ~0.25              | ~0.25           | ~0.25              |
| 100          | ~0.4            | ~0.42              | ~0.35           | ~0.38              |
| 200          | ~0.5            | ~0.52              | ~0.45           | ~0.48              |
| 300          | ~0.6            | ~0.62              | ~0.55           | ~0.58              |
| 400          | ~0.7            | ~0.72              | ~0.65           | ~0.68              |
| 500          | ~0.75           | ~0.77              | ~0.7             | ~0.73              |
| 600          | ~0.78           | ~0.8               | ~0.73           | ~0.76              |
| 700          | ~0.8             | ~0.82              | ~0.75           | ~0.78              |

#### C.2 Accuracy with RL Training in Ablation Studies

We present accuracy results from various ablation studies in Section 5.3, as shown in Figure 12.

### Figure 12: Accuracy with RL Training in Ablation Studies.
The dashed line indicates the baseline performance of vanilla GRPO with dense rewards. Error bars indicate the standard deviation across 8 runs.

#### A. Dynamic Resampling (Accuracy (%))

| Global Steps | Positive Resampling | Neutral Resampling | Vanilla GRPO |
| :----------- | :------------------ | :----------------- | :----------- |
| 200          | ~38                 | ~38                | ~38.5        |
| 300          | ~39                 | ~38                | ~38.5        |
| 400          | ~39                 | ~38                | ~39          |
| 500          | ~39                 | ~38                | ~39          |
| 600          | ~39                 | ~38                | ~39          |
| 700          | ~39                 | ~38                | ~39          |

#### B. Cognitive Behaviors (Accuracy (%))

| Global Steps | CoT-First | Differential Thinking | Vanilla GRPO |
| :----------- | :-------- | :-------------------- | :----------- |
| 200          | ~35       | ~31                   | ~38.5        |
| 300          | ~36       | ~31                   | ~38.5        |
| 400          | ~36       | ~32                   | ~39          |
| 500          | ~36       | ~33                   | ~39          |
| 600          | ~36       | ~33                   | ~39          |
| 700          | ~36       | ~33                   | ~39          |

#### C. KL Divergence (Accuracy (%))

| Global Steps | No KL | KL Decay | Vanilla GRPO |
| :----------- | :---- | :------- | :----------- |
| 200          | ~38   | ~38      | ~38.5        |
| 300          | ~39   | ~38      | ~38.5        |
| 400          | ~39   | ~38      | ~39          |
| 500          | ~39   | ~38      | ~39          |
| 600          | ~39   | ~38      | ~39          |
| 700          | ~39   | ~38      | ~39          |

#### D. GRPO Variants (Accuracy (%))

| Global Steps | DAPO Loss | Dr. GRPO Loss | Dr. GRPO Advantage | Vanilla GRPO |
| :----------- | :-------- | :------------ | :----------------- | :----------- |
| 200          | ~39       | ~37           | ~38                | ~38.5        |
| 300          | ~40       | ~38           | ~39                | ~38.5        |
| 400          | ~40       | ~38           | ~39                | ~39          |
| 500          | ~40       | ~38           | ~39                | ~39          |
| 600          | ~40       | ~38           | ~39                | ~39          |
| 700          | ~40       | ~38           | ~39                | ~39          |

#### E. Reward Shape (Accuracy (%))

| Global Steps | Strict Reward | Balanced Reward | Vanilla GRPO |
| :----------- | :------------ | :-------------- | :----------- |
| 200          | ~39           | ~38             | ~38.5        |
| 300          | ~40           | ~38             | ~38.5        |
| 400          | ~40           | ~39             | ~39          |
| 500          | ~40           | ~39             | ~39          |
| 600          | ~40           | ~39             | ~39          |
| 700          | ~40           | ~39             | ~39          |

#### F. Curriculum Learning (Accuracy (%))

| Global Steps | Remove Easy Cases | Remove Hard Cases | Remove Easy and Hard Cases | From Easy to Hard | Vanilla GRPO |
| :----------- | :---------------- | :---------------- | :------------------------- | :---------------- | :----------- |
| 200          | ~35               | ~40               | ~38                        | ~30               | ~38.5        |
| 300          | ~36               | ~40               | ~38                        | ~30               | ~38.5        |
| 400          | ~37               | ~40               | ~39                        | ~31               | ~39          |
| 500          | ~37               | ~40               | ~39                        | ~32               | ~39          |
| 600          | ~37               | ~40               | ~39                        | ~32               | ~39          |
| 700          | ~37               | ~40               | ~39                        | ~32               | ~39          |

#### C.3 Additional Results from Scaling to the Full Dataset

We present experimental results on the full dataset with varying SFT-to-RL data splits (Table 2). Our best configuration, combining DAPO loss, strict reward enforcement, and KL decay (Section 5.3), consistently outperformed vanilla GRPO across all experiments. Curriculum learning, implemented by excluding hard or easy cases, further improved performance. The best overall performance of DRG-SAPPHIRE was achieved with a 90% SFT and 10% RL split using the best GRPO configuration and hard-case exclusion.

Notably, we excluded the 90% SFT-only results and the RL results from the 95% SFT group as outliers in Figure 1 B and Figures 6 B and C. The checkpoints from the 90% SFT runs exhibited format-following instabilities, resulting in lower-than-expected scores. Despite this unstable SFT baseline, RL training remained robust, effectively leveraging the knowledge encoded through SFT and ultimately producing our best overall results. The RL outcomes from the 95% SFT experiments are likely not representative of true RL potential due to insufficient RL training (< 250 global steps). Additionally, we did not conduct experiments without KL decay or with curriculum learning for the 50% SFT group, given the limited performance observed with vanilla GRPO in that setting.

### Table 2: Scaling of GRPO on the Full Dataset.
All experiments were conducted on the full training set (N=236,192) with varying SFT-to-RL ratios, and the best result from each experiment is reported in the table. The best configuration of GRPO consists of DAPO Loss, Strict Reward, and KL Decay. The row highlighted in blue indicates the best Pass@1 performance. Bold values denote the highest score for each metric. * Checkpoint from the 90% SFT training runs exhibited format-following instabilities, resulting in lower-than-expected scores. Despite this unstable SFT baseline, RL training remained robust.

| Model                                    | DRG Pass@1 | DRG Pass@8 | DRG Maj@8 | Principal Diagnosis Pass@1 | Principal Diagnosis Pass@8 | Principal Diagnosis Maj@8 | CC/MCC Pass@1 | CC/MCC Pass@8 | CC/MCC Maj@8 |
| :--------------------------------------- | :--------- | :--------- | :-------- | :------------------------- | :------------------------- | :------------------------ | :------------ | :------------ | :----------- |
| **50% SFT**                              |            |            |           |                            |                            |                           |               |               |              |
| SFT                                      | 44.6       | 75.3       | 50.9      | 58.1                       | 77.1                       | 63.4                      | 51.8          | 80.7          | 58.4         |
| Vanilla GRPO                             | 52.8       | 64.2       | 53.9      | 63.9                       | 70.9                       | 64.9                      | 59.0          | 69.6          | 60.2         |
| Best Config                              | 53.7       | 59.1       | 53.9      | 63.5                       | 66.9                       | 63.9                      | 58.8          | 64.1          | 59.4         |
| **75% SFT**                              |            |            |           |                            |                            |                           |               |               |              |
| SFT                                      | 46.5       | 76.2       | 52.8      | 59.3                       | 77.6                       | 64.5                      | 53.3          | 80.7          | 59.6         |
| Vanilla GRPO                             | 53.5       | 64.9       | 54.6      | 64.0                       | 71.4                       | 65.1                      | 59.2          | 69.7          | 60.5         |
| Best Config                              | 54.6       | 60.6       | 54.9      | 64.4                       | 68.2                       | 64.8                      | 59.6          | 65.3          | 60.2         |
| Best Config - KL Decay                   | 54.0       | 65.3       | 55.0      | 63.8                       | 71.0                       | 64.9                      | 58.7          | 69.4          | 60.0         |
| Best Config + Remove Hard Case           | 54.4       | 58.1       | 54.5      | 63.8                       | 66.1                       | 64.0                      | 59.1          | 62.7          | 59.4         |
| Best Config + Remove Easy and Hard Case  | 54.7       | 58.8       | 54.8      | **64.5**                   | 67.1                       | 64.8                      | 59.5          | 63.5          | 59.9         |
| **90% SFT**                              |            |            |           |                            |                            |                           |               |               |              |
| SFT*                                     | 10.5       | 41.4       | 35.7      | 12.4                       | 47.1                       | 43.6                      | 11.7          | 47.0          | 41.6         |
| Vanilla GRPO                             | 54.1       | 65.2       | 55.0      | 64.3                       | 71.2                       | 65.2                      | 59.8          | 70.0          | 60.7         |
| Best Config                              | 54.6       | 62.2       | 54.9      | 64.0                       | 68.9                       | 64.6                      | 59.3          | 66.3          | 60.0         |
| Best Config - KL Decay                   | **54.2**   | 66.9       | **55.4**  | 63.9                       | **72.1**                   | 65.2                      | 59.3          | **70.8**      | **60.8**     |
| Best Config + Remove Hard Case           | **54.8**   | 60.3       | 54.9      | 64.4                       | 68.1                       | 64.7                      | **59.9**      | 64.9          | 60.4         |
| Best Config + Remove Easy and Hard Case  | 54.5       | 61.4       | 54.8      | 64.2                       | 68.8                       | **64.9**                  | 59.0          | 65.4          | 59.8         |
| **95% SFT**                              |            |            |           |                            |                            |                           |               |               |              |
| SFT                                      | 47.0       | **76.9**   | 53.3      | 59.7                       | **78.5**                   | 65.0                      | 53.9          | **81.0**      | 59.9         |
| Vanilla GRPO                             | 53.5       | 67.7       | 55.2      | 64.0                       | 72.6                       | **65.6**                  | 59.2          | 72.3          | **61.0**     |
| Best Config                              | 54.4       | 64.9       | 55.1      | 64.2                       | 70.5                       | 65.0                      | 59.4          | 69.4          | 60.5         |
| Best Config - KL Decay                   | 53.0       | 69.4       | 55.1      | 63.2                       | 73.5                       | 65.1                      | 58.4          | 73.4          | 60.8         |
| Best Config + Remove Hard Case           | 54.3       | 62.8       | 54.7      | 64.2                       | 69.7                       | 64.9                      | 59.5          | 67.4          | 60.3         |
| Best Config + Remove Easy and Hard Case  | 52.9       | 69.3       | 54.9      | 63.1                       | 73.0                       | 65.0                      | 58.1          | 73.3          | 60.5         |

### D Additional Discussion

#### D.1 Clinical Applications of Automated DRG Coding with Reasoning

In discussions with domain experts, DRG-Sapphire shows significant potential for real-world clinical applications. Here are two examples:

1.  Currently, DRGs are assigned by professional coders and are typically available only after hospital discharge. DRG-Sapphire can provide early DRG predictions to inform hospital operations and financial forecasting.
2.  DRG-Sapphire can support DRG-related quality improvement initiatives, such as those aimed at reducing the geometric length of stay, a metric directly determined by DRG. It provides transparent, interpretable explanations of DRG assignments, enabling clinicians to improve their clinical documentation to better reflect patient severity.

#### D.2 Practical Implication of Improved Pass@1 but Not Pass@k

Our experiments demonstrate that RL improves Pass@1 (i.e., accuracy) but not Pass@k for higher k values, indicating that RL enhances the model's ability to produce the correct DRG code in a single attempt without necessarily improving its broader reasoning capacity. However, this outcome aligns well with the requirements of high-stakes clinical applications like DRG coding, where only the first prediction truly matters, as users typically do not sample multiple outputs. Moreover, selecting the correct answer from multiple candidate responses is challenging, as methods beyond the best-of-N approach, which RL already optimizes by improving Pass@1 through better majority voting (Maj@k), are not well-established.

#### D.3 DRG vs ICD Coding

Although both DRG and International Classification of Diseases (ICD) codes serve clinical and administrative purposes, they differ significantly in classification approach and real-world applications. DRG assignment is typically formulated as a multi-class classification task, in which exactly one DRG code is assigned to summarize resource utilization and clinical complexity for an entire hospitalization. In contrast, ICD coding is a multi-label classification problem, as multiple ICD codes—covering both diagnoses and procedures—may be assigned to document a single encounter. Furthermore, the two coding systems exhibit distinct hierarchical structures: DRG assignment explicitly emphasizes identifying a principal diagnosis that primarily drives the hospitalization, along with secondary conditions and procedures that influence clinical complexity and reimbursement [8]. Finally, the utilization contexts for these codes differ significantly; DRGs are directly tied to inpatient reimbursement mechanisms and hospital resource management, whereas ICD codes have broader applications, including both inpatient and outpatient clinical documentation.

### E Limitations and Future Work

Our study encountered several limitations. First, we employed only rule-based rewards for final DRG assignments, without utilizing process supervision during the reasoning steps. While it's unclear how best to implement such supervision, theoretically, more granular and dense reward signals throughout the reasoning process could help guide the policy toward more effective exploration. Future work exploring this direction—potentially combining explicit DRG rules with techniques such as process reward modeling—represents an intriguing avenue.

Second, we applied relatively static curriculum learning and case-filtering strategies, which were conducted only once following the completion of a base run. A dynamic, online, difficulty-based filtering approach—applied at the per-batch level—may be more effective and warrants further investigation.

Lastly, our work focused exclusively on the challenging task of DRG coding. Extending our approach to other medical-domain tasks, or even diverse OOD tasks across different domains, would be valuable. In particular, it would be compelling to investigate whether scaling RL methods across multiple tasks and domains encourages exploration of more diverse reasoning pathways beyond the base model, rather than merely refining the output distribution toward outcomes that yield higher immediate rewards.

### F Data Access

Access to MIMIC-IV can be requested via [2], which requires signing a data use agreement. The training and test datasets used in this study can be obtained by following the instructions in [1]. For experiments involving MIMIC-IV data and proprietary models, we adhered to the guidelines in [3] and utilized the Azure OpenAI service.

### G Instruction to Reviewers

Instructions for physician expert reviewers on scoring the reasoning chase of DRG-Sapphire are provided below.

**Instruction to Reviewers**

1.  You will be provided with a discharge summary from the public MIMIC-IV dataset, along with a corresponding DRG code assignment and its rationale generated by a large language model (LLM).
2.  Please note that, similar to existing DRG prediction tools currently in use, the LLM-generated DRG code assignment may be either correct or incorrect.
3.  Your task is to rate the LLM output along two dimensions: **Helpfulness** and **Accuracy**, using a scale from **1 to 5 (very poor, poor, acceptable, good, or very good)**, where higher scores indicate better quality.
4.  **Helpfulness**: For this dimension, please answer the question: "Is the LLM's reasoning and explanation helpful to frontline healthcare providers?” Reflect on real-world initiatives you are engaged in that center around DRG optimization (e.g., efforts to reduce geo- metric mean length of stay). Assess whether the information provided by the LLM would meaningfully assist physicians in such settings, addressing questions commonly raised in practice.

    **Rubric:**
    *   **Score of 1 (very poor)**: The content is not helpful — for example, it may be too generic, lack necessary detail, or be overly vague.
    *   **Score of 3 (acceptable)**: The content is sufficiently helpful and acceptable for use in real-world clinical settings.
    *   **Score of 5 (very helpful)**: The content is highly helpful and could positively impact real-world DRG-related initiatives.
5.  **Accuracy**: For this dimension, please answer the question: “Does the information provided by the LLM accurately reflect MS-DRG assignment rules?” Base your evaluation on your best knowledge and understanding of the MS-DRG system.

    **Rubric:**
    *   **Score of 1 (very poor)**: The information is substantially inaccurate.
    *   **Score of 3 (acceptable)**: The information is accurate enough to support decision-making by frontline healthcare providers.
    *   **Score of 5 (very accurate)**: The information is highly accurate and consistent with MS-DRG assignment rules.

### H Prompts to LLM

Prompt for Qwen2.5-7B-Instruct for SFT cold-start dataset genearation.

**Generate Reasoning for DRG Assignment**

MS-DRG (Medicare Severity Diagnosis-Related Groups) is a system used by the U.S. Centers for Medicare & Medicaid Services to classify hospital inpatient stays for payment purposes. It groups patients with similar clinical conditions and resource needs into categories to determine reimbursement amounts. Each MS-DRG is assigned based on the patient's principal diagnosis, secondary diagnoses, procedures performed, age, discharge status, and other factors. The goal is to ensure fair and consistent hospital reimbursement based on the severity of the illness and the complexity of care required.

CC and MCC in MS-DRG:
*   CC (Complication or Comorbidity): A secondary diagnosis that increases the com- plexity of care and resource utilization.
*   MCC (Major Complication or Comorbidity): A more severe secondary condition that has a significant impact on resource use and hospital reimbursement.
*   MCCs have a greater effect on the DRG weight than CCs due to increased patient care complexity.

What is the most likely Medicare Severity Diagnosis Related Group (MS-DRG) based on the following discharge summary of a hospitalization? Provide the complete text description of the DRG code without including the numerical code.

[Discharge Summary]

The answer is [DRG Code]

Explain the reason why the DRG code is assigned. In your reasoning step, assume you don't know the right DRG code yet.

For SFT dataset generation with differential thinking, we added the following content to the prompt above.

**Additional Prompt to Generate Differential Thinking**

First, analyze the case details, including the principal diagnosis, secondary diagnoses, procedures performed, and any other relevant factors that may influence DRG assignment.

Next, list three possible DRG codes that you believe are the most likely. Enclose each DRG code in square brackets, like this: [Heart failure without MCC]. One of the three DRG codes should be [DRG Code].

Then, provide a detailed analysis for each of the three DRG codes. For each one, explain:
*   Why this DRG code might be appropriate?
*   Why it might not be appropriate?

Finally, based on your analysis, identify the most likely DRG code and it should be [DRG Code].

### I Example Outputs from DRG-Sapphire

**Example One**
<think>
Based on the provided discharge summary, the most likely Medicare Severity Diagnosis-Related Group (MS-DRG) is "ESOPHAGITIS GASTROENTERITIS AND MISCELLANEOUS DIGESTIVE DISORDERS WITHOUT MCC."
Let's break down the reasoning step-by-step:

1.  Principal Diagnosis and Conditions:
    *   The principal diagnosis is colitis, which is a digestive disorder.
    *   There are no complications or comorbidities (CCs or MCCs) that significantly im- pact the complexity of care or resource utilization more than the primary diagnosis of colitis.
2.  Secondary Conditions and CCs/MCCs:
    *   Atypical Carcinoid Tumor of the Lung: A significant condition, but not the primary reason for the hospitalization and does not elevate complexity of care related to colitis.
    *   Metastatic Pancreatic Neuroendocrine Tumor: Also significant, but not directly impacting the care for colitis.
    *   Perforated Diverticulitis: Required prior hospitalization but is not contributing to the current hospital stay.
    *   Transaminitis: Mild and resolved quickly; not related to the colitis.
    *   Hypothyroidism: Chronic and well-managed; no impact on the complexity of care.
    *   Ischemia: A possible cause of colitis, but not confirmed and does not significantly impact complexity.
3.  Complications or Comorbidities (CCs):
    *   The atypical carcinoid tumor, metastatic pancreatic neuroendocrine tumor, and ischemia are significant but not classified as CCs that elevate complexity for colitis.
    *   The perforated diverticulitis is historical and not relevant to the current hospital stay.
4.  Major Complication or Comorbidity (MCC):
    *   No significant conditions qualify as an MCC that would elevate the DRG weight beyond the primary diagnosis of colitis.

Given these conditions, the DRG code is assigned based on the primary diagnosis of digestive disorders, specifically colitis, without any significant complications or comorbidities that would elevate it to a higher complexity category. Therefore, the most appropriate DRG is "ESOPHAGITIS GASTROENTERITIS AND MISCELLANEOUS DIGESTIVE DISORDERS WITHOUT MCC."
</think>
<answer>
ESOPHAGITIS GASTROENTERITIS AND MISCELLANEOUS DIGESTIVE DISOR- DERS WITHOUT MCC
</answer>

**Example Two**
<think>
Based on the discharge summary provided, the most likely Medicare Severity Diagnosis-Related Group (MS-DRG) is PANCREAS LIVER AND SHUNT PROCEDURES WITH MCC.
To determine this, let's break down the key elements of the case:

1.  Principal Diagnosis and Procedures:
    *   The patient underwent a left hepatic lobectomy and placement of biliary stents.
    *   The lobectomy is a major liver surgery—this alone is a significant factor for DRG assignment.
2.  Complications and Comorbidities:
    *   Post-operative bilious drainage from the JP (jejunostomy) tube.
    *   Increased wound drainage requiring VAC (wound vacuum) placement.
    *   ERCP performed to manage bile duct stones and issues.
    *   Biliary stents placed during ERCP.
    *   Elevated WBC count and suspected infection requiring IV antibiotics and infectious disease consultation.
3.  MCC Analysis:
    *   The ERCP, biliary stents, and post-op complications significantly impacted recov- ery.
    *   These events qualify as a Major Complication or Comorbidity (MCC) under DRG criteria.
4.  Severity and Complexity:
    *   Multiple interventions (ERCP, stent placement, IV antibiotics).
    *   Extended inpatient stay due to complications.
    *   High resource utilization consistent with MCC-level DRG classification.
5.  Consistency with MS-DRG Criteria:
    *   MS-DRG assignment accounts for diagnosis, procedures, and complications.
    *   This case aligns with the category for major liver/pancreas procedures involving MCCs.

Therefore, the DRG code PANCREAS LIVER AND SHUNT PROCEDURES WITH MCC is assigned because the case involves a significant liver procedure (left hepatic lobectomy) and multiple major complications and interventions. The care complexity and extended course fulfill criteria for an MCC classification.
</think>
<answer>
PANCREAS LIVER AND SHUNT PROCEDURES WITH MCC
</answer>

### J Example Outputs Demonstrating Different Cognitive Behaviors

**Example from Answer-First**
<think>
Based on the provided discharge summary, the most likely Medicare Severity Diagnosis-Related Group (MS-DRG) is "OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC."
Reasoning:

1.  Principal Diagnosis:
    *   The patient presents with chest pain and ST elevations, initially suggestive of STEMI (ST-Elevation Myocardial Infarction).
    *   Cardiac catheterization revealed normal coronary arteries, ruling out coronary artery disease.
    *   ST elevations without coronary lesions point to myopericarditis or a related circu- latory system condition.
2.  CC (Complication or Comorbidity):
    *   Chest pain, ST elevations, and treatment with ibuprofen and colchicine increased care complexity.
    *   Mild echocardiographic findings (dilated aortic root and ascending aorta) required follow-up and additional monitoring, qualifying as a CC.
3.  MCC (Major Complication or Comorbidity):
    *   Despite significant diagnostic workup, no MCC-level condition was present.
    *   No major complication or comorbidity substantially elevated resource use or reimbursement criteria.
4.  Discharged with Specific Medications and Follow-Up Plans:
    *   The patient was discharged on ibuprofen and colchicine.
    *   A follow-up echocardiogram was planned, indicating ongoing resource use.

Conclusion:
The DRG code "OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC" is assigned because the patient's primary condition, although initially concerning, did not meet the criteria for an MCC. The presence of chest pain, ST elevations, treatment complexity, and follow-up needs justify a CC classification due to increased resource utilization and care complexity.
</think>
<answer>
OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC
</answer>

**Example from CoT-First**
<think>
To determine the most likely MS-DRG for this patient, let's break down the key elements from the discharge summary:

1.  Principal Diagnosis:
    *   The patient presents with ST elevations in the EKG, initially consistent with a STEMI (ST-Elevation Myocardial Infarction).
    *   Cardiac catheterization revealed normal coronaries (RRA – Right Rectus Artery), indicating a false positive STEMI.
    *   The patient was diagnosed with myopericarditis, which can cause chest pain and ST segment elevations.
2.  Secondary Diagnoses and Procedures:
    *   Myopericarditis (secondary to chest pain and ST elevations)
    *   Normal cardiac catheterization
    *   No significant past medical history
    *   Mildly dilated aortic root and ascending aorta on TTE
    *   GERD, insomnia, and OSA—no active treatment or significant impact on hospital- ization
3.  Complications and Comorbidities (CC and MCC):
    *   Myopericarditis increases the complexity of care and qualifies as a CC.
    *   There is no indication that it qualifies as a major complication or comorbidity (MCC).

Reasoning for the DRG Assignment:
*   Myopericarditis is the principal diagnosis and is considered a CC, not an MCC.
*   Mildly dilated aortic root and other secondary findings do not affect DRG assignment.
*   GERD, insomnia, and OSA are managed conditions that do not impact hospital resource use.

DRG Code Assignment:
The most likely MS-DRG for this patient, based on the provided information, is "OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC." This assignment reflects the presence of a complication (myopericarditis) that increases care complexity, without rising to the level of an MCC.
</think>
<answer>
OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC
</answer>

**Example from Differential Thinking**
<think>
Analysis of the Case Details
Principal Diagnosis:
*   ST-Elevations and Myopericarditis

Secondary Diagnoses:
*   No significant past medical history
*   GERD (Gastroesophageal Reflux Disease)
*   Insomnia
*   OSA (Obstructive Sleep Apnea)
*   HIV and Syphilis (screening recommended)

Procedures Performed:
*   Cardiac Catheterization

Other Relevant Factors:
*   Normal cardiac catheterization result
*   Mildly dilated aortic root and ascending aorta
*   Started on ibuprofen and colchicine

Possible DRG Codes
1.  Heart failure and shock without CC/MCC
2.  Other circulatory system diagnoses with CC
3.  Other circulatory system diagnoses with MCC

Analysis for Each DRG Code
1.  Heart failure and shock without CC/MCC
    Why this DRG code might be appropriate:
    *   The patient was admitted with ST-elevations, which can be indicative of heart issues.
    *   Mild dilation of the aortic root and ascending aorta could suggest early signs of heart failure.
    *   Started on medications for pain and inflammation, which are common in heart failure management.
    Why it might not be appropriate:
    *   Cardiac catheterization was normal, which doesn't support a diagnosis of heart failure.
    *   No history of heart failure or shock—only myopericarditis and mild aortic dilation.
    *   GERD, insomnia, and OSA are unrelated to heart failure.
2.  Other circulatory system diagnoses with CC
    Why this DRG code might be appropriate:
    *   ST-elevations and myopericarditis are circulatory system issues.
    *   GERD (a CC) increases complexity of care and resource utilization.
    Why it might not be appropriate:
    *   Condition managed with ibuprofen and colchicine—does not indicate severe complica- tions.
    *   Absence of heart failure or severe circulatory system condition to warrant a more specific DRG.
3.  Other circulatory system diagnoses with MCC
    Why this DRG code might be appropriate:
    *   ST-elevations and myopericarditis are significant circulatory issues.
    *   GERD (a CC) could be misclassified as MCC if chronic impact is overestimated.
    Why it might not be appropriate:
    *   Condition managed conservatively—not indicative of MCC-level complexity.
    *   No evidence of heart failure or other major circulatory complications.

Conclusion
Most Likely DRG Code: Other circulatory system diagnoses with CC
Reasoning:
*   Myopericarditis with ST-elevations is a significant circulatory condition.
*   GERD, as a CC, increases care complexity but not to the level of MCC.
*   No evidence of heart failure or other severe conditions to justify a different DRG code.

Based on the provided information, the most appropriate DRG code is Other circulatory system diagnoses with CC, as it best reflects the patient's primary condition and the presence of a CC (GERD) that increases complexity of care.
</think>
<answer>
OTHER CIRCULATORY SYSTEM DIAGNOSES WITH CC
</answer>