---
title: 'Conversational Time Series Foundation Models: Towards Explainable and Effective
  Forecasting'
authors:
- Defu Cao
- Michael Gee
- Jinbo Liu
- Hengxuan Wang
- Wei Yang
- Rui Wang
- Yan Liu
date: '2025-12-17'
source: arxiv
categories:
- cs.AI
pdf_url: https://arxiv.org/pdf/2512.16022v1
tags:
- paper
- source/arxiv
- topic/cs-AI
arxiv_id: 2512.16022v1
---

# Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting

**Authors:** Defu Cao, Michael Gee, Jinbo Liu, Hengxuan Wang, Wei Yang (+2 more)

**Date:** 2025-12-17

**Source:** arxiv | **Categories:** cs.AI

ðŸ“„ [PDF](https://arxiv.org/pdf/2512.16022v1)

## Abstract

The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

## Notes

<!-- Add your notes here -->
