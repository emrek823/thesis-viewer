# How to get 'responsible AI' right

**Source:** Google News
**URL:** https://news.google.com/rss/articles/CBMidEFVX3lxTE1MaXRkNF84SWloLVJLWExaYzJ6Q1luNWVtMWZUbnpqZDY1RFE3Y0QzY05GRV9kRmswelVBSEZQZjZmZnFQVk9QcVFuZjhTVGhtZ1gwOFNJd1E1VnRVeDZ3a1htYzQ0eFdVOU9hc3BydF90Z0hC?oc=5
**Date:** 2026-01-10

---

A physician AI expert walks through the correct ways for hospitals and health systems to handle artificial intelligence and where responsible AI can make the biggest near-term impact without creating new burdens.

AI & ML Intelligence Global

By Bill Siwicki , Managing Editor | January 9, 2026 | 11:44 AM

Dr. Heather Bassett, chief medical officer at Xsolis

Photo: Dr. Heather Bassett

In healthcare, "responsible AI" refers to a disciplined system of oversight that integrates governance, transparency and ethics – all working together to build trust in the intent of the artificial intelligence model among stakeholders.

But it's also a commitment to two non-negotiables that must precede innovation: patient safety and regulatory compliance. Healthcare is one of the most highly regulated industries, and for good reason since it deals with human lives not just transactions or clicks.

So responsible AI means understanding the level of risk associated with a given tool, putting strong governance around it, and making sure professionals can explain and monitor its behavior over time.

"Responsible AI requires clinicians with decision-making authority at critical junctures – not just 'humans in the loop' rubber-stamping outputs," said Dr. Heather Bassett, chief medical officer at Xsolis, vendor of an AI-powered predictive analytics platform. "This means clinical validation during development and continuous monitoring by people who understand both the AI's limitations and the clinical consequences of its failures.

"This oversight must be continuous – AI models degrade as data drifts, clinical practices evolve, and real-world deployment surfaces edge cases validation couldn't anticipate," she continued.

In the same vein, it's not enough for an algorithm to be clever – it must be safe, appropriately validated, and used in a way that aligns with clinical standards of care, she added. That risk lens is central to how professionals need to think about responsible AI in healthcare, she said.

Big impact, no new burdens

Given clinician burnout and workflow overload, there are places responsible AI can make big near-term impact without creating new burdens.

"Clinician burnout is very real, and it's being compounded by workforce shortages across nurses and physicians, as well as by the growing administrative load in today's modern healthcare enterprises," Bassett said. "Once more, graduating medical students are choosing not to practice at an alarming rate, exacerbating the industry problem.

"AI can help this situation, but I don't think we can solve that by guessing what clinicians need from the outside," she continued. "The most impactful AI efforts are the ones that are truly clinician-driven. That means sitting side-by-side with physicians and nurses, watching how they actually work, and asking, 'What are the repetitive, low-value tasks you wish you could hand off?'"

Very often, these tasks are administrative – documentation in EHRs, chasing orders, coordinating follow-ups – rather than the core clinical reasoning for which clinicians are trained.

First define the problem

"AI systems should target those friction points in ways that respect existing workflows, but only if we clearly define the problem first, which is a fundamental requirement of responsible AI," Bassett explained.

"This can't happen without clinicians actively engaged in the design process. I understand the reluctance – you're already stretched thin. If we want AI that actually fits clinical reality, physicians and nurses must show up to shape it.

"Agentic and ambient AI are good examples of where we can see near-term benefit," she added. "Ambient AI that listens to the patient-clinician encounter and drafts the notes can dramatically reduce 'pajama time,' when clinicians are at home trying to finish documentation hours after the patient visit."

Agentic AI can go a step further by acting on the information that surfaces in the conversation. For example, if a clinician says, "You'll need a follow-up with urology and your primary care doctor," the AI agent can trigger or even complete those scheduling tasks in the background. This removes low-value, time-consuming work from the clinician and reduces the chance that important follow-ups fall through the cracks.

"The key is to design these tools with clinicians at the table – if they're not part of the solution, vendors will keep rolling out things that look great on paper or in a demo but add work in the real world of delivering care," Bassett said.

How is healthcare doing with responsible AI?

As both a physician and an AI leader, Bassett sees places where healthcare is getting responsible AI right and where it is falling short.

"Where we're falling short, frankly, is in keeping up with the pace and complexity of both innovation and regulation at the same time," she said. "On one side, AI capabilities are evolving incredibly quickly – this past year alone we've seen agentic AI move from a niche concept into mainstream conversations.

"On the other side, the regulatory environment is shifting constantly, with attempts at state-level rules, presidential executive orders and emerging frameworks all creating a patchwork that's hard to navigate," she continued. "This dynamic can leave healthcare organizations unsure how to operationalize responsible AI in a way that will still be compliant a year from now."

The good news, she added, is there is meaningful progress, especially among larger health systems and payers.

"Many of them now have AI committees, more mature governance processes, and far more sophisticated questions for vendors than they did even a couple of years ago," she noted. "I used to get a handful of questions – now we get 100-question packets as part of RFPs and AI committee reviews asking about training data, model monitoring, bias and governance.

"Organizations like the Coalition for Healthcare AI and others are also helping by convening and supporting hospitals, payers and vendors to crowdsource best practices and frameworks, rather than having one entity dictate policy in a vacuum," she continued.

Is an AI tool clinically acceptable?

So how can healthcare organizations practically assess whether an AI tool is transparent enough to be clinically acceptable? Bassett offers a strategy.

"The first step is to define the level of clinical risk associated with the tool, because that determines how much transparency you should demand," she explained. "If an AI system is recommending diagnoses or treatments, the potential for patient harm is much higher than if it's optimizing claim workflows or helping prioritize charts for review.

"I sometimes use an analogy of a car purchase: I don't need to be an engineer who understands every nut and bolt, but I do need to know enough about the safety features to feel comfortable putting my family and loved ones inside," she continued. "In AI terms, that means understanding what kind of data the model was trained on, how it performs on standard metrics, how often it is retrained, and what its appropriate use cases and limitations are."

Organizations should expect vendors to deliver a model card – a succinct, structured document that explains how an AI/ML model was built, how it performs, and how it should and should not be used, she added.

AI nutrition labels

"Think of it as a nutrition label on a food product, delivering insights," she said. "A model card is the safety data sheet for an AI model, and it typically covers things like the model's intended use, the data it was trained on, key performance metrics overall and for different subgroups, known limitations, and any ethical or safety considerations.

"The idea originated as a way to make AI systems more transparent and accountable, especially for non-data scientists who still need to evaluate whether a model is trustworthy," she added.

In healthcare, a model card is particularly important because it helps clinicians, compliance and AI governance committees judge whether an AI model is clinically acceptable. Higher-risk tools should come with a higher bar for that level of detail and oversight, she warned.

"But here's what's often missing: Clinicians need to be involved in defining what transparency actually means for their context," she stated. "A radiologist evaluating an AI that flags lung nodules needs different information than a utilization review nurse working with AI-generated clinical summaries. The transparency standards can't be generic – they must be shaped by the people who will rely on the tool and bear responsibility for the decisions it informs.

"Ongoing reporting by a vendor should show how the tool performs in the real world, whether it is meeting agreed-upon KPIs or ROI targets, and how often it's being recalibrated as the underlying population or workflows change," she continued. "Because many AI systems are still perceived as black boxes, it's important to either offer some interpretability or, where that's limited, to compensate with robust evidence that the tool is consistently more accurate or effective than the baseline."

Independent third-party studies can be powerful validators, too.

"For Xsolis' AI Dragonfly platform, we recently had three independent studies conducted by Mayo Clinic Health System, Baylor Scott White and Yale New Haven Health System," Bassett noted. "That was very powerful for our customers and prospects. It's one thing for a vendor to say, 'This tool works,' and another for external data to show the tool is truly doing what it claims and delivering accurate results.

"Ultimately, transparency isn't just technical documentation – it's a partnership between vendors and frontline clinicians to ensure the right information flows to the right people to make safe, informed decisions," she concluded. "That partnership has to start during development, not after deployment."

Email him: bsiwicki@himss.org

Healthcare IT News is a HIMSS Media publication.

WATCH NOW: How to launch a healthcare AI project, per the VA AI chief

Topic:

Artificial Intelligence, Quality and Safety, Women In Health IT

---

#news #google-news
