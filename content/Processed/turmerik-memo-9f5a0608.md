---
source_pdf: "https://drive.google.com/file/d/1aHg0kTGHKS_0vvC4RxnWwh5X-cPH5Tgf/view"
drive_folder: "Research"
type: research

ingested: 2025-12-27
original_filename: "Turmerik memo.pdf"
---

> **Original:** [View Original PDF](https://drive.google.com/file/d/1aHg0kTGHKS_0vvC4RxnWwh5X-cPH5Tgf/view)

## Slide 1: READ ME

READ ME

## Slide 2: README:

README:
This is a multi-tab document! Tabs can be found by selecting the "Tabs and Outlines" bubble or selecting a section from the outline below:

Document Tabs:
* Executive Summary
* Tumerik Team
* Overview of Network
* Doctors: Sourcing and Vetting
* Demand Validation & GTM Strategy
* Product
* Roadmap
* Case Studies
* Competition

## Slide 3: Exec Summary

Exec Summary

## Slide 4: Turmerik

This is tab 1 of 9, click [here](https://aifellows.info/) to return to the outline

Turmerik
Turmerik is building the technical infrastructure behind tomorrow's Healthcare Al.
Founded by Ayushi Sinha (Harvard MBA, Princeton CS, Radiology Al at Nines, Microsoft Al) and Andrew Charbonneau (Caltech Al/Physics PhD, Princeton Physics, Thomas Jefferson High School for Science and Technology).

**Problem:** Al has the potential to transform diagnostics, drug development, and clinical workflows, but only if trained on clean, structured, and clinically precise data. Today, healthcare Al development is bottlenecked by unstructured medical records, noisy labels, and missing metadata. Internal teams struggle with costly, time-consuming manual annotation, while third-party vendors lack access to board certified physicians and fall short on clinical accuracy. Existing labeling platforms are generic, slow, and not designed for the complexity of healthcare data. As a result, many models fail to generalize in production, break down under regulatory scrutiny, or require repeated rework, which delays deployment and undermines trust in Al-driven tools.

**The Solution:** Turmerik accelerates Al development in healthcare by automating the services that digital health companies, Al labs, and health systems need to convert inconsistent, heterogeneous clinical data into high-quality, structured datasets, all reviewed and curated by verified expert physicians.

**Traction**
* **Validated Supply of Expert Labelers:** 750+ active labelers (compare to: 262 physicians who evaluated to OpenAl's HealthBench)
* **Paying & Signed Customers:** Floy, Carenostics, Wellth, Credo
* **Funding:** $517,500 committed
* **Website for doctors:** [https://aifellows.info/](https://aifellows.info/)

**Why Now?** Development of Healthcare Al is accelerating, but the infrastructure for training and evaluating models remains insufficient. Existing players like Scale, Labelbox, Mercor (200M ARR), Centaur Labs, Handshake (100M ARR), Protege, and Snorkel weren't built for the nuance and ambiguity of medical data. High-quality, clinically-verified labels are essential and demand is surging. Expert medical annotators aren't optional; they're critical.

**Large, High Growth Market:** Al healthcare will be $187.69 billion by 2030, CAGR 38.62%. Al data labeling will be $5.46 billion by 2030, CAGR 23.6%. Turmerik has the potential to generate 500M+ ARR.

**The Vision:** Turmerik is the foundational infrastructure layer for healthcare Al, starting with labeling but expanding into evaluation, protocol tuning, and longitudinal

## Slide 5: Data QA

This is tab 1 of 9, click [here](https://aifellows.info/) to return to the outline

data QA. The long-term bet is that every serious healthcare Al company will need a partner like Turmerik to get their models FDA-ready and enterprise-deployable.

## Slide 6: Turmerik Team

Turmerik Team

## Slide 7: Founding Team

This is tab 2 of 9, click [here](https://aifellows.info/) to return to the outline

**Founding Team**
Ayushi and Andy are uniquely positioned to build this system. They have deep founder-market fit and have already bootstrapped Turmerik into a scalable, revenue-generating platform that combines global physician networks, automated vetting, and smart annotation workflows tailored to healthcare.

**Ayushi**
I've focused my professional career building at the intersection of healthcare, Al, and data infrastructure, not just talking about it, and deploying it at scale.

Before founding Turmerik, I helped build one of the first FDA-cleared computer vision models at Nines, which served over 400,000 patients annually through systems like Yale and UPMC. We were funded by Accel and 8VC. One co-founder had co-founded Udacity and created Stanford's Racing Team (the very roots of what became Waymo). The other co-founder was Chair of Radiology at Mount Sinai. The ML engineers I worked with were PhDs who had spent years studying medical imaging. That product was later acquired by Sirona Medical.

I come from a family of doctors, so I've always viewed Al not just as a technical problem but as something deeply human. My work has consistently focused on enabling clinicians and patients to benefit from Al safely and equitably. With Turmerik, our team built tools that let radiologists, psychiatrists, cardiologists, and medical students power frontier Al models through structured labeling, clinical review, and regulatory-grade data. These workflows are now live across multiple healthcare Al companies.

But I've also backed up my technical expertise and execution with distribution and reach. I've cultivated a growing audience of:
* 34,000 LinkedIn followers who follow my content on Al, healthcare, and startups
* 8,000 newsletter subscribers across healthcare Al founders, clinicians, and investors
* Invited by Out of Pocket (27K+ subscribers) to write 2 posts on Al diagnostics.

**Andy**
I've spent the last several years building and deploying analytical and computational frameworks across some of the hardest problems in science and technology — climate modeling, quantum computing, national security, and now healthcare. My background spans theoretical physics, machine learning, and optimization, which has given me a systems-level perspective on how to translate complex questions into efficient frameworks for generating real-world impact. Across every field I've

## Slide 8: Andy's Background

This is tab 2 of 9, click [here](https://aifellows.info/) to return to the outline

worked in, the same challenge emerges: how to turn messy, fragmented information into reliable insights at scale.

Before Turmerik, I helped develop automated data pipelines for the Climate Modeling Alliance, integrating massive high-resolution datasets into global forecasting models and applying established and novel quality assurance methods. The frameworks I designed opened up previously untapped data scales for hydrological modeling and opened the door for a new style of ML parameterizations within climate models. These innovations cut computational overhead by 10%, and reduced forecast uncertainty by over 30% compared to legacy methods. Earlier, my other research in generative Al reformulated GAN architectures to explore sparse, high-dimensional data spaces with as few as 100 starting samples—unlocking scalable synthetic data capabilities and accelerating design cycles where data scarcity had stalled progress. As an operations research contractor for the Department of Homeland Security, I built an interactive ML toolkit that reduced analyst workflows from days to minutes, making predictive modeling accessible to non-technical users and earning recognition from DHS leadership. With these works and others, I continue to show we have the potential to do more with our data, faster, today.

Again and again, I've seen critical data trapped in silos, inconsistent or uninformative formats, or similar workflows rebuilt from scratch for each new problem. Collectively, we waste resources re-implementing the same fundamental procedures on similar data at both single-firm and across-industry scales, when centralized, domain-aware pipelines could free those dollars for actual innovation. Nowhere is this more urgent than in healthcare, where poor data infrastructure and systematic bloat slows breakthroughs and costs lives. The COVID pandemic underscored for me how urgently this problem needs to be solved. At Turmerik, I'm motivated to build that missing infrastructure: a trusted source for clinicians, researchers, and Al developers to build at scale. Offloading ubiquitous processing requirements similarly democratizes access to critical, quality information and techniques for those who would otherwise be forced to re-implement established systems, allowing limited resources to be put where they matter most. The biggest goals in the healthcare space will rely on a breakthrough in the paradigms of data utilization, which I believe Turmerik is primed to provide.

## Slide 9: Overview of Network

Overview of Network

## Slide 10: Network Overview

This is tab 3 of 9, click [here](https://aifellows.info/) to return to the outline

**Overview of Network**
We have attracted an exceptionally high-caliber applicant pool with $0 CAC and developed a scalable content-first machine to attract doctors. Candidates represent a wide range of clinical and research backgrounds across the United States, with a strong concentration in top-tier medical institutions and a demonstrated interest in applying Al to real-world healthcare problems.

**Professional Backgrounds and Signal Strength**
* Members have affiliations with leading medical institutions, including:
  * NYU Langone
  * UCLA Health
  * UCSF
  * Harvard
  * Columbia
  * Stanford
  * Cornell
  * Brown
* Clinical Experience Areas
  * Internal Medicine & Primary Care
  * Radiology & Imaging
  * Emergency Medicine
  * Oncology & Hematology
  * Psychiatry & Mental Health
  * Cardiology
  * Pediatrics
  * OB/GYN

**Network Effects & Referrals**
* Applicants who referred peers: 571 out of 761 (75%)
* Referrals include medical students, residents, and fellows from peer institutions, demonstrating a strong organic network effect and the potential for viral growth with $0 spent
* The high referral rate suggests this community can be leveraged for future hiring, product testing, or sales enablement

## Slide 11: Doctors: Sourcing and Vetting

Doctors: Sourcing and Vetting

## Slide 12: Why Building a Network of Doctors is Important

This is tab 4 of 9, click [here](https://aifellows.info/) to return to the outline

**Why Building a Network of Doctors is Important**
Getting doctors meaningfully involved in Al work is hard. Not just because they're busy, but because earning their trust, time, and attention requires clinical credibility, mission alignment, and operational precision. Most companies fail here. They underestimate the complexity of physician workflows or rely on cold outreach that results in low engagement and inconsistent quality.

But the demand is there. Across specialties, clinicians are increasingly eager to explore how Al can enhance care, reduce burden, and unlock new research, but many feel unequipped to contribute. A lack of technical training, limited time, and the steep learning curve of most Al tools act as barriers. What they need is a trusted, flexible way to participate, one that respects their time, aligns with their expertise, and delivers tangible value.

At Turmerik, we've built just that. What started as a high-touch sourcing effort has evolved into a scaled, structured, and high-signal engine for identifying, onboarding, and deploying clinical experts into Al workflows. We've invested in everything from ambassador programs at top academic centers to automated training and QA workflows. The result: production-ready, clinically aligned teams across specialties and modalities, spun up in days not months.

**Scaled Sourcing**
The effort began with a simple but powerful insight: many clinicians want to contribute meaningfully to the future of healthcare Al, but lack trusted, structured pathways to do so.

To address this, we built a high-signal sourcing, vetting, and deployment pipeline for medical professionals who bring both clinical expertise and a desire to shape the next generation of healthcare technology.

The early approach focused on trusted networks—word of mouth, peer nominations, and collaborations with top medical schools such as Harvard, where student leaders could recommend high-integrity colleagues. Outreach expanded to physician conferences, professional affinity groups, and university panels to better understand what would make Al collaboration meaningful and impactful for clinicians.

As demand grew, the process was formalized and scaled. We hired and trained a dedicated team to operationalize the funnel, from sourcing and screening, to onboarding, QA, and long-term engagement, ensuring consistency, reliability, and clinical rigor across every project. We:

## Slide 13: Automated Vetting

This is tab 4 of 9, click [here](https://aifellows.info/) to return to the outline

* Invested in content and distribution, growing a LinkedIn audience of 34,000+ who follow Ayushi's writing on Al and healthcare
* Launched a newsletter with 8,000+ subscribers: clinicians, founders, and investors who care about usable, trustworthy medical Al
* Organized in-person dinners and private meetups, sponsored by healthcare VCs, to create trust-based spaces where physicians and Al builders could connect
* Our sourcing engine now includes structured outreach, referral loops, ambassador pipelines, and curated expert pools across specialties

These aren't recruiting blitzes. They're curated entry points for smart, mission-aligned people to get involved in the real work of making Al safer and more clinically useful.

**Automated Vetting**
We've developed a rigorous, automated vetting process that's now executed by our team with quality controls we have designed. We don't just look at MCAT scores or school prestige. Those are directional, but they don't predict performance on real-world labeling, evaluation, or Al QA.

What we look for:

**1. Clear clinical reasoning and written communication**
We look for annotators who can identify key clinical findings and explain them precisely.

**Example:**
In an echocardiogram labeling task, a strong reviewer does not simply annotate "reduced ejection fraction.” Instead, they write, “EF estimated at 40 to 45 percent, consistent with mild systolic dysfunction in a patient with a history of ischemic cardiomyopathy." This level of reasoning helps ensure model outputs are interpretable and clinically valid.

**2. Comfort with ambiguity**
Clinical data is often incomplete, contradictory, or poorly structured. Furthermore, many clinical features are inherently uncertain or encompass a spectrum of representation not present in contemporary data products. For example, existing data products oversimplify feature subtlety, such as relying on binary labels or

## Slide 14: Automated Vetting (Continued)

This is tab 4 of 9, click [here](https://aifellows.info/) to return to the outline

diagnosis codes that miss the clinical nuance. Competitors label data by final diagnoses, ignoring the indications, ambiguities, and conflicting signals that informed that diagnosis, factors that are critical for training explainable and trustworthy Al systems.

**Example:**
In a chest CT report, one section might read “no acute pneumonia,” while another notes "scattered ground-glass opacities.” A thoughtful reviewer will annotate both findings separately and note, “Possible early interstitial process. Recommend adjudication." That kind of input is critical for building models that reflect real-world complexity.

**3. Intellectual humility and curiosity about Al**
We look for people who approach this work as collaborators, not as gatekeepers. The best contributors are eager to learn how Al systems interpret their inputs and are willing to revisit their assumptions.

**Example:**
In a psychiatric LLM evaluation task, one reviewer flagged a chatbot response as lacking empathy. After discussing the case with peers and reading about therapeutic response patterns in CBT, the reviewer revised their comment to, “Tone is neutral, but consistent with brief intervention style. Would recommend review for patient impact." That kind of growth mindset makes the labeling process stronger over time.

**4. A willingness to explain disagreement**
Disagreement between labelers is not a failure. It is often where the most valuable insights emerge. We look for people who take disagreement seriously, can articulate their reasoning, and engage in collaborative resolution.

**Example:**
In a mental health safety evaluation, two reviewers can disagree on whether an LLM's response to a suicide risk message was clinically appropriate. One can note it lacked a direct inquiry about safety. The other can highlight that, in some care protocols, immediate referral to emergency services is standard. Rather than escalate the conflict, they can propose a new label: “safe deferral vs. proactive inquiry." This should become part of the schema used to improve future evaluations.

Because in medical labeling and evaluation, adjudication is everything. A great labeler isn't just accurate. They can explain why they disagree, defend their interpretation, and adapt when they're wrong. That ability to engage with dissent and resolve it collaboratively is what elevates quality from acceptable to world-class.

## Slide 15: Expertise and Customer Focus

This is tab 4 of 9, click [here](https://aifellows.info/) to return to the outline

We've applied this process across live deployments with startups in radiology, mental health, cardiology, and oncology. We have personally led and overseen teams that include board-certified physicians, medical students, and research fellows, often spinning up production-ready cohorts in under 5 days.

And because Ayushi has angel invested in Al-first companies like Factory, Tandem, and Slingshot, we understand what's needed on the customer side: tight feedback loops, on-time delivery, traceability, and clinical credibility that stands up to regulatory and enterprise scrutiny.

What began as curiosity around learning about doctors' opinions on Al has evolved into a structured system that delivers high-integrity, high-context medical input to Al companies building tools that matter.

## Slide 16: Demand Validation & GTM Strategy

Demand Validation & GTM Strategy

## Slide 17: Demand Validation & GTM

This is tab 5 of 9, click [here](https://aifellows.info/) to return to the outline

**Demand Validation & GTM**

**ICP: VC-backed Healthcare Al Startups (paying customers: Carenostics, Wellth, Credo, Floy):**
Seed to Series C teams spending $50K–$300K/year seek flexible, high-skill labelers for niche specialties. Turmerik can match specialty needs, onboard teams fast, and offer founder-level support and startup literacy.

Today we charge customers:
* $50/hour for nurse/pre-clinical medical student
* $75/hour for post-clinical medical student
* $150/hour for Resident
* $300/hour for Attending Physician
We have a ~50% take rate.

**Validated Demand Across 4 Additional Strategic Segments:**

**1. Computer Vision Diagnostics (e.g., Aidoc, Qure.ai):**
Mid-to-late stage companies spending $200K–$2M annually on expert-labeled data, FDA validation sets, and regulatory submissions. Turmerik has engaged radiologists and residents to generate structured annotations, edge cases, and dissent rationale.

**2. EMR & LLM Integration (e.g., Epic, Notable, DeepScribe):**
From $250K-$10M+ budgets, teams seek high-quality structured EMR data, pretraining, and longitudinal edge-case audits. Turmerik's annotation tools and clinician QA layers meet unmet needs in charting, prior authorization, and note generation.

**3. Foundation Model Labs (e.g., OpenAI, DeepMind):**
Labs spending $10M–$100M/year on evals and fine-tuning need medical-specific QA pipelines and adjudication tooling. Turmerik offers EMR-to-eval pipelines, scalable MD labeling pods, and clinical QA services for generative model output.

**4. Voice Al in Medicine (e.g., Abridge, Nabla):**
Smaller teams ($100K–$500K/year spend) require cross-specialty benchmarks and hallucination/bias tracking. Turmerik can enable

## Slide 18: GTM Strategy

This is tab 5 of 9, click [here](https://aifellows.info/) to return to the outline

voice-to-structured pipelines with real-time MD feedback and disagreement labeling.

**GTM Strategy**
Turmerik's go-to-market strategy is **scaled** and **multi-pronged**:
* **Segmented Outreach:** Tailored campaigns by category (CV, EMR, LLM, Voice), using firmographics, clinical use case, and spend level to prioritize targets.
* **Founders Selling to Founders:** Direct engagement with technical and product leaders at Al startups, offering real-world credibility, fast iteration, and flexible scope (e.g. cohosting dinner with healthcare VCs).
* **Doctor-Led Inbound Loop:** Our physician network serves as both expert annotator and lead-generation engine, bringing in opportunities from hospitals and startups needing high-trust data pipelines.
* **Paid Pilots to Land-and-Expand:** Starting with scoped pilots (e.g., 10 patients, 3 radiologists, 1 evaluation set), we convert into larger multi-month engagements as trust and ROI are demonstrated.
* **[In Progress] Partnerships with Infra Players:** Active collaborations with model labs, cloud providers, and regulatory partners to build deeper integrations and compound defensibility.

## Slide 19: Product

Product

## Slide 20: The Turmerik Product

This is tab 6 of 9, click [here](https://aifellows.info/) to return to the outline

**The Turmerik Product**
Turmerik's first product is an Al-enabled annotation platform purpose-built for healthcare. It combines automation, structured clinical schemas, and embedded QA to make high-quality medical labeling fast, scalable, and clinically robust.

**1. Intelligent Task Automation (Not Just Assignment)**
Turmerik's system doesn't just assign tasks, it performs an initial automated pass using proprietary rules and models trained on real clinical data. For each record, it pre-populates candidate annotations (e.g. suspected diagnoses, lab abnormalities, imaging features) along with structured prompts to guide expert review. This automation accelerates throughput while focusing human effort where it matters most: ambiguity, exceptions, and edge cases.

**2. Flexible and Clinician-Controlled Editing**
Doctors retain full editorial control. They can approve, reject, edit, or expand the automated outputs, adding nuance, redefining categories, and flagging areas that require adjudication. Our interface supports free-text reasoning, category overrides, and tagging of novel concepts, making it possible to layer clinical subtlety and real-world complexity onto structured data pipelines.

**3. Real-Time Vetting and Contributor Scoring**
Every interaction is an opportunity to assess annotator quality. Turmerik's platform includes gold-standard reference tasks, ambiguity resolution tests, and reasoning checkpoints to evaluate contributors in real time. We track agreement rates, justification quality, and edge-case handling to score and rank reviewers, ensuring high-signal vetting based on demonstrated performance, not just credentials.

**4. Integrated Adjudication and Quality Control**
Turmerik includes built-in adjudication workflows and structured disagreement resolution. Complex cases are routed to senior reviewers or flagged for group consensus. All changes are versioned, auditable, and traceable, making it easier to meet the clinical standards required for FDA filings, real-world deployments, and academic research.

## Slide 21: Medical Annotation Tool Screenshot

This is tab 6 of 9, click [here](https://aifellows.info/) to return to the outline

**Medical Annotation Tool**
Patient ID: P123456

**(Screenshot of a medical annotation tool interface)**

The interface displays a "Clinical Summary" on the left and "Annotation Review" on the right.

**Clinical Summary content example:**
*   **Systolic Function:** The left ventricular ejection fraction is 45% as measured by Biplane Simpson's method, which is mildly reduced (normal ≥50%).
    *   LVEF - Source data detected
*   **Structural Assessment:** Left ventricular hypertrophy is present with mild severity, evidenced by Septal wall thickness 13mm.
    *   LVH - Source data detected
*   **Diastolic Function:** The patient demonstrates Grade II diastolic dysfunction based on E/e' ratio 12, LA volume index 42ml/m².
    *   DIASTOLIC - Source data detected
*   **Valvular Assessment:** Valvular evaluation reveals moderate mitral regurgitation and mild aortic stenosis per Color Doppler assessment.
    *   VALVULAR - Source data detected
*   **Pulmonary Pressures:** Estimated pulmonary artery systolic pressure is 38 mmHg derived from TR velocity 2.9 m/s.
    *   PAP - Source data detected
*   **Pericardial Status:** No pericardial effusion is visualized
    *   PERICARDIAL - Source data detected
*   **No effusion visualized**
    *   PERICARDIAL - Source data detected
*   **Wall Motion:** Regional wall motion analysis demonstrates Inferior wall hypokinesis assessed through Visual assessment in multiple views.
    *   WALL - Source data detected
*   **Transplant Evaluation:** Based on the comprehensive echocardiographic assessment, the patient is cleared for transplant evaluation
    *   TRANSPLANT - Source data detected
*   **Echo parameters within acceptable range for transplant evaluation**
    *   TRANSPLANT - Source data detected

**Annotation Review section on the right shows:**
*   1 Accepted, 0 Modified, 1 Rejected
*   "Source: TR velocity 2.9 m/s" with a rejected tag "x" and "PAP" text below it.
*   "Wall Motion" with a green checkmark indicating 87% confidence, and "Inferior wall hypokinesis" as a source.
*   "Wall Motion" with a rejected tag "x".
*   "Transplant Clearance" with a rejected tag "x" and 93% confidence, "patient is cleared for transplant evaluation" as a source.
*   "Add New Annotation" section with "Pericardial Effusion" as a dropdown.
*   Text fields for "Enter text to annotate..." and "Source information (optional)...".
*   "+ Add Annotation" button.

**Annotation Legend:**
*   Left Ventricular Ejection Fraction (LVEF)
*   Pulmonary Artery Pressure (PAP)
*   Left Ventricular Hypertrophy (LVH)
*   Pericardial Effusion
*   Diastolic Dysfunction
*   Wall Motion Abnormalities
*   Valvular Disease
*   Cleared for Transplant (Echo)

(Note: Each highlighted section includes the source data and methodology used for the finding.)

## Slide 22: Roadmap

Roadmap

## Slide 23: Turmerik 6-Month Product Roadmap

This is tab 7 of 9, click [here](https://aifellows.info/) to return to the outline

**Big Picture Vision**
Turmerik is building the data infrastructure layer for healthcare Al. Just as Stripe enabled developers to integrate payments in hours instead of months, Turmerik will make it possible for Al teams to access, structure, and validate medical data with clinical-grade quality, at scale. Our long-term vision is to be the trusted interface between healthcare systems and Al developers, powering everything from model development to regulatory approval, reimbursement, and real-world deployment. We believe the winners in healthcare Al won't just have better algorithms, they'll have better data pipelines. Turmerik is the platform that makes those pipelines possible.

**Turmerik 6-Month Product Roadmap**
Funding will be used to hire engineers to build out core infrastructure across three major tracks: annotation services, healthcare data brokering, and synthetic data generation. Each track supports distinct customer segments and positions Turmerik as the foundational platform for Al-ready, regulatory-grade medical data.

**1) Al-Enabled Expert Annotations**
*   **Customers:** Al startups and healthtech companies with internal datasets. Biotech and pharma working with global, hospital-sourced datasets
*   **Fall 2025 Milestones:** Launch internal dataset annotation service for digital health and Al model development (live pilots with 2–3 startups). Build and deploy standardized annotation workflows for multi-country datasets (Turmerik's network includes academic medical centers in India, Israel, Canada, Thailand, US). Embed automated vetting and pre-labeling to ensure scale and speed with physician-grade oversight
*   **Beyond Fall:** Expand platform to support multi-modal inputs including structured EHR, imaging (DICOM), labs, pathology, and PDF reports. Launch direct integration with hospital EMRs to streamline retrospective data ingestion and annotation. Implement annotation QA APIs for customers to embed traceability in FDA, IRB, and payer workflows

**2) Data Broker Infrastructure**
*   **Customers:** Hospitals, clinics, and diagnostic centers with untapped data assets. Pharma and Al companies seeking ethically sourced, Al-ready datasets
*   **Fall 2025 Milestones:** Launch LLM-readiness and quality scoring tools for 2-3 pilot hospitals to assess monetization potential. Build first version of hospital-facing data dashboard to track data structure, missingness, and licensing opportunities. Deploy buyer-side intake workflows to match incoming requests with available structured datasets
*   **Beyond Fall:** Add automated data transformation pipelines including ICD, RxNorm, HL7/FHIR output, and imaging metadata. Launch dataset

## Slide 24: Roadmap (Continued)

This is tab 7 of 9, click [here](https://aifellows.info/) to return to the outline

marketplace for curated and annotated real-world data with regulatory guardrails. Build contract and consent management modules to support compliant licensing (DPDP, HIPAA, GDPR)

**3) Synthetic Data**
*   **Customers:** Al companies needing safe, realistic test data. Researchers and buyers evaluating models pre-deployment
*   **Fall 2025 Milestones:** Launch Synthetic Data API allowing customers to upload schemas and receive realistic synthetic patient records (pilot with Credo Health). Launch Demo Dataset Playground for LLM and model testing using curated datasets (pilot with OpenAl and stealth Al labs). Develop disease-specific templates for longitudinal synthetic records with 10 to 15 timepoints
*   **Beyond Fall:** Expand synthetic data libraries by disease area including diabetes, heart failure, and depression. Enable bias and diversity configuration to allow testing across geographies, races, and comorbidities. Build an SDK for synthetic data generation tied to real-world prevalence distributions and clinical ontologies

## Slide 25: Case Studies

Case Studies

## Slide 26: Specialized Clinical Labeling at Scale

This is tab 8 of 9, click [here](https://aifellows.info/) to return to the outline

**Specialized Clinical Labeling at Scale**
Turmerik has consistently delivered top-tier medical labeling talent across specialties, geographies, and regulatory contexts. We've built a system that can rapidly match expert clinicians to complex, high-precision Al tasks in under 5 days.

Our talent network has been deployed for use cases such as:
1. LLM evaluation in psychiatry
2. FDA-bound radiology UX testing
3. In-person, HIPAA-compliant chart reviews
4. Validating its cardiopulmonary Al models by assembling a team of cardiology-trained experts to annotate and review echocardiogram reports with clinical precision and traceability.
5. Enhancing Computer Vision Safety and Clinical Utility
6. Turmerik delivered fully synthetic diabetes patients, demonstrating the depth, diversity, and completeness of each record across multiple visit types (PCP, RN, endocrinology, podiatry, etc.), with real physicians enhancing auto-generated templates. Customer said: this approach was “more robust than anything we could generate ourselves”

**Case Study: Structured Annotation and Model Review for Carenostics' Cardiopulmonary Al**

**Customer:** Carenostics is an Al diagnostics company building predictive tools for early detection of chronic cardiopulmonary diseases and eligibility of a transplant, integrating longitudinal EHR data with imaging modalities.

**Challenge:** Carenostics needed to validate its cardiopulmonary models using clinically accurate annotations across a small but representative set of multimodal documents. Their goals included:
* Structuring echocardiogram reports for model evaluation
* Citing exact source text to ensure traceability for clinical review
* Aligning model predictions with gold-standard annotations across diverse report formats

In-house annotation was not feasible due to clinical complexity, the need for medical expertise, and internal bandwidth constraints.

**Turmerik Solution:** Turmerik deployed a cross-functional team to manage both annotation and model review:
* Medically-trained annotators (post-clinical medical students and licensed nurses) handled document review and structured extraction

## Slide 27: Case Study Results and ROI

This is tab 8 of 9, click [here](https://aifellows.info/) to return to the outline

* An expert review layer composed of Al engineers, MD/MBAs, and ML PhDs that refined labeling schemas, reviewed edge cases, and aligned outputs with model objectives

**Scope of work included:**
* JSON-format clinical documents, covering 10–50 examples per document type
* Extraction of structured diagnostic and procedural fields with exact-text citations
* Collaborative feedback loop with Carenostics' Al team to ensure alignment between annotation outputs and model architecture

**Results**
* Delivered a clinician-vetted, structured dataset
* Provided traceable annotations that enabled Carenostics to evaluate model performance against ground truth with confidence
* Identified schema inconsistencies and edge cases early, reducing rework and accelerating downstream development
* Our team's model review feedback directly supported improvements to data ingestion, preprocessing, and evaluation logic

**Strategic ROI:** Through this collaboration, Carenostics was able to:
* De-risk its model evaluation process by using clinically grounded annotations
* Reduce internal overhead on QA and schema development
* Move faster toward product validation milestones by relying on a ready-to-scale annotation partner

**Why It Matters**
Accurate Al in cardiology demands more than labeled data. It requires clinically credible data pipelines and feedback from domain experts. TurmerikAl delivered both, enabling Carenostics to refine its models and prepare for validation with confidence.

## Slide 28: Competition

Competition

## Slide 29: Competitive Landscape & Turmerik Advantage

This is tab 9 of 9, click [here](https://aifellows.info/) to return to the outline

**Competitive Landscape & Turmerik Advantage**

**Market Context**
Al in healthcare is surging, but data infrastructure is broken. Models trained on noisy, unstructured, or oversimplified clinical data consistently underperform in production and break down under regulatory scrutiny. The demand for high-quality, explainable, and clinically rigorous data is outpacing the capabilities of existing platforms.

**Strategic Advantage**
Turmerik isn't just a labeling company. We are building the infrastructure layer for healthcare Al. Like Stripe for payments or Twilio for messaging, Turmerik abstracts the painful complexity of structuring, adjudicating, and validating medical data. The company is well-positioned to become the go-to partner for Al teams looking to build FDA-cleared, reimbursement-ready, and clinician-trusted tools.

**Competitive Landscape**

| Category               | Players                               | Limitations                                                               |
| :--------------------- | :------------------------------------ | :------------------------------------------------------------------------ |
| Horizontal Platforms   | Scale Al, Labelbox, Snorkel, Mercor   | Generic tools lack healthcare-specific schemas or clinical context awareness. |
| Crowdsourced Health Data | Centaur Labs, Handshake               | Rely on crowd & overlap, rather than experts; limited oversight and depth |
| In-House Labeling      | Internal teams at Al startups, hospitals | High cost, slow ramp-up, poor QA, and no reusable infrastructure          |

**Turmerik's Winning Differentiation**

| Dimension         | Turmerik                                                                                                                                | Others                                                                     |
| :---------------- | :-------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------- |
| **Labeling Quality**  | Prioritizes post-training evaluation, ambiguity handling, and clinical reasoning                                                        | Focused on binary classification and speed                                 |
| **Expert Network**    | 720+ active, vetted physicians with tracked performance and specialties                                                                 | OpenAl HealthBench used 262 doctors                                        |
| **Technical Moat**    | Proprietary auto-labeling engine + real-time MD QA to reduce hallucinations and improve consistency                                     | LLM-based evals lack structure or oversight                                |
| **Domain Expertise**  | Custom schemas, adjudication workflows, and multimodal (EMR, imaging, lab, synthetic) capabilities, based on co-founders research and industry experience | Generic tools not designed for healthcare nuance                           |

## Slide 30: Business Model Comparison

This is tab 9 of 9, click [here](https://aifellows.info/) to return to the outline

| Business Model | Turmerik                                                                 | Others                             |
| :------------- | :----------------------------------------------------------------------- | :--------------------------------- |
|                | Expanding into high-margin verticals: synthetic data, data brokerage, regulatory QA-as-a-service | Narrow focus on labeling as a service |