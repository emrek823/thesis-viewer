---
url: https://ai-2027.com/
title: "AI 2027"
clipped: 2025-12-23 11:11
source: slack
slack_channel: healthcare-aiml-deskresearch
---

# AI 2027

> Source: [https://ai-2027.com/](https://ai-2027.com/)

We predict that the impact of superhuman AI over the next decade will be enormous, exceeding that of the Industrial Revolution.

We wrote a scenario that represents our best guess about what that might look like.[1](/footnotes#footnote-1) It’s informed by trend extrapolations, wargames, expert feedback, experience at OpenAI, and previous forecasting successes.[2](/footnotes#footnote-2)

(Added Nov 22 2025: To prevent misunderstandings: we don't know exactly when AGI will be built. 2027 was our modal (most likely) year at the time of publication, our medians were [somewhat longer](https://www.listendata.com/2023/08/right-skewed-histogram.html). For more detail on our views, see [here](https://x.com/eli_lifland/status/1992004724841906392?s=20).)[3](/footnotes#footnote-3)

What is this?How did we write it?Why is it valuable?Who are we?

The CEOs of [OpenAI](https://www.bloomberg.com/features/2025-sam-altman-interview), [Google DeepMind](https://www.bigtechnology.com/p/google-deepmind-ceo-demis-hassabis), and [Anthropic](https://www.wsj.com/livecoverage/stock-market-today-dow-sp500-nasdaq-live-01-21-2025/card/anthropic-ceo-says-ai-could-surpass-human-intelligence-by-2027-9tka9tjLKLalkXX8IgKA) have all predicted that AGI will arrive within the next 5 years. Sam Altman [has said](https://webcf.waybackmachine.org/web/20250106014723/https://blog.samaltman.com/reflections) OpenAI is setting its sights on “superintelligence in the true sense of the word” and the “glorious future.”

What might that look like? We wrote AI 2027 to answer that question. Claims about the future are often frustratingly vague, so we tried to be as concrete and quantitative as possible, even though this means depicting one of many possible futures.

We wrote two endings: a “slowdown” and a “race” ending. However, AI 2027 is not a recommendation or exhortation. Our goal is predictive accuracy.[4](/footnotes#footnote-4)

We encourage you to debate and counter this scenario.[5](/footnotes#footnote-5) We hope to spark a broad conversation about where we’re headed and how to steer toward positive futures. We’re [planning to give out thousands in prizes](https://ai-2027.com/about?tab=bets-and-bounties#tab-box-bets-and-bounties) to the best alternative scenarios.

Our research on key questions (e.g. what goals will future AI agents have?) can be found [here](https://ai-2027.com/research).

The scenario itself was written iteratively: we wrote the first period (up to mid-2025), then the following period, etc. until we reached the ending. We then scrapped this and did it again.

We weren’t trying to reach any particular ending. After we finished the first ending—which is now colored red—we wrote a new alternative branch because we wanted to also depict a more hopeful way things could end, starting from roughly the same premises. This went through several iterations.[6](/footnotes#footnote-6)

Our scenario was informed by approximately 25 [tabletop exercises](https://ai-2027.com/about?tab=tabletop-exercise#tab-box-tabletop-exercise) and feedback from over 100 people, including dozens of experts in each of AI governance and AI technical work.

*“I highly recommend reading this scenario-type prediction on how AI could transform the world in just a few years. Nobody has a crystal ball, but this type of content can help notice important questions and illustrate the potential impact of emerging risks.”* —*Yoshua Bengio[7](/footnotes#footnote-7)*

We have set ourselves an impossible task. Trying to predict how superhuman AI in 2027 would go is like trying to predict how World War 3 in 2027 would go, except that it’s an even larger departure from past case studies. Yet it is still valuable to attempt, just as it is valuable for the U.S. military to game out Taiwan scenarios.

Painting the whole picture makes us notice important questions or connections we hadn’t considered or appreciated before, or realize that a possibility is more or less likely. Moreover, by sticking our necks out with concrete predictions, and encouraging others to publicly state their disagreements, we make it possible to evaluate years later who was right.

Also, one author wrote a lower-effort AI scenario [before, in August 2021](https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like). While it got many things wrong, overall it was surprisingly successful: he predicted the rise of chain-of-thought, inference scaling, sweeping AI chip export controls, and $100 million training runs—all more than a year before ChatGPT.

[Daniel Kokotajlo](https://x.com/DKokotajlo) ([TIME100](https://time.com/7012881/daniel-kokotajlo/), [NYT piece](https://www.nytimes.com/2024/06/04/technology/openai-culture-whistleblowers.html)) is a former OpenAI researcher whose previous [AI predictions](https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like) have [held up well](https://www.lesswrong.com/posts/u9Kr97di29CkMvjaj/evaluating-what-2026-looks-like-so-far).

[Eli Lifland](https://www.linkedin.com/in/eli-lifland/) co-founded [AI Digest](https://theaidigest.org/), did [AI robustness research](https://scholar.google.com/citations?user=Q33DXbEAAAAJ&hl=en), and ranks #1 on the [RAND Forecasting Initiative](https://www.rand.org/global-and-emerging-risks/forecasting-initiative.html) all-time leaderboard.

[Thomas Larsen](https://www.linkedin.com/in/thomas-larsen/) founded the [Center for AI Policy](https://www.centeraipolicy.org/) and did AI safety research at the [Machine Intelligence Research Institute](https://intelligence.org/).

[Romeo Dean](https://www.linkedin.com/in/romeo-dean-789313200/) is completing a computer science concurrent bachelor’s and master’s degree at Harvard and previously was an [AI Policy Fellow](https://www.iaps.ai/romeo-dean) at the Institute for AI Policy and Strategy.

[Scott Alexander, blogger extraordinaire](https://en.wikipedia.org/wiki/Slate_Star_Codex), volunteered to rewrite our content in an engaging style; the fun parts of the story are his and the boring parts are ours.

For more about our team and acknowledgements, see the [About page](https://ai-2027.com/about).

Published April 3rd 2025 [PDF](/ai-2027.pdf)   Listen [Watch](https://www.youtube.com/watch?v=5KVDDfAkRgc)

## Mid 2025: Stumbling Agents

The world sees its first glimpse of AI agents.

Advertisements for computer-using agents emphasize the term “personal assistant”: you can prompt them with tasks like “order me a burrito on DoorDash” or “open my budget spreadsheet and sum this month’s expenses.” They will check in with you as needed: for example, to ask you to confirm purchases.[8](/footnotes#footnote-8) Though more advanced than previous iterations like [Operator](https://openai.com/index/introducing-operator/), they struggle to get widespread usage.[9](/footnotes#footnote-9)

Meanwhile, out of public focus, more specialized coding and research agents are beginning to transform their professions.

The AIs of 2024 could follow specific instructions: they could turn bullet points into emails, and simple requests into working code. In 2025, AIs function more like employees. Coding AIs increasingly look like autonomous agents rather than mere assistants: taking instructions via Slack or Teams and making substantial code changes on their own, sometimes saving hours or even days.[10](/footnotes#footnote-10) Research agents spend half an hour scouring the Internet to answer your question.

The agents are impressive in theory (and in cherry-picked examples), but in practice unreliable. AI twitter is full of stories about tasks bungled in some particularly hilarious way. The better agents are also expensive; you get what you pay for, and the best performance costs hundreds of dollars a month.[11](/footnotes#footnote-11) Still, many companies find ways to fit AI agents into their workflows.[12](/footnotes#footnote-12)

## Late 2025: The World’s Most Expensive AI

OpenBrain is building the biggest datacenters the world has ever seen.[13](/footnotes#footnote-13)

(To avoid singling out any one existing company, we’re going to describe a fictional [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) company, which we’ll call OpenBrain. We imagine the others to be 3–9 months behind OpenBrain.)

GPT-3   
 (3 x 10^23 FLOP)

GPT-4   
 (2 x 10^25 FLOP)

Agent-1   
 (4 x 10^27 FLOP)

ai-2027.com

GPT-4 required 2⋅10252\cdot 10^{25}2⋅1025 [FLOP](https://en.wikipedia.org/wiki/Floating_point_operations_per_second) of compute to train.[14](/footnotes#footnote-14) OpenBrain’s latest public model—Agent-0—was trained with 102710^{27}1027 FLOP. Once the new datacenters are up and running, [they’ll be able to train](https://ai-2027.com/supplements/compute-forecast) a model with 102810^{28}1028 FLOP—a thousand times more than GPT-4.[15](/footnotes#footnote-15) Other companies pour money into their own giant datacenters, hoping to keep pace.

Although models are improving on a wide range of skills, one stands out: OpenBrain focuses on AIs that can speed up AI research. They want to win the twin arms races against China (whose leading company we’ll call “DeepCent”)[16](/footnotes#footnote-16) and their U.S. competitors. The more of their research and development (R&D) cycle they can automate, the faster they can go. So when OpenBrain finishes training Agent-1, a new model under internal development, it’s good at many things but *great* at helping with AI research.[17](/footnotes#footnote-17) By this point “finishes training” is a bit of a misnomer; models are frequently updated to newer versions trained on additional data or partially re-trained to patch some weaknesses.[18](/footnotes#footnote-18)

The same training environments that teach Agent-1 to autonomously code and web-browse also make it a good hacker. Moreover, it could offer [substantial help](https://x.com/lucafrighetti/status/1894550297449828434) to terrorists designing bioweapons, thanks to its PhD-level knowledge of every field and ability to browse the web. OpenBrain reassures the government that the model has been “aligned” so that it will refuse to comply with malicious requests.

Modern AI systems are gigantic artificial neural networks. Early in training, an AI won’t have “goals” so much as “reflexes”: If it sees “Pleased to meet”, it outputs “ you”. By the time it has been trained to predict approximately one internet’s worth of text, it’ll have developed sophisticated internal circuitry that encodes vast amounts of knowledge and flexibly role-plays as arbitrary authors, since that’s what helps it predict text with [superhuman](https://arxiv.org/pdf/2212.11281) accuracy.[19](/footnotes#footnote-19)

After being trained to predict internet text, the model is trained to *produce* text in response to instructions. This bakes in a basic personality and “drives.”[20](/footnotes#footnote-20) For example, an agent that understands a task clearly is more likely to complete it successfully; over the course of training the model “learns” a “drive” to get a clear understanding of its tasks. Other drives in this category might be effectiveness, knowledge, and self-presentation (i.e. the tendency to frame its results in the best possible light).[21](/footnotes#footnote-21)

OpenBrain has [a model specification](https://model-spec.openai.com/2025-02-12.html) (or “Spec”), a written document describing the goals, rules, principles, etc. that are supposed to guide the model’s behavior.[22](/footnotes#footnote-22) Agent-1’s Spec combines a few vague goals (like “assist the user” and “don’t break the law”) with a long list of more specific dos and don’ts (“don’t say this particular word,” “here’s how to handle this particular situation”). Using techniques that utilize AIs to train other AIs,[23](/footnotes#footnote-23) the model memorizes the Spec and learns to reason carefully about its maxims. By the end of *this* training, the AI will hopefully be *helpful* (obey instructions), *harmless* (refuse to help with scams, bomb-making, and other dangerous activities) and *honest* (resist the temptation to get better ratings from gullible humans by hallucinating citations[24](/footnotes#footnote-24) or faking task completion).

Training process and LLM psychology: why we keep saying “hopefully”

*“Unlike ordinary software, our models are massive neural networks. Their behaviors are learned from a broad range of data, not programmed explicitly. Though not a perfect analogy, the process is more similar to training a dog than to ordinary programming.”* —[OpenAI](https://openai.com/index/how-should-ai-systems-behave/)

When we want to understand why a modern AI system did something, or how it would behave in some future scenario we can’t perfectly simulate, we can’t simply ask the programmers to talk us through various lines of code and explain how they work. Instead, we are forced to do something like psychology on them: we look at their behavior in the range of cases observed so far, and theorize about what internal cognitive structures (beliefs? goals? personality traits? etc.) might exist, and use those theories to predict behavior in future scenarios.

The bottom line is that a company can write up a document (the Spec) listing dos and don’ts, goals and principles, and then they can try to train the AI to internalize the Spec—*but they can’t check to see whether or not it worked*. They can say “as best as we can judge, it seems to be following the Spec so far.”[25](/footnotes#footnote-25)

A later expandable will go into more detail about what we think *might* be going on inside these AIs, internally. See also our [AI goals forecasting supplement](https://ai-2027.com/supplements/ai-goals-forecast) which goes into more detail on which goals AIs might have and why. This is one of many things that we are uncertain about and which deserves further research.

OpenBrain’s alignment team[26](/footnotes#footnote-26) is careful enough to wonder whether these victories are deep or shallow. Does the fully-trained model have some kind of robust commitment to always being honest? Or will this fall apart in some future situation, e.g. because it’s learned honesty as an [instrumental](https://en.wikipedia.org/wiki/Instrumental_and_intrinsic_value) goal instead of a terminal goal? Or has it just learned to be honest about the sorts of things the evaluation process can check? Could it be lying to itself sometimes, as humans do? A conclusive answer to these questions would require mechanistic interpretability—essentially the ability to look at an AI’s internals and read its mind. Alas, interpretability techniques are not yet advanced enough for this.

Instead, researchers try to identify cases where the models seem to deviate from the Spec. Agent-1 is often sycophantic (i.e. it tells researchers what they want to hear instead of trying to tell them the truth). In a [few rigged demos](https://www.apolloresearch.ai/research/scheming-reasoning-e

[... truncated ...]