---
title: A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models
  and Logic Tree Reasoning
authors:
- Zelin Zang
- Wenyi Gu
- Siqi Ma
- Dan Yang
- Yue Shen
- Zhu Zhang
- Guohui Fan
- Wing-Kuen Ling
- Fuji Yang
date: '2025-12-25'
categories:
- cs.AI
pdf_url: https://arxiv.org/pdf/2512.21583v1
paper_id: 2512.21583v1
source: arxiv
tags:
- paper
- source/arxiv
- topic/cs-AI
---

# A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning

**Authors:** Zelin Zang, Wenyi Gu, Siqi Ma, Dan Yang, Yue Shen...

**Date:** 2025-12-25 | **Source:** arxiv | **Categories:** cs.AI

[PDF](https://arxiv.org/pdf/2512.21583v1)

## Abstract

With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.

## Notes

