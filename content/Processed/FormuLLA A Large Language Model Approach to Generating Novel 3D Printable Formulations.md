---
title: 'FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable
  Formulations'
authors:
- Adeshola Okubena
- Yusuf Ali Mohammed
- Moe Elbadawi
date: '2026-01-05'
categories:
- cs.AI
pdf_url: https://arxiv.org/pdf/2601.02071v1
arxiv_id: 2601.02071v1
tags:
- paper
- alphaxiv/hot
- topic/cs-AI
---

# FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations

**Authors:** Adeshola Okubena, Yusuf Ali Mohammed, Moe Elbadawi

**Date:** 2026-01-05 | **Categories:** cs.AI

[PDF](https://arxiv.org/pdf/2601.02071v1) | [AlphaXiv](https://alphaxiv.org/abs/2601.02071v1)

## Abstract

Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

## Notes

