---
url: https://robertwachter.substack.com/p/medicines-ai-knowledge-war-heats
title: "Medicine’s AI Knowledge War Heats Up"
clipped: 2025-12-23 13:01
source: slack
slack_channel: healthcare-aiml-deskresearch
---

# Medicine’s AI Knowledge War Heats Up

> Source: [https://robertwachter.substack.com/p/medicines-ai-knowledge-war-heats](https://robertwachter.substack.com/p/medicines-ai-knowledge-war-heats)

# Medicine’s AI Knowledge War Heats Up

### The Battlegrounds May Surprise You

[![Robert Wachter's avatar](https://substackcdn.com/image/fetch/$s_!bFsX!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72b21f7a-0b7a-4065-99f5-55043363bc7c_1606x1606.jpeg)](https://substack.com/@robertwachter)

[Robert Wachter](https://substack.com/@robertwachter)

Oct 01, 2025

13

14

Share

Many of today’s leading AI use cases – in areas such as clinical documentation, billing, and call centers – are designed to reduce administrative friction. As important as these uses are, the real potential for AI to transform healthcare lies in building effective decision support – helping doctors, as well as patients, make decisions that result in higher-quality, safer, and less expensive care without driving everybody crazy.

This means that, while many of [today’s business skirmishes](https://robertwachter.substack.com/p/platforms) are between companies like Abridge, Ambience, and Nabla (AI scribe companies now broadening their offerings in the face of growing commoditization), the real war will be fought on a different battlefield: the one to become the AI decision support tool of choice.

Thanks for reading Pattern Recognition! Subscribe for free to receive new posts and support my work.

The competition intensified last week with [UpToDate’s announcement of a new AI-enabled feature](https://www.wolterskluwer.com/en/solutions/uptodate/ai-clinical-decision-support). With this announcement, UpToDate demonstrated that it was not about to play dead and allow an upstart called OpenEvidence to dominate the field of AI clinical decision support. The battle will be fought over features, no doubt, but it will fundamentally be about a more intriguing question: What is the optimal source of knowledge for the practice of medicine? Here’s why.

### ***A Little Background***

The breakneck speed with which OpenEvidence supplanted UpToDate over the past two years to become the go-to resource for clinical decision-support gave me an odd sense of déjà vu. I’ve seen this before, I thought. But where?

And then I remembered – it was about 25 years ago, when UpToDate did precisely the same thing to the textbooks that I grew up with in medical school.

Before the mid-1990s, everybody had their favorite textbook – every doctor was either a Harrison’s or a Cecil person. Then, seemingly overnight, the tomes (including one I edited, *[Hospital Medicine](https://www.amazon.com/Hospital-Medicine-Wachter-dp-0781747279/dp/0781747279/)*) began gathering dust on shelves everywhere, as a new kind of tool easily demonstrated its superiority in point-of-care medical knowledge retrieval.

[![](https://substackcdn.com/image/fetch/$s_!24f8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a007a95-81f3-4c3a-a7aa-4c28cfaf16fc_1286x1846.png)](https://substackcdn.com/image/fetch/$s_!24f8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a007a95-81f3-4c3a-a7aa-4c28cfaf16fc_1286x1846.png)

Today, UpToDate remains an extraordinary resource, created by over 7,500 human experts charged with culling and interpreting the medical literature and guidelines to produce chapters on every conceivable clinical topic – and keeping the chapters, as the name says, up to date. From the time it emerged in the late 1990s until about 2023, it deservedly had what felt like an unshakeable position as the dominant point-of-care tool for health systems and clinicians.

And then, before you could say Clayton Christensen, OpenEvidence displaced UpToDate – particularly among doctors-in-training, often the vanguard of tech-driven change – because it could perform a trick that UpToDate couldn’t: take an entire clinical case, in all its staggering complexity, and produce an AI-generated “curbside consult” that was impressively accurate, context-specific, and, yes, up to date. Suddenly, UpToDate’s approach, which had seemed revolutionary a generation earlier, seemed stale.

[![](https://substackcdn.com/image/fetch/$s_!R7n-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e6e4e39-305c-4a66-9c23-23942b90dda0_454x584.png)](https://substackcdn.com/image/fetch/$s_!R7n-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e6e4e39-305c-4a66-9c23-23942b90dda0_454x584.png)

The late Clay Christensen, the father of “Disruptive Innovation”

### ***Two Very Different Approaches***

Particularly for those of you who aren’t clinicians, let me demonstrate what I mean. Here’s a relatively complex case I saw a couple of years ago when I was visiting professor at Yale. I’ll present it briefly, as I might if I had run into my favorite hepatologist in the hospital cafeteria and asked her for a curbside consult:

*“I’m caring for a 75-year-old man with Waldenstrom’s macroglobulinemia who came in with fever, hypoxia, and pulmonary infiltrates. We started him on Zosyn, and he rapidly developed liver dysfunction, with transaminases in the thousands but normal alk phos and bili. What do you think is going on?”*

If that was Greek to you, here’s a lay explanation: the case involves an elderly gentleman with a rare, chronic blood disorder, an acute lung problem that is probably (but not definitely) a pneumonia, and now an unhappy liver with a distinctive pattern of blood tests.

[![](https://substackcdn.com/image/fetch/$s_!icHT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5fce6d9-024c-4959-82c0-a180fd286160_880x764.png)](https://substackcdn.com/image/fetch/$s_!icHT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5fce6d9-024c-4959-82c0-a180fd286160_880x764.png)

GPT-5’s idea of a sad liver

“All happy families are alike; each unhappy family is unhappy in its own way,” reads Tolstoy’s first sentence in *Anna Karenina*. As with many patients, particularly those who are acutely ill, this one’s body was unhappy in its very own way. Accordingly, the answer won’t be found in a single textbook chapter.

When I entered my long prompt in UpToDate, it produced links to four of its chapters:

· *Approach to the immunocompromised patient with fever and pulmonary infiltrates*

· *Epidemiology, pathogenesis, clinical manifestations, and diagnosis of Waldenstrom macroglobulinemia*

· *Sepsis in children: definition, clinical manifestations, and diagnosis*

· *Evaluation and management of fever in children and adults with sickle cell disease*

The first two are off point – my question was about why this patient has a failing liver, not about what’s going on in his lungs or what his rare blood disorder is about. The last two links, to pediatric chapters, are bizarre, since my query clearly concerned a 75-year-old man.

Recognizing that I was asking UpToDate to do something it isn’t built to do (grapple with a complex case, rather than a narrowly framed problem), I tried deconstructing my query into a single focused question: *“Can Zosyn cause liver injury?”* This time, UpToDate took me to its chapter on Zosyn (an intravenous antibiotic), which was fine but didn’t answer my question. Several more links followed, most of them clinically irrelevant. The most on-point link was seventh on the list: a chapter on acute liver injury.

In contrast, when I put my original prompt into OpenEvidence, it “thought” for about 10 seconds and then gave me an impressive answer that considered the entire case presentation. First, it reviewed the most likely diagnoses, which it considered to be ischemic hepatitis (“shock liver”), drug-induced hepatocellular injury, and sepsis-associated liver injury. It then offered several other “*Most Important Not to Miss Diagnoses,”* including Tylenol overdose, Budd-Chiari syndrome (hepatic vein thrombosis), and fulminant viral hepatitis. These two lists mirror the way experienced clinicians approach clinical cases – we often create lists of both the most likely diagnoses and those that might be fatal if missed.

> Unsurprisingly, the difference between OpenEvidence’s results vs. those from UpToDate is reminiscent of the difference between the links delivered by a traditional Google search and the results of a query on GPT, Gemini, Claude, or any modern general-purpose large language model. The new tools permit an input that reflects real-world complexity, and their output is a human-like synthesis, not a series of links. Like the iPhone, before you saw it, you didn’t know you needed it; afterwards, you don’t know how you operated without it.

### ***Disruption: What Goes Around…***

It’s always fascinating to see legacy businesses try to adapt to an upstart that threatens to upend their business models. In the late 1990s and early 2000s, as UpToDate was gaining momentum, the publishers of medical textbooks didn’t sit still. Within a couple of years, most of them created digital platforms that blended all their textbooks into a single searchable database. Using that platform approach (Elsevier’s version was called *MD Consult*, later rebranded into *ClinicalKey*; McGraw-Hill’s was *AccessMedicine*), the publishers remained viable, although their legacy textbooks are now used more for reference and teaching than for answering point-of-care clinical questions. That latter category has, until recently, been dominated by UpToDate.

The huge Dutch publishing house Wolters Kluwer took a different approach than Elsevier’s and McGraw-Hill’s, and probably the wiser one: in 2008, the conglomerate purchased UpToDate for an undisclosed sum rumored to be in the hundreds of millions of dollars. If you can’t beat them, buy them, I guess.

Thus my sense of déjà vu, as OpenEvidence is currently doing to UpToDate what UpToDate did to the textbook publishers. Based on its investments to date, OpenEvidence has been valued at $6 billion; one wonders whether that valuation was based partly on the assumption that UpToDate would sit back and allow its lunch to be eaten by its upstart rival.

If so, investors in OpenEvidence may be in for a rude awakening. In last week’s announcement, UpToDate said it would soon roll out its own AI-based tool, called *UpToDate Expert AI*. Note the careful branding, designed to highlight the fact that the UpToDate tool won’t scour the entire medical literature or the unfiltered internet for insights. Instead, UpToDate’s AI will draw its wisdom exclusively from its thousands of continuously updated chapters, written by experts.

### ***What is the Optimal Source of Truth?***

This raises a question that I can’t answer, at least not yet: Now that UpToDate is adding genAI capabilities, which tool will provide better results? Will it be tools like OpenEvidence, whose AI reviews the world’s medical literature via searches of journals and society guidelines, and then applies genAI to create answers and references? (Note that OpenEvidence’s process isn’t entirely devoid of human touch. As Daniel Nadler, founder and CEO of OpenEvidence told me, OpenEvidence’s content is honed through “Reinforcement Learning from Human Feedback,” RLHF, a process in which humans – both hired staff and clinician end-users of the tool – assess and tune the AI-drafted answers, ultimately teaching the AI to give better responses.)

Or will UpToDate’s more human-crafted approach – in which the AI limits its source of truth to the chapters in the UpToDate dataset – produce better results? In announcing the new AI tool, Wolters Kluwer Chief Medical Officer Peter Bonis [said of UpToDate’s content experts](https://www.newsweek.com/uptodate-launches-generative-ai-clinical-decision-support-access-health-2134087), “They understand the intersection of evidence, real-world patient care, the fact that there isn’t a randomized study for everything, and they have judgment.” He didn’t mention OpenEvidence, but he didn’t need to.

Finally, there’ll be another competitor, one that will approach decision support from a very different angle. Epic, the largest electronic health record (EHR) vendor, also [announced a new set of AI tools last month](https://www.fiercehealthcare.com/health-tech/epic-unveils-major-ai-features-ai-charting-microsoft-cosmos-ai-risk-prediction-and-rcm). One of them, named “Art” for clinicians, is designed to review individual patients’ records as well as Epic’s database (“Cosmos”) of deidentified records on millions of patients cared for in health systems that use the company’s EHR.

Over time, one assumes Epic will be able to mine two sources of data unavailable to UpToDate, OpenEvidence, or any of the general large language models: a) the diagnostic and treatment strategies of tens of thousands of clinicians that use Epic’s EHR, and b) the clinical courses of millions of patients in Cosmos. Before long, Epic’s AI may be able to tell a clinician that “patients like yours did better on drug A than drug B” and “92 percent of doctors like you prescribed drug X in this situation.” Personally, I’m not confident these approaches will be supremely helpful, but I’m open to being surprised.

### ***The Promise of EHR Integration***

The AI tools being adopted in healthcare today – such as digital scribes, AI chart review, and prior authorization assistance – are helpful, but the real impact and savings will be in this clinical decision support, particularly once we figure out how to tightly integrate it into the EHR.

This last point is crucial. Currently, although UpToDate is a click away on most clinicians’ Epic, Athena, or Oracle desktops (increasingly, OpenEvidence is as well), these tools aren’t integrated into EHRs in any meaningful way. The clinician interested in the best treatment for high blood pressure or the proper tests to rule out lymphoma needs to leave their EHR workflow and conduct a search of UpToDate or OpenEvidence for the answer. And that search won’t know anything about the patient at hand beyond what the clinician types into the search box.

> Imagine a world in which the clinician doesn’t need to enter, “This is a 75-year-old man with Waldenstrom’s who presents with fever and pulmonary infiltrates and develops elevated liver tests,” but rather one in which AI is “reading” the patient’s chart automatically, so that it “knows” all that contextual information. The AI tool might suggest – and perhaps even “tee up” – recommended diagnostic tests or treatments based on its review of the patient’s actual data.

It might even allow the clinician to ask very specific questions (*“Review the blood pressure trends for this patient and tell me if shock liver is a possible explanation for the abnormal liver tests”* or *“Given when the patient recei

[... truncated ...]